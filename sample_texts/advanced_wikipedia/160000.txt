


Anarchism is a political philosophy and movement that seeks to abolish all institutions that perpetuate authority, coercion, or hierarchy, primarily targeting the state and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. A historically left-wing movement, anarchism is usually described as the libertarian wing of the socialist movement (libertarian socialism).


Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose conclusion marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements.


Anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate or experience an anarchist society, while revolutionary tactics, which have historically taken a violent turn, aim to directly and globally target those goals. Many facets of human civilisation have been influenced by anarchist theory, critique, and praxis.


Etymology, terminology, and definition

The etymological origin of anarchism is from the Ancient Greek anarkhia (ἀναρχία), meaning "without a ruler", composed of the prefix an- ("without") and the word arkhos ("leader" or "ruler"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756–1836) and Wilhelm Weitling (1808–1871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.


The first political philosopher to call himself an anarchist (French: anarchiste) was Pierre-Joseph Proudhon (1809–1865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism; its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.


While the term libertarian has been largely synonymous with anarchism, its meaning has more recently been diluted by wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberal and socialist but more so. Many scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.


While opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is much discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.


History

Pre-modern era

The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had "significant anticipations" of anarchism.


Anarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between laws imposed by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.


In medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I. In Basra, religious sects preached against the state. In Europe, various religious sects developed anti-state and libertarian tendencies.


Renewed interest in antiquity during the Renaissance and in private judgement during the Reformation restored elements of anti-authoritarian secularism in Europe, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.


Modern era

During the French Revolution, partisan groups such as the Enragés and the sans-culottes saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 19th century: William Godwin espoused philosophical anarchism in England, morally delegitimising the state, while Max Stirner's thinking paved the way to individualism and Pierre-Joseph Proudhon's theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then-unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.


Drawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen's Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin's faction (the Jura Federation) and Proudhon's followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin predicted that if revolutionaries gained power by Marx's terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one's needs.


During this time, a minority of anarchists adopted tactics of revolutionary political violence, known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to prevent anarchists immigrating to the US, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.


By the turn of the 20th century, the terrorist movement had died down, giving way to anarchist communism and syndicalism, while anarchism had spread all over the world. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from East Asian countries, who moved to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. Anarchists were involved in the Strandzha Commune and Krusevo Republic established in Macedonia in Ilinden–Preobrazhenie Uprising of 1903, and in the Mexican Revolution of 1910. The revolutionary wave of 1917–23 saw varying degrees of active participation by anarchists.


Despite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement, especially in the Makhnovshchina. Seeing the victories of the Bolsheviks in the October Revolution and the resulting Russian Civil War, many workers and activists turned to Communist parties, which grew at the expense of anarchism and other socialist movements. In France and the United States, members of major syndicalist movements such as the General Confederation of Labour and the Industrial Workers of the World left their organisations and joined the Communist International. However, anarchists met harsh suppression after the Bolshevik government had stabilised, including during the Kronstadt rebellion. Several anarchists from Petrograd and Moscow fled to Ukraine, before the Bolsheviks crushed the anarchist movement there too. With the anarchists being repressed in Russia, two new antithetical currents emerged, namely platformism and synthesis anarchism. The former sought to create a coherent group that would push for revolution while the latter were against anything that would resemble a political party.


In the Spanish Civil War of 1936–39, anarchists and syndicalists (CNT and FAI) once again allied themselves with various currents of leftists. A long tradition of Spanish anarchism led to anarchists playing a pivotal role in the war, and particularly in the Spanish Revolution of 1936. In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain, where they collectivised the land. The Soviet Union provided some limited assistance at the beginning of the war, but the result was a bitter fight between communists and other leftists in a series of events known as the May Days, as Joseph Stalin asserted Soviet control of the Republican government, ending in another defeat of anarchists at the hands of the communists.


Post-WWII

By the end of World War II, the anarchist movement had been severely weakened. The 1960s witnessed a revival of anarchism, likely caused by a perceived failure of Marxism–Leninism and tensions built by the Cold War. During this time, anarchism found a presence in other movements critical towards both capitalism and the state such as the anti-nuclear, environmental, and peace movements, the counterculture of the 1960s, and the New Left. It also saw a transition from its previous revolutionary nature to provocative anti-capitalist reformism. Anarchism became associated with punk subculture as exemplified by bands such as Crass and the Sex Pistols. The established feminist tendencies of anarcha-feminism returned with vigour during the second wave of feminism. Black anarchism began to take form at this time and influenced anarchism's move from a Eurocentric demographic. This coincided with its failure to gain traction in Northern Europe and its unprecedented height in Latin America.


Around the turn of the 21st century, anarchism grew in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements. Interest in the anarchist movement developed alongside momentum in the anti-globalisation movement, whose leading activist networks were anarchist in orientation. Anarchists became known for their involvement in protests against the World Trade Organization (WTO), the Group of Eight and the World Economic Forum. During the protests, ad hoc leaderless anonymous cadres known as black blocs engaged in rioting, property destruction and violent confrontations with the police. Other organisational tactics pioneered at this time include affinity groups, security culture and the use of decentralised technologies such as the Internet. A significant event of this period was the confrontations at the 1999 Seattle WTO conference. As the movement shaped 21st century radicalism, wider embrace of anarchist principles signalled a revival of interest. Contemporary news coverage of black bloc demonstrations emphasises violence committed by anarchists.


While having revolutionary aspirations, many contemporary forms of anarchism are not confrontational. Instead, they are trying to build an alternative way of social organisation (following the theories of dual power), based on mutual interdependence and voluntary cooperation, for instance in groups such as Food Not Bombs and in self-managed social centers.


Anarchism's publicity has also led more scholars in fields such as anthropology and history to engage with the anarchist movement, although contemporary anarchism favours actions over academic theory. Anarchist ideas have been influential in the development of the Zapatistas in Mexico and the Democratic Federation of Northern Syria, more commonly known as Rojava, a de facto autonomous region in northern Syria.


Schools of thought

Anarchist schools of thought have been generally grouped into two main historical traditions, social anarchism and individualist anarchism, owing to their different origins, values and evolution. The individualist current emphasises negative liberty in opposing restraints upon the free individual, while the social current emphasises positive liberty in aiming to achieve the free potential of society through equality and social ownership. In a chronological sense, anarchism can be segmented by the classical currents of the late 19th century and the post-classical currents (anarcha-feminism, green anarchism, and post-anarchism) developed thereafter.


Anarchism's emphasis on anti-capitalism, egalitarianism, and for the extension of community and individuality sets it apart from anarcho-capitalism and other types of economic libertarianism. Anarchism is usually placed on the far-left of the political spectrum, though many reject state authority from conservative principles, such as anarcho-capitalists. Much of its economics and legal philosophy reflect anti-authoritarian, anti-statist, libertarian, and radical interpretations of left-wing and socialist politics such as collectivism, communism, individualism, mutualism, and syndicalism, among other libertarian socialist economic theories.


As anarchism does not offer a fixed body of doctrine from a single particular worldview, many anarchist types and traditions exist and varieties of anarchy diverge widely. One reaction against sectarianism within the anarchist milieu was anarchism without adjectives, a call for toleration and unity among anarchists first adopted by Fernando Tarrida del Mármol in 1889 in response to the bitter debates of anarchist theory at the time. Despite separation, the various anarchist schools of thought are not seen as distinct entities but rather as tendencies that intermingle and are connected through a set of shared principles such as autonomy, mutual aid, anti-authoritarianism and decentralisation.


Beyond the specific factions of anarchist political movements which constitute political anarchism lies philosophical anarchism, which holds that the state lacks moral legitimacy, without necessarily accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism, philosophical anarchism may tolerate the existence of a minimal state but claims that citizens have no moral obligation to obey government when it conflicts with individual autonomy. Philosophical currents as diverse as Objectivism and Kantianism have provided arguments drawn on in favour of philosophical anarchism, including Wolff's defence of anarchism against formal methods for legitimating it. Anarchism pays significant attention to moral arguments since ethics have a central role in anarchist philosophy. Belief in political nihilism has been espoused by anarchists.


Classical

Inceptive currents among classical anarchist currents were mutualism and individualism. They were followed by the major currents of social anarchism (collectivist, communist and syndicalist). They differ on organisational and economic aspects of their ideal society.


Mutualism is an 18th-century economic theory that was developed into anarchist theory by Pierre-Joseph Proudhon. Its aims include "abolishing the state", reciprocity, free association, voluntary contract, federation and monetary reform of both credit and currency that would be regulated by a bank of the people. Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. In What Is Property? (1840), Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property." Collectivist anarchism is a revolutionary socialist form of anarchism commonly associated with Mikhail Bakunin. Collectivist anarchists advocate collective ownership of the means of production which is theorised to be achieved through violent revolution and that workers be paid according to time worked, rather than goods being distributed according to need as in communism. Collectivist anarchism arose alongside Marxism but rejected the dictatorship of the proletariat despite the stated Marxist goal of a collectivist stateless society.


Anarcho-communism is a theory of anarchism that advocates a communist society with common ownership of the means of production, held by a federal network of voluntary associations, with production and consumption based on the guiding principle "From each according to his ability, to each according to his need." Anarcho-communism developed from radical socialist currents after the French Revolution but was first formulated as such in the Italian section of the First International. It was later expanded upon in the theoretical work of Peter Kropotkin, whose specific style would go onto become the dominating view of anarchists by the late 19th century. Anarcho-syndicalism is a branch of anarchism that views labour syndicates as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are direct action, workers' solidarity and workers' self-management.


Individualist anarchism is a set of several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants. Early influences on individualist forms of anarchism include William Godwin, Max Stirner, and Henry David Thoreau. Through many countries, individualist anarchism attracted a small yet diverse following of Bohemian artists and intellectuals as well as young anarchist outlaws in what became known as illegalism and individual reclamation.


Post-classical and contemporary

Anarchism has continued to generate many philosophies and movements, at times eclectic, drawing upon various sources and combining disparate concepts to create new philosophical approaches.The anti-capitalist tradition of classical anarchism has remained prominent in contemporary currents.


Various anarchist groups, tendencies, and schools of thought exist today, making it difficult to describe the contemporary anarchist movement. While theorists and activists have established "relatively stable constellations of anarchist principles", there is no consensus on which principles are core and commentators describe multiple anarchisms, rather than a singular anarchism, in which common principles are shared between schools of anarchism while each group prioritises those principles differently. Gender equality can be a common principle, although it ranks as a higher priority to anarcha-feminists than anarcho-communists.


Anarchists are generally committed against coercive authority in all forms, namely "all centralized and hierarchical forms of government (e.g., monarchy, representative democracy, state socialism, etc.), economic class systems (e.g., capitalism, Bolshevism, feudalism, slavery, etc.), autocratic religions (e.g., fundamentalist Islam, Roman Catholicism, etc.), patriarchy, heterosexism, white supremacy, and imperialism." Anarchist schools disagree on the methods by which these forms should be opposed.


Tactics

Anarchists' tactics take various forms but in general serve two major goals, namely, to first oppose the Establishment and secondly to promote anarchist ethics and reflect an anarchist vision of society, illustrating the unity of means and ends. A broad categorisation can be made between aims to destroy oppressive states and institutions by revolutionary means on one hand and aims to change society through evolutionary means on the other. Evolutionary tactics embrace nonviolence and take a gradual approach to anarchist aims, although there is significant overlap between the two.


Anarchist tactics have shifted during the course of the last century. Anarchists during the early 20th century focused more on strikes and militancy while contemporary anarchists use a broader array of approaches.


Classical era

During the classical era, anarchists had a militant tendency. Not only did they confront state armed forces, as in Spain and Ukraine, but some of them also employed terrorism as propaganda of the deed. Assassination attempts were carried out against heads of state, some of which were successful. Anarchists also took part in revolutions. Many anarchists, especially the Galleanists, believed that these attempts would be the impetus for a revolution against capitalism and the state. Many of these attacks were done by individual assailants and the majority took place in the late 1870s, the early 1880s and the 1890s, with some still occurring in the early 1900s. Their decrease in prevalence was the result of further judicial power and of targeting and cataloguing by state institutions.


Anarchist perspectives towards violence have always been controversial. Anarcho-pacifists advocate for non-violence means to achieve their stateless, nonviolent ends. Other anarchist groups advocate direct action, a tactic which can include acts of sabotage or terrorism. This attitude was quite prominent a century ago when seeing the state as a tyrant and some anarchists believing that they had every right to oppose its oppression by any means possible. Emma Goldman and Errico Malatesta, who were proponents of limited use of violence, stated that violence is merely a reaction to state violence as a necessary evil.


Anarchists took an active role in strike actions, although they tended to be antipathetic to formal syndicalism, seeing it as reformist. They saw it as a part of the movement which sought to overthrow the state and capitalism. Anarchists also reinforced their propaganda within the arts, some of whom practised naturism and nudism. Those anarchists also built communities which were based on friendship and were involved in the news media.


Karl Marx, considered to be one of the principal founders of Marxism, criticised anarchism as the movement of the "petty bourgeois", i.e. of formerly self-employed craftsmen and artisans who had been ruined by capitalistic industrialisation or even war and then driven to factories; even so, they refused to subject themselves to factory discipline, party leadership, and State control, were prone to violence when frustrated, and advocated seizing factories only to break down mass production and return to craftsmanship. Friedrich Engels, who is considered Marxism's other principal founder, criticised anarchism's anti-authoritarianism as inherently counter-revolutionary because in his view a revolution is by itself authoritarian. A Socialist Workers Party pamphlet by  John Molyneux, Anarchism: A Marxist Criticism argues that "anarchism cannot win", believing that it lacks the ability to properly implement its ideas. Another Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to this Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.


Revolutionary and insurrectionary

In the current era, Italian anarchist Alfredo Bonanno, a proponent of insurrectionary anarchism, has reinstated the debate on violence by rejecting the nonviolence tactic adopted since the late 19th century by Kropotkin and other prominent anarchists afterwards. Both Bonanno and the French group The Invisible Committee advocate for small, informal affiliation groups, where each member is responsible for their own actions but works together to bring down oppression using sabotage and other violent means against state, capitalism, and other enemies. Members of The Invisible Committee were arrested in 2008 on various charges, terrorism included.


Overall, contemporary anarchists are much less violent and militant than their ideological ancestors. They mostly engage in confronting the police during demonstrations and riots, especially in countries such as Canada, Greece, and Mexico. Militant black bloc protest groups are known for clashing with the police; however, anarchists not only clash with state operators, they also engage in the struggle against fascists, racists, and other bigots, taking anti-fascist action and mobilising to prevent hate rallies from happening.


Evolutionary

Anarchists commonly employ direct action. This can take the form of disrupting and protesting against unjust hierarchy, or the form of self-managing their lives through the creation of counter-institutions such as communes and non-hierarchical collectives. Decision-making is often handled in an anti-authoritarian way, with everyone having equal say in each decision, an approach known as horizontalism. Contemporary-era anarchists have been engaging with various grassroots movements that are more or less based on horizontalism, although not explicitly anarchist, respecting personal autonomy and participating in mass activism such as strikes and demonstrations. In contrast with the "big-A Anarchism" of the classical era, the newly coined term "small-a anarchism" signals their tendency not to base their thoughts and actions on classical-era anarchism or to refer to classical anarchists such as Peter Kropotkin and Pierre-Joseph Proudhon to justify their opinions. Those anarchists would rather base their thought and praxis on their own experience, which they will later theorise.


The concept of prefigurative politics is enacted by many contemporary anarchist groups, striving to embody the principles, organisation and tactics of the changed social structure they hope to bring about. As part of this the decision-making process of small anarchist affinity groups plays a significant tactical role. Anarchists have employed various methods to build a rough consensus among members of their group without the need of a leader or a leading group. One way is for an individual from the group to play the role of facilitator to help achieve a consensus without taking part in the discussion themselves or promoting a specific point. Minorities usually accept rough consensus, except when they feel the proposal contradicts anarchist ethics, goals and values. Anarchists usually form small groups (5–20 individuals) to enhance autonomy and friendships among their members. These kinds of groups more often than not interconnect with each other, forming larger networks. Anarchists still support and participate in strikes, especially wildcat strikes as these are leaderless strikes not organised centrally by a syndicate.


As in the past, newspapers and journals are used, and anarchists have gone online to spread their message. Anarchists have found it easier to create websites because of distributional and other difficulties, hosting electronic libraries and other portals. Anarchists were also involved in developing various software that are available for free. The way these hacktivists work to develop and distribute resembles the anarchist ideals, especially when it comes to preserving users' privacy from state surveillance.


Anarchists organise themselves to squat and reclaim public spaces. During important events such as protests and when spaces are being occupied, they are often called Temporary Autonomous Zones (TAZ), spaces where art, poetry, and surrealism are blended to display the anarchist ideal. As seen by anarchists, squatting is a way to regain urban space from the capitalist market, serving pragmatical needs and also being an exemplary direct action. Acquiring space enables anarchists to experiment with their ideas and build social bonds. Adding up these tactics while having in mind that not all anarchists share the same attitudes towards them, along with various forms of protesting at highly symbolic events, make up a carnivalesque atmosphere that is part of contemporary anarchist vividity.


Key issues

As anarchism is a philosophy that embodies many diverse attitudes, tendencies, and schools of thought, disagreement over questions of values, ideology, and tactics is common. Its diversity has led to widely different uses of identical terms among different anarchist traditions which has created a number of definitional concerns in anarchist theory. The compatibility of capitalism, nationalism, and religion with anarchism is widely disputed, and anarchism enjoys complex relationships with ideologies such as communism, collectivism, Marxism, and trade unionism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest, veganism, or any number of alternative ethical doctrines. Phenomena such as civilisation, technology (e.g. within anarcho-primitivism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.


The state

Objection to the state and its institutions is a sine qua non of anarchism. Anarchists consider the state as a tool of domination and believe it to be illegitimate regardless of its political tendencies. Instead of people being able to control the aspects of their life, major decisions are taken by a small elite. Authority ultimately rests solely on power, regardless of whether that power is open or transparent, as it still has the ability to coerce people. Another anarchist argument against states is that the people constituting a government, even the most altruistic among officials, will unavoidably seek to gain more power, leading to corruption. Anarchists consider the idea that the state is the collective will of the people to be an unachievable fiction due to the fact that the ruling class is distinct from the rest of society.


One of the earliest criticisms is that anarchism defies and fails to understand the biological inclination to authority. Joseph Raz states that the acceptance of authority implies the belief that following their instructions will afford more success. Raz believes that this argument is true in following both authorities' successful and mistaken instruction. Anarchists reject this criticism because challenging or disobeying authority does not entail the disappearance of its advantages by acknowledging authority such as doctors or lawyers as reliable, nor does it involve a complete surrender of independent judgement. Anarchist perception of human nature, rejection of the state, and commitment to social revolution has been criticised by academics as naive, overly simplistic, and unrealistic, respectively. Classical anarchism has been criticised for relying too heavily on the belief that the abolition of the state will lead to human cooperation prospering.


Specific anarchist attitudes towards the state vary. Robert Paul Wolff believed that the tension between authority and autonomy would mean the state could never be legitimate. Bakunin saw the state as meaning "coercion, domination by means of coercion, camouflaged if possible but unceremonious and overt if need be." A. John Simmons and Leslie Green, who leaned toward philosophical anarchism, believed that the state could be legitimate if it is governed by consensus, although they saw this as highly unlikely. Beliefs on how to abolish the state also differ.


A counterpoint of anarchism is the assertion that humans cannot self-govern and so a state is necessary for human survival; this critique was supported by Philosopher Bertrand Russell, stating that "eace and war, tariffs, regulations of sanitary conditions and the sale of noxious drugs, the preservation of a just system of distribution: these, among others, are functions which could hardly be performed in a community in which there was no central government." Another common criticism of anarchism is that it fits a world of isolation in which only the small enough entities can be self-governing; a response would be that major anarchist thinkers advocated anarchist federalism.


In Anarchy, State, and Utopia, the philosopher Robert Nozick argued that a "night-watchman state", or minarchy, would emerge from anarchy through the process of an invisible hand, in which people would exercise their liberty and buy protection from protection agencies, evolving into a minimal state. Anarchists reject these criticisms by arguing that humans in a state of nature would not just be in a state of war. Anarcho-primitivists in particular argue that humans were better off in a state of nature in small tribes living close to the land, while anarchists in general argue that the negatives of state organisation, such as hierarchies, monopolies and inequality, outweigh the benefits. Philosophy lecturer Andrew G. Fiala composed a list of common arguments against anarchism which includes critiques such as that anarchism is innately related to violence and destruction, not only in the pragmatic world, such as at protests, but in the world of ethics as well. Secondly, anarchism is evaluated as unfeasible or utopian since the state cannot be defeated practically. He says that this line of arguments most often calls for political action within the system to reform it. The third argument is that anarchism is self-contradictory as a ruling theory that has no ruling theory. Anarchism also calls for collective action while endorsing the autonomy of the individual, hence no collective action can be taken. Lastly, Fiala mentions a critique towards philosophical anarchism of being ineffective (all talk and thoughts) and in the meantime capitalism and bourgeois class remains strong.


Gender, sexuality, and free love

As gender and sexuality carry along them dynamics of hierarchy, many anarchists address, analyse, and oppose the suppression of one's autonomy imposed by gender roles.


Sexuality was not often discussed by classical anarchists but the few that did felt that an anarchist society would lead to sexuality naturally developing. Sexual violence was a concern for anarchists such as Benjamin Tucker, who opposed age-of-consent laws, believing they would benefit predatory men. A historical current that arose and flourished during 1890 and 1920 within anarchism was free love. In contemporary anarchism, this current survives as a tendency to support polyamory, relationship anarchy, and queer anarchism. Free love advocates were against marriage, which they saw as a way of men imposing authority over women, largely because marriage law greatly favoured the power of men. The notion of free love was much broader and included a critique of the established order that limited women's sexual freedom and pleasure. Those free love movements contributed to the establishment of communal houses, where large groups of travellers, anarchists and other activists slept in beds together. Free love had roots both in Europe and the United States; however, some anarchists struggled with the jealousy that arose from free love. Anarchist feminists were advocates of free love, against marriage, and pro-choice (using a contemporary term), and had a similar agenda. Anarchist and non-anarchist feminists differed on suffrage but were supportive of one another.


During the second half of the 20th century, anarchism intermingled with the second wave of feminism, radicalising some currents of the feminist movement and being influenced as well. By the latest decades of the 20th century, anarchists and feminists were advocating for the rights and autonomy of women, LGBT people, and other marginalised groups, with some feminist thinkers suggesting a fusion of the two currents. With the third wave of feminism, sexual identity and compulsory heterosexuality became a subject of study for anarchists, yielding a post-structuralist critique of sexual normality. Some anarchists distanced themselves from this line of thinking, suggesting that it leaned towards an individualism that was dropping the cause of social liberation.


Education

The interest of anarchists in education stretches back to the first emergence of classical anarchism. Anarchists consider proper education, one which sets the foundations of the future autonomy of the individual and the society, to be an act of mutual aid. Anarchist writers such as William Godwin (Political Justice) and Max Stirner ("The False Principle of Our Education") attacked both state education and private education as another means by which the ruling class replicate their privileges.


In 1901, Catalan anarchist and free thinker Francisco Ferrer established the Escuela Moderna in Barcelona as an opposition to the established education system which was dictated largely by the Catholic Church. Ferrer's approach was secular, rejecting both state and church involvement in the educational process while giving pupils large amounts of autonomy in planning their work and attendance. Ferrer aimed to educate the working class and explicitly sought to foster class consciousness among students. The school closed after constant harassment by the state and Ferrer was later arrested. Nonetheless, his ideas formed the inspiration for a series of modern schools around the world. Christian anarchist Leo Tolstoy, who published the essay Education and Culture, also established a similar school with its founding principle being that "for education to be effective it had to be free." In a similar token, A. S. Neill founded what became the Summerhill School in 1921, also declaring being free from coercion.


Anarchist education is based largely on the idea that a child's right to develop freely and without manipulation ought to be respected and that rationality would lead children to morally good conclusions; however, there has been little consensus among anarchist figures as to what constitutes manipulation. Ferrer believed that moral indoctrination was necessary and explicitly taught pupils that equality, liberty and social justice were not possible under capitalism, along with other critiques of government and nationalism.


Late 20th century and contemporary anarchist writers (Paul Goodman, Herbert Read, and Colin Ward) intensified and expanded the anarchist critique of state education, largely focusing on the need for a system that focuses on children's creativity rather than on their ability to attain a career or participate in consumerism as part of a consumer society. Contemporary anarchists such as Ward claim that state education serves to perpetuate socioeconomic inequality.


While few anarchist education institutions have survived to the modern-day, major tenets of anarchist schools, among them respect for child autonomy and relying on reasoning rather than indoctrination as a teaching method, have spread among mainstream educational institutions. Judith Suissa names three schools as explicitly anarchists' schools, namely the Free Skool Santa Cruz in the United States which is part of a wider American-Canadian network of schools, the Self-Managed Learning College in Brighton, England, and the Paideia School in Spain.


The arts

The connection between anarchism and art was quite profound during the classical era of anarchism, especially among artistic currents that were developing during that era such as futurists, surrealists and others. In literature, anarchism was mostly associated with the New Apocalyptics and the neo-romanticism movement. In music, anarchism has been associated with music scenes such as punk. Anarchists such as Leo Tolstoy and Herbert Read stated that the border between the artist and the non-artist, what separates art from a daily act, is a construct produced by the alienation caused by capitalism and it prevents humans from living a joyful life.


Other anarchists advocated for or used art as a means to achieve anarchist ends. In his book Breaking the Spell: A History of Anarchist Filmmakers, Videotape Guerrillas, and Digital Ninjas, Chris Robé claims that "anarchist-inflected practices have increasingly structured movement-based video activism." Throughout the 20th century, many prominent anarchists (Peter Kropotkin, Emma Goldman, Gustav Landauer and Camillo Berneri) and publications such as Anarchy wrote about matters pertaining to the arts.


Three overlapping properties made art useful to anarchists. It could depict a critique of existing society and hierarchies, serve as a prefigurative tool to reflect the anarchist ideal society and even turn into a means of direct action such as in protests. As it appeals to both emotion and reason, art could appeal to the whole human and have a powerful effect. The 19th-century neo-impressionist movement had an ecological aesthetic and offered an example of an anarchist perception of the road towards socialism. In Les chataigniers a Osny by anarchist painter Camille Pissarro, the blending of aesthetic and social harmony is prefiguring an ideal anarchistic agrarian community.


See also

Anarchist communities


References

Explanatory notes

Citations

General and cited sources

Primary sources

Secondary sources

Tertiary sources

Further reading

External links







Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years.


Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common. Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline. Higher temperatures are also causing more intense storms, droughts, and other weather extremes. Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct. Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.


Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result. The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Societies and ecosystems will experience more severe risks without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached. Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.


Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60 °C (2.88 °F) since regular tracking began in 1850. Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet. Under the 2015 Paris Agreement, nations collectively agreed to keep warming "well under 2 °C". However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century.


There is widespread support for climate action worldwide, and most countries aim to stop emitting carbon dioxide. Fossil fuels can be phased out by stopping subsidising them, conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power. Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes. Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that store carbon in soil.


Terminology

Before the 1980s, it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. In the 1980s, the terms global warming and climate change became more common, often being used interchangeably. Scientifically, global warming refers only to increased global average surface temperature, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes.


Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history as result of natural processes. The term anthropogenic climate change is sometimes used to describe climate change resulting from human activities.


Global warming—used as early as 1975—became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate. Since the 2000s, usage of climate change has increased. Various scientists, politicians and media may use the terms climate crisis or climate emergency to talk about climate change, and may use the term global heating instead of global warming.


Global temperature rise

Temperatures prior to present-day global warming

Over the last few million years the climate cycled through ice ages. One of the hotter periods was the Last Interglacial, around 125,000 years ago, where temperatures were between 0.5 °C and 1.5 °C warmer than before the start of global warming. This period saw sea levels 5 to 10 metres higher than today. The most recent glacial maximum 20,000 years ago was some 5–7 °C colder. This period has sea levels that were over 125 metres (410 ft) lower than today.


Temperatures stabilized in the current interglacial period beginning 11,700 years ago. This period also saw the start of agriculture. Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions. Climate information for that period comes from climate proxies, such as trees and ice cores.


Warming since the Industrial Revolution

Around 1850 thermometer records began to provide global coverage.
Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.


Ongoing changes in climate have had no precedent for several thousand years. Multiple datasets all show worldwide increases in surface temperature, at a rate of around 0.2 °C per decade. The 2014–2023 decade warmed to an average 1.19 °C  compared to the pre-industrial baseline (1850–1900). Not every single year was warmer than the last: internal climate variability processes can make any year 0.2 °C warmer or colder than the average. From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO) and Atlantic Multidecadal Oscillation (AMO) caused a short slower period of warming called the "global warming hiatus". After the "hiatus", the opposite occurred, with 2024 well above the recent average at more than +1.5 °C. This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal.: 5 


A wide range of other observations reinforce the evidence of warming. The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space. Warming reduces average snow cover and forces the retreat of glaciers. At the same time, warming also causes greater evaporation from the oceans, leading to more atmospheric humidity, more and heavier precipitation. Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas.


Differences by region

Different regions of the world warm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature. This is because oceans lose more heat by evaporation and oceans can store a lot of heat. The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean. The rest has heated the atmosphere, melted ice, and warmed the continents.


The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat. Local black carbon deposits on snow and ice also contribute to Arctic warming. Arctic surface temperatures are increasing between three and four times faster than in the rest of the world. Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation, which further changes the distribution of heat and precipitation around the globe.


Future global temperatures

The World Meteorological Organization estimates there is almost a 50% chance of the five-year average global temperature exceeding +1.5 °C between 2024 and 2028. The IPCC expects the 20-year average to exceed +1.5 °C in the early 2030s.


The IPCC Sixth Assessment Report (2021) included projections that by 2100 global warming is very likely to reach 1.0–1.8 °C under a scenario with very low emissions of greenhouse gases, 2.1–3.5 °C under an intermediate emissions scenario,
or 3.3–5.7 °C under a very high emissions scenario. The warming will continue past 2100 in the intermediate and high emission scenarios, with future projections of global surface temperatures by year 2300 being similar to millions of years ago.


The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases. According to UNEP, global warming can be kept below 2.0 °C with a 50% chance if emissions after 2023 do not exceed 900 gigatonnes of CO2. This carbon budget corresponds to around 16 years of current emissions.


Causes of recent global temperature rise

The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Niño events cause short-term spikes in surface temperature while La Niña events cause short term cooling. Their relative frequency can affect global temperature trends on a decadal timescale. Other changes are caused by an imbalance of energy from external forcings. Examples of these include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.


To determine the human contribution to climate change, unique "fingerprints" for all potential causes are developed and compared with both observed patterns and known internal climate variability. For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed. Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful.


Greenhouse gases

Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time.


While water vapour (≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone, CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures.


Before the Industrial Revolution, naturally occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence. Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas), has increased the amount of greenhouse gases in the atmosphere. In 2022, the concentrations of CO2 and methane had increased by about 50% and 164%, respectively, since 1750. These CO2 levels are higher than they have been at any time during the last 14 million years. Concentrations of methane are far higher than they were over the last 800,000 years.


Global human-caused greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases. CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity. Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminium, and fertilizer. Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction. Nitrous oxide emissions largely come from the microbial decomposition of fertilizer.


While methane only lasts in the atmosphere for an average of 12 years, CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays. Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. The ocean has absorbed 20 to 30% of emitted CO2 over the last two decades. CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete.


Land surface changes

Around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land. Deforestation is the main land use change contributor to global warming, as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink. Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink.


Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns. In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler. At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains. Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect.


Other factors

Aerosols and clouds

Air pollution, in the form of aerosols, affects the climate on a large scale. Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming, and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel. Smaller contributions come from black carbon (from combustion of fossil fuels and biomass), and from dust. Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.


Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight. Indirect effects of aerosols are the largest uncertainty in radiative forcing.


While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050. The effect of decreasing sulfur content of fuel oil for ships since 2020 is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050.


Solar and volcanic activity

As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system. Solar irradiance has been measured directly by satellites, and indirect measurements are available from the early 1600s onwards. Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere). The upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling.
This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere.


Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures. These impacts on temperature only last for several years, because both water vapour and volcanic material have low persistence in the atmosphere. volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions. Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution.


Climate change feedbacks

The climate system's response to an initial forcing is shaped by feedbacks, which either amplify or dampen the change. Self-reinforcing or positive feedbacks increase the response, while balancing or negative feedbacks reduce it. The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and the net cloud feedback. The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth. Feedbacks are expected to trend in a positive direction as greenhouse gas emissions continue, raising climate sensitivity.


These feedback processes alter the pace of global warming. For instance, warmer air can hold more moisture in the form of water vapour, which is itself a potent greenhouse gas. Warmer air can also make clouds higher and thinner, and therefore more insulating, increasing climate warming. The reduction of snow cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region and accelerates Arctic warming. This additional warming also contributes to permafrost thawing, which releases methane and CO2 into the atmosphere.


Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%. This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer. The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution. Uncertainty over feedbacks, particularly cloud cover, is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.


Modelling

A climate model is a representation of the physical, chemical and biological processes that affect the climate system. Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks. Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.


The physical realism of models is tested by examining their ability to simulate current or past climates. Past models have underestimated the rate of Arctic shrinkage and underestimated the rate of precipitation increase. Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. The 2017 United States-published National Climate Assessment notes that "climate models may still be underestimating or missing relevant feedback processes". Additionally, climate models may be unable to adequately predict short-term regional climatic shifts.


A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth, and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change. Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm.


Impacts

Environmental effects

The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. Extremely wet or dry events within the monsoon period have increased in India and East Asia. Monsoonal precipitation over the Northern Hemisphere has increased since 1980. The rainfall rate and intensity of hurricanes and typhoons is likely increasing, and the geographic range likely expanding poleward in response to climate warming. The frequency of tropical cyclones has not increased as a result of climate change.


Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets. Sea level rise has increased over time, reaching 4.8 cm per decade between 2014 and 2023. Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario. Marine ice sheet instability processes in Antarctica may add substantially to these values, including the possibility of a 2-meter sea level rise by 2100 under high emissions.


Climate change has led to decades of shrinking and thinning of the Arctic sea ice. While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C. Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic. Because oxygen is less soluble in warmer water, its concentrations in the ocean are decreasing, and dead zones are expanding.


Tipping points and long-term impacts

Greater degrees of global warming increase the risk of passing through 'tipping points'—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state. For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place. While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades. The collapse of the AMOC would be a severe climate catastrophe, resulting in a cooling of the Northern Hemisphere.


The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, ocean acidification and ocean deoxygenation. The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime. The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years. Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years. Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date. Further, the West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years.


Nature and wildlife

Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5 km/year over the past 55 years. Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. A related phenomenon driven by climate change is woody plant encroachment, affecting up to 500 million hectares globally. Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics. The size and speed of global warming is making abrupt changes in ecosystems more likely. Overall, it is expected that climate change will result in the extinction of many species.


The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds. Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs. Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life. Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts. Plants have come under increased stress from damage by insects.


Humans

The effects of climate change are impacting humans everywhere in the world. Impacts can be observed on all continents and ocean regions, with low-latitude, less developed areas facing the greatest risk. Continued warming has potentially "severe, pervasive and irreversible impacts" for people and ecosystems. The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.


Health and food

The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Scientists have warned about the irreversible harms it poses. Extreme weather events affect public health, and food and water security. Temperature extremes lead to increased illness and death. Climate change increases the intensity and frequency of extreme weather events. It can affect transmission of infectious diseases, such as dengue fever and malaria. According to the World Economic Forum, 14.5 million more deaths are expected due to climate change by 2050. 30% of the global population currently live in areas where extreme heat and humidity are already associated with excess deaths. By 2100, 50% to 75% of the global population would live in such areas.


While total crop yields have been increasing in the past 50 years due to agricultural improvements, climate change has already decreased the rate of yield growth. Fisheries have been negatively affected in multiple regions. While agricultural productivity has been positively affected in some high latitude areas, mid- and low-latitude areas have been negatively affected. According to the World Economic Forum, an increase in drought in certain regions could cause 3.2 million deaths from malnutrition by 2050 and stunting in children. With 2 °C warming, global livestock headcounts could decline by 7–10% by 2050, as less animal feed will be available. If the emissions continue to increase for the rest of century, then over 9 million climate-related deaths would occur annually by 2100.


Livelihoods and inequality

Economic damages due to climate change may be severe and there is a chance of disastrous consequences. Severe impacts are expected in South-East Asia and sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources. Heat stress can prevent outdoor labourers from working. If warming reaches 4 °C then labour capacity in those regions could be reduced by 30 to 50%. The World Bank estimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation.


Inequalities based on wealth and social status have worsened due to climate change. Major difficulties in mitigating, adapting to, and recovering from climate shocks are faced by marginalized people who have less control over resources. Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change. An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities.


While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience. For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress.


Climate migration

Low-lying islands and coastal communities are threatened by sea level rise, which makes urban flooding more common. Sometimes, land is permanently lost to the sea. This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu. In some regions, the rise in temperature and humidity may be too severe for humans to adapt to. With worst-case climate change, models project that areas almost one-third of humanity live in might become Sahara-like uninhabitable and extremely hot climates.


These factors can drive climate or environmental migration, within and between countries. More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to "trapped populations" who are not able to move due to a lack of resources.


Reducing and recapturing emissions

Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere. To limit global warming to less than 2 °C global greenhouse gas emissions need to be net-zero by 2070. This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.


The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2 °C. With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8 °C by the end of the century (range: 1.9–3.7 °C, depending on exact implementation and technological progress). When only considering current policies, this raises to 3.1 °C. Globally, limiting warming to 2 °C may result in higher economic benefits than economic costs.


Although there is no single pathway to limit global warming to 2 °C, most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions. To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry, such as preventing deforestation and restoring natural ecosystems by reforestation.


Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century. There are concerns, though, about over-reliance on these technologies, and environmental impacts.


Solar radiation modification (SRM) is a proposal for reducing global warming by reflecting some sunlight away from Earth and back into space. Because it does not reduce greenhouse gas concentrations, it would not address ocean acidification and is not considered mitigation. SRM should be considered only as a supplement to mitigation, not a replacement for it,  due to risks such as rapid warming if it were abruptly stopped and not restarted. The most-studied approach is stratospheric aerosol injection. SRM could reduce global warming and some of its impacts, though imperfectly. It poses environmental risks, such as changes to rainfall patterns, as well as political challenges, such as who would decide whether to use it.


Clean energy

Renewable energy is key to limiting climate change. For decades, fossil fuels have accounted for roughly 80% of the world's energy use. The remaining share has been split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy). Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions. Renewables represented 86% of all new electricity generation installed in 2023. Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.


While solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations, green energy policies are needed to achieve a rapid transition from fossil fuels to renewables. To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.


Electricity generated from renewable sources would also need to become the main energy source for heating and transport. Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking). For shipping and flying, low-carbon fuels would reduce emissions. Heating could be increasingly decarbonized with technologies like heat pumps.


There are obstacles to the continued rapid growth of clean energy, including renewables. Wind and solar produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and fossil fuel power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs. Bioenergy is often not carbon-neutral and may have negative consequences for food security. The growth of nuclear power is constrained by controversy around radioactive waste, nuclear weapon proliferation, and accidents. Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.


Low-carbon energy improves human health by minimizing climate change as well as reducing air pollution deaths, which were estimated at 7 million annually in 2016. Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty. Improving air quality also has economic benefits which may be larger than mitigation costs.


Energy conservation

Reducing energy demand is another major aspect of reducing emissions. If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimizes carbon-intensive infrastructure development. Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy. Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.


Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles. Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes. In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting. The use of technologies like heat pumps can also increase building energy efficiency.


Agriculture and industry

 Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand. A set of actions could reduce agriculture and forestry-based emissions by two-thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.


On the demand side, a key component of reducing emissions is shifting people towards plant-based diets. Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use. Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.


Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries. Where energy production or CO2-intensive heavy industries continue to produce waste CO2, technology can sometimes be used to capture and store most of the gas instead of releasing it to the atmosphere. This technology, carbon capture and storage (CCS), could have a critical but limited role in reducing emissions. It is relatively expensive and has been deployed only to an extent that removes around 0.1% of annual greenhouse gas emissions.


Carbon dioxide removal

Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels. Reforestation and afforestation (planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns. Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments. Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction. Restoration/recreation of coastal wetlands, prairie plots and seagrass meadows increases the uptake of carbon into organic matter. When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.


The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere. It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5 °C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.


Adaptation

Adaptation is "the process of adjustment to current or expected changes in climate and its effects".: 5  Without additional mitigation, adaptation cannot avert the risk of "severe, widespread and irreversible" impacts. More severe climate change requires more transformative adaptation, which can be prohibitively expensive. The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less. The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.


Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and building flood controls. If that fails, managed retreat may be needed. There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody. In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes. Education, migration and early warning systems can reduce climate vulnerability. Planting mangroves or encouraging other coastal vegetation can buffer storms.


Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favourable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.


There are synergies but also trade-offs between adaptation and mitigation. An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation. An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compact urban development may reduce emissions from transport and construction, but may also increase the urban heat island effect, exposing people to heat-related health risks.


Policies and politics

Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness. Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized in Sustainable Development Goal 13 which is to "take urgent action to combat climate change and its impacts". The goals on food, clean water and ecosystem protection have synergies with climate mitigation.


The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. Sometimes mitigation also has localized benefits though. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions. Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell.


Policy options

A wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions. Carbon can be priced with carbon taxes and emissions trading systems. Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in. Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths. Money saved on fossil subsidies could be used to support the transition to clean energy instead. More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry. Several countries require utilities to increase the share of renewables in power production. An Open Coalition on Compliance Carbon Markets with the aim of creating a global cap and trade system was established at COP30 (2025). According to some calculations it can increase emissions reduction seven-fold over current policies, deliver $200 billion per year for clean-energy and social programs and even close the gap between current emissions trajectory and the goals of the Paris agreement.


Climate justice

Policy designed through the lens of climate justice tries to address human rights issues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt.


Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%. Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulative climate reparations of $5.4 trillion over the period 2025–2050. To achieve a just transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments.


International climate agreements

Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC). The goal of the UNFCCC is to prevent dangerous human interference with the climate system. As stated in the convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained. The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed. Its yearly conferences are the stage of global negotiations.


The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions. During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to " the lead" in reducing their emissions, since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.


The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77. Associated parties aimed to limit the global temperature rise to below 2 °C.  The accord set the goal of sending $100 billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund. As of 2020, only 83.3 billion were delivered. Only in 2023 the target is expected to be achieved.


In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0 °C and contains an aspirational goal of keeping warming under 1.5 °C. The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years. The Paris Agreement restated that developing countries must be financially supported. As of March 2025, 194 states and the European Union have acceded to or ratified the agreement.


The 1987 Montreal Protocol, an international agreement to phase out production of ozone-depleting gases, has had benefits for climate change mitigation. Several ozone-depleting gases like chlorofluorocarbons are powerful greenhouse gases, so banning their production and usage may have avoided a temperature rise of 0.5 °C–1.0 °C, as well as additional warming by preventing damage to vegetation from ultraviolet radiation. It is estimated that the agreement has been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so. The most recent amendment to the Montreal Protocol, the 2016 Kigali Amendment, committed to reducing the emissions of hydrofluorocarbons, which served as a replacement for banned ozone-depleting gases and are also potent greenhouse gases. Should countries comply with the amendment, a warming of 0.3 °C–0.5 °C is estimated to be avoided.


National responses

In 2019, the United Kingdom parliament became the first national government to declare a climate emergency. Other countries and jurisdictions followed suit. That same year, the European Parliament declared a "climate and environmental emergency". The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050. In 2021, the European Commission released its "Fit for 55" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035.


Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060. While India has strong incentives for renewables, it also plans a significant expansion of coal in the country. Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter.


As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.


Society and culture

Denial and misinformation

Public debate about climate change has been strongly affected by climate change denial and misinformation, which first emerged in the United States and has since spread to other countries, particularly Canada and Australia. It originated from fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists. Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results. People who hold unwarranted doubt about climate change are sometimes called climate change "skeptics", although "contrarians" or "deniers" are more appropriate terms.


There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimize the negative impacts of climate change. Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community to delay policy changes. Strategies to promote these ideas include criticism of scientific institutions, and questioning the motives of individual scientists. An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.


Public awareness and opinion

Climate change came to international public attention in the late 1980s. Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion. In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change.


Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat. College biology textbooks from the 2010s featured less content on climate change compared to those from the preceding decade, with decreasing emphasis on solutions. Partisan gaps also exist in many countries, and countries with high CO2 emissions tend to be less concerned. Views on causes of climate change vary widely between countries. Media coverage linked to protests has had impacts on public sentiment as well as on which aspects of climate change are focused upon. Higher levels of worry are associated with stronger public support for policies that address climate change. Concern has increased over time, and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency. A 2024 survey across 125 countries found that 89% of the global population demanded intensified political action, but systematically underestimated other peoples' willingness to act.


Climate movement

Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities. Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish activist and then-teenager Greta Thunberg. Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport.


Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change. Lawsuits against fossil-fuel companies generally seek compensation for loss and damage. On 23 July 2025, the UN's International Court of Justice issued its advisory opinion, saying explicitly that states must act to stop climate change, and if they fail to accomplish that duty, other states can sue them. This obligation includes implementing their commitments in international agreements they are parties to, such as the 2015 Paris Climate Accord.


History

Early discoveries

Scientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change. In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.
In 1856 Eunice Newton Foote demonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). In "Circumstances Affecting the Heat of the Sun's Rays" she concluded that "n atmosphere of that gas would give to our earth a high temperature".


Starting in 1859, John Tyndall established that nitrogen and oxygen—together totalling 99% of dry air—are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages.


Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5–6 °C. Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating. Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising, but his calculations met the same objections.


Development of a scientific consensus

In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase—the "Keeling Curve"—which was part of continued scientific investigation through the 1960s into possible human causation of global warming. Studies such as the National Research Council's 1979 Charney Report supported the accuracy of climate models that forecast significant warming. Human causation of observed global warming and dangers of unmitigated warming were publicly presented in James Hansen's 1988 testimony before a US Senate committee. The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research. As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles.


There is a nearly unanimous scientific consensus that the climate is warming and that this is caused by human activities. No scientific body of national or international standing disagrees with this view. As of 2019, agreement in recent literature reached over 99%. The 2021 IPCC Assessment Report stated that it is "unequivocal" that climate change is caused by humans. Consensus has further developed that action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions.


Recent developments

Extreme event attribution (EEA), also known as attribution science, was developed in the early decades of the 21st century. EEA uses climate models to identify and quantify the role that human-caused climate change plays in the frequency, intensity, duration, and impacts of specific individual extreme weather events. Results of attribution studies allow scientists and journalists to make statements such as, "this weather event was made at least n times more likely by human-caused climate change" or "this heatwave was made m degrees hotter than it would have been in a world without global warming" or "this event was effectively impossible without climate change". Greater computing power in the 2000s and conceptual breakthroughs in the early to mid 2010s  enabled attribution science to detect the effects of climate change on some events with high confidence. Scientists use attribution methods and climate simulations that have already been peer reviewed, allowing "rapid attribution studies" to be published within a "news cycle" time frame after weather events.


References

Sources

 This article incorporates text from a free content work. Licensed under CC BY-SA 3.0. Text taken from The status of women in agrifood systems – Overview, FAO, FAO.


IPCC reports

Fourth Assessment Report


Fifth Assessment report





Special Report: Global Warming of 1.5 °C


Special Report: Climate change and Land


Special Report: The Ocean and Cryosphere in a Changing Climate


Sixth Assessment Report


Other peer-reviewed sources

Books, reports and legal documents

Non-technical sources

External links




A conscience is a cognitive process that elicits emotion and rational associations based on an individual's moral philosophy or value system. Conscience is not an elicited emotion or thought produced by associations based on immediate sensory perceptions and reflexive responses, as in sympathetic central nervous system responses. In common terms, conscience is often described as leading to feelings of remorse when a person commits an act that conflicts with their moral values. The extent to which conscience informs moral judgment before an action and whether such moral judgments are or should be based on reason has occasioned debate through much of modern history between theories of basics in ethic of human life in juxtaposition to the theories of romanticism and other reactionary movements after the end of the Middle Ages.


Religious views of conscience usually see it as linked to a morality inherent in all humans, to a beneficent universe and/or to divinity. The diverse ritualistic, mythical, doctrinal, legal, institutional and material features of religion may not necessarily cohere with experiential, emotive, spiritual or contemplative considerations about the origin and operation of conscience. Common secular or scientific views regard the capacity for conscience as probably genetically determined, with its subject probably learned or imprinted as part of a culture.


Commonly used metaphors for conscience include the "voice within", the "inner light", or even Socrates' reliance on what the Greeks called his "daimōnic sign", an averting (ἀποτρεπτικός apotreptikos) inner voice heard only when he was about to make a mistake. Conscience, as is detailed in sections below, is a concept in national and international law, is increasingly conceived of as applying to the world as a whole, has motivated numerous notable acts for the public good and been the subject of many prominent examples of literature, music and film.


Views

Religious

In the literary traditions of the Upanishads, Brahma Sutras and the Bhagavad Gita, conscience is the label given to attributes composing knowledge about good and evil, that a soul acquires from the completion of acts and consequent accretion of karma over many lifetimes. According to Adi Shankara in his Vivekachudamani morally right action (characterised as humbly and compassionately performing the primary duty of good to others without expectation of material or spiritual reward), helps "purify the heart" and provide mental tranquility but it alone does not give us "direct perception of the Reality". This knowledge requires discrimination between the eternal and non-eternal and eventually a realization in contemplation that the true self merges in a universe of pure consciousness.


In the Zoroastrian faith, after death a soul must face judgment at the Bridge of the Separator; there, evil people are tormented by prior denial of their own higher nature, or conscience, and "to all time will they be guests for the House of the Lie." The Chinese concept of Ren, indicates that conscience, along with social etiquette and correct relationships, assist humans to follow The Way (Tao) a mode of life reflecting the implicit human capacity for goodness and harmony.


Conscience also features prominently in Buddhism. In the Pali scriptures, for example, Buddha links the positive aspect of conscience to a pure heart and a calm, well-directed mind. It is regarded as a spiritual power, and one of the "Guardians of the World". The Buddha also associated conscience with compassion for those who must endure cravings and suffering in the world until right conduct culminates in right mindfulness and right contemplation. Santideva (685–763 CE) wrote in the Bodhicaryavatara (which he composed and delivered in the great northern Indian Buddhist university of Nalanda) of the spiritual importance of perfecting virtues such as generosity, forbearance and training the awareness to be like a "block of wood" when attracted by vices such as pride or lust; so one can continue advancing towards right understanding in meditative absorption. Conscience thus manifests in Buddhism as unselfish love for all living beings which gradually intensifies and awakens to a purer awareness where the mind withdraws from sensory interests and becomes aware of itself as a single whole.


The Roman Emperor Marcus Aurelius wrote in his Meditations that conscience was the human capacity to live by rational principles that were congruent with the true, tranquil and harmonious nature of our mind and thereby that of the Universe: "To move from one unselfish action to another with God in mind. Only there, delight and stillness ... the only rewards of our existence here are an unstained character and unselfish acts."


The Islamic concept of Taqwa is closely related to conscience. In the Qur’ān verses 2:197 & 22:37 Taqwa refers to "right conduct" or "piety", "guarding of oneself" or "guarding against evil". Qur’ān verse 47:17 says that God is the ultimate source of the believer's taqwā which is not simply the product of individual will but requires inspiration from God. In Qur’ān verses 91:7–8, God the Almighty talks about how He has perfected the soul, the conscience and has taught it the wrong (fujūr) and right (taqwā). Hence, the awareness of vice and virtue is inherent in the soul, allowing it to be tested fairly in the life of this world and tried, held accountable on the day of judgment for responsibilities to God and all humans.


Qur’ān verse 49:13 states: "O humankind! We have created you out of male and female and constituted you into different groups and societies, so that you may come to know each other-the noblest of you, in the sight of God, are the ones possessing taqwā." In Islam, according to eminent theologians such as Al-Ghazali, although events are ordained (and written by God in al-Lawh al-Mahfūz, the Preserved Tablet), humans possess free will to choose between wrong and right and are thus responsible for their actions; the conscience being a dynamic personal connection to God enhanced by knowledge and practise of the Five Pillars of Islam, deeds of piety, repentance, self-discipline, and prayer; and disintegrated and metaphorically covered in blackness through sinful acts. Marshall Hodgson wrote the three-volume work: The Venture of Islam: Conscience and History in a World Civilization.


In the Protestant Christian tradition, Martin Luther insisted at the Diet of Worms that his conscience was captive to the Word of God, and it was neither safe nor right to go against conscience. To Luther, conscience falls within the ethical, rather than the religious, sphere. John Calvin saw conscience as a battleground: "the enemies who rise up in our conscience against his Kingdom and hinder his decrees prove that God's throne is not firmly established therein". Many Christians regard following one's conscience as important as, or even more important than, obeying human authority. According to the Bible, as enunciated in Romans 2:15, conscience is the one bearing witness, accusing or excusing one another, so we would know when we break the law written in our hearts; the guilt we feel when we do something wrong tells us that we need to repent." This can sometimes (as with the conflict between William Tyndale and Thomas More over the translation of the Bible into English) lead to moral quandaries: "Do I unreservedly obey my Church/priest/military/political leader or do I follow my own inner feeling of right and wrong as instructed by prayer and a personal reading of scripture?" Some contemporary Christian churches and religious groups hold the moral teachings of the Ten Commandments or of Jesus as the highest authority in any situation, regardless of the extent to which it involves responsibilities in law. In the Gospel of John (7:53–8:11, King James Version), Jesus challenges those accusing a woman of adultery: "'He that is without sin among you, let him first cast a stone at her.' And again he stooped down, and wrote on the ground. And they which heard it, being convicted by their own conscience, went out one by one" (see Jesus and the woman taken in adultery). Of note, however, the word 'conscience' is not in the original New Testament Greek and is not in the vast majority of Bible versions. In the Gospel of Luke (10:25–37), Jesus tells the story of how a despised and heretical Samaritan (see Parable of the Good Samaritan) who (out of compassion or pity; the word 'conscience' is not used) helps an injured stranger beside a road, qualifies better for eternal life by loving his neighbor than a priest who passes by on the other side.


This dilemma of obedience in conscience to divine or state law, was demonstrated dramatically in Antigone's defiance of King Creon's order against burying her brother an alleged traitor, appealing to the "unwritten law" and to a "longer allegiance to the dead than to the living".


Catholic theology sees conscience as the last practical "judgment of reason which at the appropriate moment enjoins  to do good and to avoid evil". The Second Vatican Council (1962–65) describes: "Deep within his conscience man discovers a law which he has not laid upon himself but which he must obey. Its voice, ever calling him to love and to do what is good and to avoid evil, tells him inwardly at the right movement: do this, shun that. For man has in his heart a law inscribed by God. His dignity lies in observing this law, and by it he will be judged. His conscience is man’s most secret core, and his sanctuary. There he is alone with God whose voice echoes in his depths." Thus, conscience is not like the will, nor a habit like prudence, but "the interior space in which we can listen to and hear the truth, the good, the voice of God. It is the inner place of our relationship with Him, who speaks to our heart and helps us to discern, to understand the path we ought to take, and once the decision is made, to move forward, to remain faithful". In terms of logic, conscience can be viewed as the practical conclusion of a moral syllogism whose major premise is an objective norm and whose minor premise is a particular case or situation to which the norm is applied. Thus, Catholics are taught to carefully educate themselves as to revealed norms and norms derived therefrom, so as to form a correct conscience. Catholics are also to examine their conscience daily and with special care before confession. Catholic teaching holds that, "Man has the right to act according to his conscience and in freedom so as personally to make moral decisions. He must not be forced to act contrary to his conscience. Nor must he be prevented from acting according to his conscience, especially in religious matters". This right of Conscience allows one to form their Morality from sincere and traditional sources and form their opinions from therein. Thus, the Church teaches that one must form their morality and then follow it to the best of their ability. Nevertheless it is taught in more than one area, that the conscience can, and sometimes should, stand against the teaching of the Church. Thus the Church teaches that the Conscience is a supreme authority, even above that of the Popes, Bishops, and Priests. Thus while the Conscience does grant man a great degree of freedom, if one is going to disagree with conventional morality or with the teachings of the Church, it is absolutely necessary to make sure that one's conscience is well formed and certain of what it is claiming or not claiming. A sincere conscience presumes one is diligently seeking moral truth from authentic sources, whether that be from the Church, or from Scripture, or from the numerous Church Fathers. Nevertheless, despite one's best effort, "t can happen that moral conscience remains in ignorance and makes erroneous judgments about acts to be performed or already committed ... This ignorance can, but not always, be imputed to personal responsibility, This is the case when a man "takes little trouble to find out what is true and good", or in other words, puts forth very little effort and does not take the forming of the Conscience seriously.  In such cases, the person is culpable for the wrong he commits." Not necessarily because of the error itself, but because of the bad faith or miniscule effort put forth by the one whos Conscience is in question.  The Catholic Church has warned that "rejection of the Church's authority and her teaching ... can sometimes be at the source of errors in judgment in moral conduct". An example of someone following his conscience to the point of accepting the consequence of being condemned to death is Sir Thomas More (1478-1535). A theologian who wrote on the distinction between the 'sense of duty' and the 'moral sense', as two aspects of conscience, and who saw the former as some feeling that can only be explained by a divine Lawgiver, was John Henry Cardinal Newman. A well known saying of him is that he would first toast on his conscience and only then on the pope, since his conscience brought him to acknowledge the authority of the pope. The formation of a good conscience is also noted in Catholic writings as one of the main objectives of education: Pope Leo XIV refers in a survey of significant Catholic educators to those who "taught literacy, evangelized, took care of practical matters of daily life, elevated their  spirits through the cultivation of the arts, and, above all, formed consciences".


Judaism arguably does not require uncompromising obedience to religious authority; the case has been made that throughout Jewish history, rabbis have circumvented laws they found unconscionable, such as capital punishment. Similarly, although an occupation with national destiny has been central to the Jewish faith (see Zionism) many scholars (including Moses Mendelssohn) stated that conscience as a personal revelation of scriptural truth was an important adjunct to the Talmudic tradition. The concept of inner light in the Religious Society of Friends or Quakers is associated with conscience. Freemasonry describes itself as providing an adjunct to religion and key symbols found in a Freemason Lodge are the square and compasses explained as providing lessons that Masons should "square their actions by the square of conscience", learn to "circumscribe their desires and keep their passions within due bounds toward all mankind." The historian Manning Clark viewed conscience as one of the comforters that religion placed between man and death but also a crucial part of the quest for grace encouraged by the Book of Job and the Book of Ecclesiastes, leading us to be paradoxically closest to the truth when we suspect that what matters most in life ("being there when everyone suddenly understands what it has all been for") can never happen. Leo Tolstoy, after a decade studying the issue (1877–1887), held that the only power capable of resisting the evil associated with materialism and the drive for social power of religious institutions, was the capacity of humans to reach an individual spiritual truth through reason and conscience. Many prominent religious works about conscience also have a significant philosophical component: examples are the works of Al-Ghazali, Avicenna, Aquinas, Joseph Butler and Dietrich Bonhoeffer (all discussed in the philosophical views section).


Secular

The secular approach to conscience includes psychological, physiological, sociological, humanitarian, and authoritarian views. Lawrence Kohlberg considered critical conscience to be an important psychological stage in the proper moral development of humans, associated with the capacity to rationally weigh principles of responsibility, being best encouraged in the very young by linkage with humorous personifications (such as Jiminy Cricket) and later in adolescents by debates about individually pertinent moral dilemmas. Erik Erikson placed the development of conscience in the 'pre-schooler' phase of his eight stages of normal human personality development. The psychologist Martha Stout terms conscience "an intervening sense of obligation based in our emotional attachments." Thus a good conscience is associated with feelings of integrity, psychological wholeness and peacefulness and is often described using adjectives such as "quiet", "clear" and "easy".


Sigmund Freud regarded conscience as originating psychologically from the growth of civilisation, which periodically frustrated the external expression of aggression: this destructive impulse being forced to seek an alternative, healthy outlet, directed its energy as a superego against the person's own "ego" or selfishness (often taking its cue in this regard from parents during childhood). According to Freud, the consequence of not obeying our conscience is guilt, which can be a factor in the development of neurosis; Freud claimed that both the cultural and individual super-ego set up strict ideal demands with regard to the moral aspects of certain decisions, disobedience to which provokes a 'fear of conscience'.


Antonio Damasio considers conscience an aspect of extended consciousness beyond survival-related dispositions and incorporating the search for truth and desire to build norms and ideals for behavior.



Conscience as a society-forming instinct

Michel Glautier argues that conscience is one of the instincts and drives which enable people to form societies: groups of humans without these drives or in whom they are insufficient cannot form societies and do not reproduce their kind as successfully as those that do.


Charles Darwin considered that conscience evolved in humans to resolve conflicts between competing natural impulses-some about self-preservation but others about safety of a family or community; the claim of conscience to moral authority emerged from the "greater duration of impression of social instincts" in the struggle for survival. In such a view, behavior destructive to a person's society (either to its structures or to the persons it comprises) is bad or "evil". Thus, conscience can be viewed as an outcome of those biological drives that prompt humans to avoid provoking fear or contempt in others; being experienced as guilt and shame in differing ways from society to society and person to person. A requirement of conscience in this view is the capacity to see ourselves from the point of view of another person. Persons unable to do this (psychopaths, sociopaths, narcissists) therefore often act in ways which are "evil".


Fundamental in this view of conscience is that humans consider some "other" as being in a social relationship. Thus, nationalism is invoked in conscience to quell tribal conflict and the notion of a Brotherhood of Man is invoked to quell national conflicts. Yet such crowd drives may not only overwhelm but redefine individual conscience. Friedrich Nietzsche stated: "communal solidarity is annihilated by the highest and strongest drives that, when they break out passionately, whip the individual far past the average low level of the 'herd-conscience.'" Jeremy Bentham noted that: "fanaticism never sleeps ... it is never stopped by conscience; for it has pressed conscience into its service." Hannah Arendt in her study of the trial of Adolf Eichmann in Jerusalem, notes that the accused, as with almost all his fellow Germans, had lost track of his conscience to the point where they hardly remembered it; this wasn't caused by familiarity with atrocities or by psychologically redirecting any resultant natural pity to themselves for having to bear such an unpleasant duty, so much as by the fact that anyone whose conscience did develop doubts could see no one who shared them: "Eichmann did not need to close his ears to the voice of conscience ... not because he had none, but because his conscience spoke with a "respectable voice", with the voice of the respectable society around him".


Sir Arthur Keith in 1948 developed the Amity-enmity complex. We evolved as tribal groups surrounded by enemies; thus conscience evolved a dual role; the duty to save and protect members of the in-group, and the duty to show hatred and aggression towards any out-group.


An interesting area of research in this context concerns the similarities between our relationships and those of animals, whether animals in human society (pets, working animals, even animals grown for food) or in the wild. One idea is that as people or animals perceive a social relationship as important to preserve, their conscience begins to respect that former "other", and urge actions that protect it. Similarly, in complex territorial and cooperative breeding bird communities (such as the Australian magpie) that have a high degree of etiquettes, rules, hierarchies, play, songs and negotiations, rule-breaking seems tolerated on occasions not obviously related to survival of the individual or group; behaviour often appearing to exhibit a touching gentleness and tenderness.


Evolutionary biology

Contemporary scientists in evolutionary biology seek to explain conscience as a function of the brain that evolved to facilitate altruism within societies. In his book The God Delusion, Richard Dawkins states that he agrees with Robert Hinde's Why Good is Good, Michael Shermer's The Science of Good and Evil, Robert Buckman's Can We Be Good Without God? and Marc Hauser's Moral Minds, that our sense of right and wrong can be derived from our Darwinian past. He subsequently reinforced this idea through the lens of the gene-centered view of evolution, since the unit of natural selection is neither an individual organism nor a group, but rather the "selfish" gene, and these genes could ensure their own "selfish" survival by, inter alia, pushing individuals to act altruistically towards its kin.


Neuroscience and artificial conscience

Numerous case studies of brain damage have shown that damage to areas of the brain (such as the anterior prefrontal cortex) results in the reduction or elimination of inhibitions, with a corresponding radical change in behaviour. When the damage occurs to adults, they may still be able to perform moral reasoning; but when it occurs to children, they may never develop that ability.


Attempts have been made by neuroscientists to locate the free will necessary for what is termed the 'veto' of conscience over unconscious mental processes (see Neuroscience of free will and Benjamin Libet) in a scientifically measurable awareness of an intention to carry out an act occurring 350–400 microseconds after the electrical discharge known as the 'readiness potential.'


Jacques Pitrat claims that some kind of artificial conscience is beneficial in artificial intelligence systems to improve their long-term performance and direct their introspective processing.


Philosophical

The word "conscience" derives etymologically from the Latin conscientia, meaning "privity of knowledge"
or "with-knowledge". The English word implies internal awareness of a moral standard in the mind concerning the quality of one's motives, as well as a consciousness of our own actions. Thus conscience considered philosophically may be first, and perhaps most commonly, a largely unexamined "gut feeling" or "vague sense of guilt" about what ought to be or should have been done. Conscience in this sense is not necessarily the product of a process of rational consideration of the moral features of a situation (or the applicable normative principles, rules or laws) and can arise from parental, peer group, religious, state or corporate indoctrination, which may or may not be presently consciously acceptable to the person ("traditional conscience"). Conscience may be defined as the practical reason employed when applying moral convictions to a situation ("critical conscience"). In purportedly morally mature mystical people who have developed this capacity through daily contemplation or meditation combined with selfless service to others, critical conscience can be aided by a "spark" of intuitive insight or revelation (called marifa in Islamic Sufi philosophy and synderesis in medieval Christian scholastic moral philosophy). Conscience is accompanied in each case by an internal awareness of 'inner light' and approbation or 'inner darkness' and condemnation as well as a resulting conviction of right or duty either followed or declined.


Medieval

The medieval Islamic scholar and mystic Al-Ghazali divided the concept of Nafs (soul or self (spirituality)) into three categories based on the Qur’an:


The medieval Persian philosopher and physician Muhammad ibn Zakariya al-Razi believed in a close relationship between conscience or spiritual integrity and physical health; rather than being self-indulgent, man should pursue knowledge, use his intellect and apply justice in his life. The medieval Islamic philosopher Avicenna, whilst imprisoned in the castle of Fardajan near Hamadhan, wrote his famous isolated-but-awake "Floating Man" sensory deprivation thought experiment to explore the ideas of human self-awareness and the substantiality of the soul; his hypothesis being that it is through intelligence, particularly the active intellect, that God communicates truth to the human mind or conscience. According to the Islamic Sufis conscience allows Allah to guide people to the marifa, the peace or "light upon light" experienced where a Muslim's prayers lead to a melting away of the self in the inner knowledge of God; this foreshadowing the eternal Paradise depicted in the Qur’ān.


Some medieval Christian scholastics such as Bonaventure made a distinction between conscience as a rational faculty of the mind (practical reason) and inner awareness, an intuitive "spark" to do good, called synderesis arising from a remnant appreciation of absolute good and when consciously denied (for example to perform an evil act), becoming a source of inner torment. Early modern theologians such as William Perkins and William Ames developed a syllogistic understanding of the conscience, where God's law made the first term, the act to be judged the second and the action of the conscience (as a rational faculty) produced the judgement. By debating test cases applying such understanding conscience was trained and refined (i.e. casuistry).


In the 13th century, St. Thomas Aquinas regarded conscience as the application of moral knowledge to a particular case (S.T. I, q. 79, a. 13).  Thus, conscience was considered an act or judgment of practical reason that began with synderesis, the structured development of our innate remnant awareness of absolute good (which he categorised as involving the five primary precepts proposed in his theory of Natural Law) into an acquired habit of applying moral principles. According to Singer, Aquinas held that conscience, or conscientia was an imperfect process of judgment applied to activity because knowledge of the natural law (and all acts of natural virtue implicit therein) was obscured in most people by education and custom that promoted selfishness rather than fellow-feeling (Summa Theologiae, I–II, I). Aquinas also discussed conscience in relation to the virtue of prudence to explain why some people appear to be less "morally enlightened" than others, their weak will being incapable of adequately balancing their own needs with those of others.


Aquinas reasoned that acting contrary to conscience is an evil action but an errant conscience is only blameworthy if it is the result of culpable or vincible ignorance of factors that one has a duty to have knowledge of. Aquinas also argued that conscience should be educated to act towards real goods (from God) which encouraged human flourishing, rather than the apparent goods of sensory pleasures. In his Commentary on Aristotle's Nicomachean Ethics Aquinas claimed it was weak will that allowed a non-virtuous man to choose a principle allowing pleasure ahead of one requiring moral constraint.


Thomas A Kempis in the medieval contemplative classic The Imitation of Christ (ca 1418) stated that the glory of a good man is the witness of a good conscience. "Preserve a quiet conscience and you will always have joy. A quiet conscience can endure much, and remains joyful in all trouble, but an evil conscience is always fearful and uneasy." The anonymous medieval author of the Christian mystical work The Cloud of Unknowing similarly expressed the view that in profound and prolonged contemplation a soul dries up the "root and ground" of the sin that is always there, even after one's confession and however busy one is in holy things: "therefore, whoever would work at becoming a contemplative must first cleanse his  conscience." The medieval Flemish mystic John of Ruysbroeck likewise held that true conscience has four aspects that are necessary to render a man just in the active and contemplative life: "a free spirit, attracting itself through love"; "an intellect enlightened by grace", "a delight yielding propension or inclination" and "an outflowing losing of oneself in the abyss of ... that eternal object which is the highest and chief blessedness ... those lofty amongst men, are absorbed in it, and immersed in a certain boundless thing."


Modern

Benedict de Spinoza in his Ethics, published after his death in 1677, argued that most people, even those that consider themselves to exercise free will, make moral decisions on the basis of imperfect sensory information, inadequate understanding of their mind and will, as well as emotions which are both outcomes of their contingent physical existence and forms of thought defective from being chiefly impelled by self-preservation. The solution, according to Spinoza, was to gradually increase the capacity of our reason to change the forms of thought produced by emotions and to fall in love with viewing problems requiring moral decision from the perspective of eternity. Thus, living a life of peaceful conscience means to Spinoza that reason is used to generate adequate ideas where the mind increasingly sees the world and its conflicts, our desires and passions sub specie aeternitatis, that is without reference to time. Hegel's obscure and mystical Philosophy of Mind held that the absolute right of freedom of conscience facilitates human understanding of an all-embracing unity, an absolute which was rational, real and true. Nevertheless, Hegel thought that a functioning State would always be tempted not to recognize conscience in its form of subjective knowledge, just as similar non-objective opinions are generally rejected in science. A similar idealist notion was expressed in the writings of Joseph Butler who argued that conscience is God-given, should always be obeyed, is intuitive, and should be considered the "constitutional monarch" and the "universal moral faculty": "conscience does not only offer itself to show us the way we should walk in, but it likewise carries its own authority with it." Butler advanced ethical speculation by referring to a duality of regulative principles in human nature: first, "self-love" (seeking individual happiness) and second, "benevolence" (compassion and seeking good for another) in conscience (also linked to the agape of situational ethics). Conscience tended to be more authoritative in questions of moral judgment, thought Butler, because it was more likely to be clear and certain (whereas calculations of self-interest tended to probable and changing conclusions). John Selden in his Table Talk expressed the view that an awake but excessively scrupulous or ill-trained conscience could hinder resolve and practical action; it being "like a horse that is not well wayed, he starts at every bird that flies out of the hedge".


As the sacred texts of ancient Hindu and Buddhist philosophy became available in German translations in the 18th and 19th centuries, they influenced philosophers such as Schopenhauer to hold that in a healthy mind only deeds oppress our conscience, not wishes and thoughts; "for it is only our deeds that hold us up to the mirror of our will"; the good conscience, thought Schopenhauer, we experience after every disinterested deed arises from direct recognition of our own inner being in the phenomenon of another, it affords us the verification "that our true self exists not only in our own person, this particular manifestation, but in everything that lives. By this the heart feels itself enlarged, as by egotism it is contracted."


Immanuel Kant, a central figure of the Age of Enlightenment, likewise claimed that two things filled his mind with ever new and increasing admiration and awe, the oftener and more steadily they were reflected on: "the starry heavens above me and the moral law within me ... the latter begins from my invisible self, my personality, and exhibits me in a world which has true infinity but which I recognise myself as existing in a universal and necessary (and not only, as in the first case, contingent) connection." The 'universal connection' referred to here is Kant's categorical imperative: "act only according to that maxim by which you can at the same time will that it should become a universal law." Kant considered critical conscience to be an internal court in which our thoughts accuse or excuse one another; he acknowledged that morally mature people do often describe contentment or peace in the soul after following conscience to perform a duty, but argued that for such acts to produce virtue their primary motivation should simply be duty, not expectation of any such bliss. Rousseau expressed a similar view that conscience somehow connected man to a greater metaphysical unity. John Plamenatz in his critical examination of Rousseau's work considered that conscience was there defined as the feeling that urges us, in spite of contrary passions, towards two harmonies: the one within our minds and between our passions, and the other within society and between its members; "the weakest can appeal to it in the strongest, and the appeal, though often unsuccessful, is always disturbing. However, corrupted by power or wealth we may be, either as possessors of them or as victims, there is something in us serving to remind us that this corruption is against nature."


Other philosophers expressed a more sceptical and pragmatic view of the operation of "conscience" in society.
John Locke in his Essays on the Law of Nature argued that the widespread fact of human conscience allowed a philosopher to infer the necessary existence of objective moral laws that occasionally might contradict those of the state. Locke highlighted the metaethics problem of whether accepting a statement like "follow your conscience" supports subjectivist or objectivist conceptions of conscience as a guide in concrete morality, or as a spontaneous revelation of eternal and immutable principles to the individual: "if conscience be a proof of innate principles, contraries may be innate principles; since some men with the same bent of conscience prosecute what others avoid." Thomas Hobbes likewise pragmatically noted that opinions formed on the basis of conscience with full and honest conviction, nevertheless should always be accepted with humility as potentially erroneous and not necessarily indicating absolute knowledge or truth. William Godwin expressed the view that conscience was a memorable consequence of the "perception by men of every creed when the descend into the scene of busy life" that they possess free will. Adam Smith considered that it was only by developing a critical conscience that we can ever see what relates to ourselves in its proper shape and dimensions; or that we can ever make any proper comparison between our own interests and those of other people. John Stuart Mill believed that idealism about the role of conscience in government should be tempered with a practical realisation that few men in society are capable of directing their minds or purposes towards distant or unobvious interests, of disinterested regard for others, and especially for what comes after them, for the idea of posterity, of their country, or of humanity, whether grounded on sympathy or on a conscientious feeling. Mill held that certain amount of conscience, and of disinterested public spirit, may fairly be calculated on in the citizens of any community ripe for representative government, but that "it would be ridiculous to expect such a degree of it, combined with such intellectual discernment, as would be proof against any plausible fallacy tending to make that which was for their class interest appear the dictate of justice and of the general good."


Josiah Royce (1855–1916) built on the transcendental idealism view of conscience, viewing it as the ideal of life which constitutes our moral personality, our plan of being ourself, of making common sense ethical decisions. But, he thought, this was only true insofar as our conscience also required loyalty to "a mysterious higher or deeper self".
In the modern Christian tradition this approach achieved expression with Dietrich Bonhoeffer who stated during his imprisonment by the Nazis in World War II that conscience for him was more than practical reason, indeed it came from a "depth which lies beyond a man's own will and his own reason and it makes itself heard as the call of human existence to unity with itself." For Bonhoeffer a guilty conscience arose as an indictment of the loss of this unity and as a warning against the loss of one's self; primarily, he thought, it is directed not towards a particular kind of doing but towards a particular mode of being. It protests against a doing which imperils the unity of this being with itself. Conscience for Bonhoeffer did not, like shame, embrace or pass judgment on the morality of the whole of its owner's life; it reacted only to certain definite actions: "it recalls what is long past and represents this disunion as something which is already accomplished and irreparable". The man with a conscience, he believed, fights a lonely battle against the "overwhelming forces of inescapable situations" which demand moral decisions despite the likelihood of adverse consequences. Simon Soloveychik has similarly claimed that the truth distributed in the world, as the statement about human dignity, as the affirmation of the line between good and evil, lives in people as conscience.


As Hannah Arendt pointed out, however, (following the utilitarian John Stuart Mill on this point): a bad conscience does not necessarily signify a bad character; in fact only those who affirm a commitment to applying moral standards will be troubled with remorse, guilt or shame by a bad conscience and their need to regain integrity and wholeness of the self. Representing our soul or true self by analogy as our house, Arendt wrote that "conscience is the anticipation of the fellow who awaits you if and when you come home." Arendt believed that people who are unfamiliar with the process of silent critical reflection about what they say and do will not mind contradicting themselves by an immoral act or crime, since they can "count on its being forgotten the next moment;" bad people are not full of regrets. Arendt also wrote eloquently on the problem of languages distinguishing the word consciousness from conscience. One reason, she held, was that conscience, as we understand it in moral or legal matters, is supposedly always present within us, just like consciousness: "and this conscience is also supposed to tell us what to do and what to repent; before it became the lumen naturale or Kant's practical reason, it was the voice of God."


Albert Einstein, as a self-professed adherent of humanism and rationalism, likewise viewed an enlightened religious person as one whose conscience reflects that he "has, to the best of his ability, liberated himself from the fetters of his selfish desires and is preoccupied with thoughts, feelings and aspirations to which he clings because of their super-personal value."
Einstein often referred to the "inner voice" as a source of both moral and physical knowledge: "Quantum mechanics is very impressive. But an inner voice tells me that it is not the real thing. The theory produces a good deal but hardly brings one closer to the secrets of the Old One. I am at all events convinced that He does not play dice."


Simone Weil who fought for the French resistance (the Maquis) argued in her final book The Need for Roots: Prelude to a Declaration of Duties Towards Mankind that for society to become more just and protective of liberty, obligations should take precedence over rights in moral and political philosophy and a spiritual awakening should occur in the conscience of most citizens, so that social obligations are viewed as fundamentally having a transcendent origin and a beneficent impact on human character when fulfilled. Simone Weil also in that work provided a psychological explanation for the mental peace associated with a good conscience: "the liberty of men of goodwill, though limited in the sphere of action, is complete in that of conscience. For, having incorporated the rules into their own being, the prohibited possibilities no longer present themselves to the mind, and have not to be rejected."


Alternatives to such metaphysical and idealist opinions about conscience arose from realist and materialist perspectives such as those of Charles Darwin. Darwin suggested that "any animal whatever, endowed with well-marked social instincts, the parental and filial affections being here included, would inevitably acquire a moral sense or conscience, as soon as its intellectual powers had become as well, or as nearly as well developed, as in man." Émile Durkheim held that the soul and conscience were particular forms of an impersonal principle diffused in the relevant group and communicated by totemic ceremonies. A. J. Ayer was a more recent realist who held that the existence of conscience was an empirical question to be answered by sociological research into the moral habits of a given person or group of people, and what causes them to have precisely those habits and feelings. Such an inquiry, he believed, fell wholly within the scope of the existing social sciences. George Edward Moore bridged the idealistic and sociological views of 'critical' and 'traditional' conscience in stating that the idea of abstract 'rightness' and the various degrees of the specific emotion excited by it are what constitute, for many persons, the specifically 'moral sentiment' or conscience. For others, however, an action seems to be properly termed 'internally right', merely because they have previously regarded it as right, the idea of 'rightness' being present in some way to his or her mind, but not necessarily among his or her deliberately constructed motives.


The French philosopher Simone de Beauvoir in A Very Easy Death (Une mort très douce, 1964) reflects within her own conscience about her mother's attempts to develop such a moral sympathy and understanding of others.


"The sight of her tears grieved me; but I soon realised that she was weeping over her failure, without caring about what was happening inside me ... We might still have come to an understanding if, instead of asking everybody to pray for my soul, she had given me a little confidence and sympathy. I know now what prevented her from doing so: she had too much to pay back, too many wounds to salve, to put herself in another's place. In actual doing she made every sacrifice, but her feelings did not take her out of herself. Besides, how could she have tried to understand me since she avoided looking into her own heart? As for discovering an attitude that would not have set us apart, nothing in her life had ever prepared her for such a thing: the unexpected sent her into a panic, because she had been taught never to think, act or feel except in a ready-made framework."


Michael Walzer claimed that the growth of religious toleration in Western nations arose amongst other things, from the general recognition that private conscience signified some inner divine presence regardless of the religious faith professed and from the general respectability, piety, self-limitation, and sectarian discipline which marked most of the men who claimed the rights of conscience. Walzer also argued that attempts by courts to define conscience as a merely personal moral code or as sincere belief, risked encouraging an anarchy of moral egotisms, unless such a code and motive was necessarily tempered with shared moral knowledge: derived either from the connection of the individual to a universal spiritual order, or from the common principles and mutual engagements of unselfish people. Ronald Dworkin maintains that constitutional protection of freedom of conscience is central to democracy but creates personal duties to live up to it: "Freedom of conscience presupposes a personal responsibility of reflection, and it loses much of its meaning when that responsibility is ignored. A good life need not be an especially reflective one; most of the best lives are just lived rather than studied. But there are moments that cry out for self-assertion, when a passive bowing to fate or a mechanical decision out of deference or convenience is treachery, because it forfeits dignity for ease." Edward Conze stated it is important for individual and collective moral growth that we recognise the illusion of our conscience being wholly located in our body; indeed both our conscience and wisdom expand when we act in an unselfish way and conversely "repressed compassion results in an unconscious sense of guilt."


The philosopher Peter Singer considers that usually when we describe an action as conscientious in the critical sense we do so in order to deny either that the relevant agent was motivated by selfish desires, like greed or ambition, or that he acted on whim or impulse.


Moral anti-realists debate whether the moral facts necessary to activate conscience supervene on natural facts with a posteriori necessity; or arise a priori because moral facts have a primary intension and naturally identical worlds may be presumed morally identical. It has also been argued that there is a measure of moral luck in how circumstances create the obstacles which conscience must overcome to apply moral principles or human rights and that with the benefit of enforceable property rights and the rule of law, access to universal health care plus the absence of high adult and infant mortality from conditions such as malaria, tuberculosis, HIV/AIDS and famine, people in relatively prosperous developed countries have been spared pangs of conscience associated with the physical necessity to steal scraps of food, bribe tax inspectors or police officers, and commit murder in guerrilla wars against corrupt government forces or rebel armies. Roger Scruton has claimed that true understanding of conscience and its relationship with morality has been hampered by an "impetuous" belief that philosophical questions are solved through the analysis of language in an area where clarity threatens vested interests. Susan Sontag similarly argued that it was a symptom of psychological immaturity not to recognise that many morally immature people willingly experience a form of delight, in some an erotic breaking of taboo, when witnessing violence, suffering and pain being inflicted on others. Jonathan Glover wrote that most of us "do not spend our lives on endless landscape gardening of our self" and our conscience is likely shaped not so much by heroic struggles, as by choice of partner, friends and job, as well as where we choose to live. Garrett Hardin, in a famous article called "The Tragedy of the Commons", argues that any instance in which society appeals to an individual exploiting a commons to restrain himself or herself for the general good—by means of his or her conscience—merely sets up a system which, by selectively diverting societal power and physical resources to those lacking in conscience, while fostering guilt (including anxiety about his or her individual contribution to over-population) in people acting upon it, actually works toward the elimination of conscience from the race.


John Ralston Saul expressed the view in The Unconscious Civilization that in contemporary developed nations many people have acquiesced in turning over their sense of right and wrong, their critical conscience, to technical experts; willingly restricting their moral freedom of choice to limited consumer actions ruled by the ideology of the free market, while citizen participation in public affairs is limited to the isolated act of voting and private-interest lobbying turns even elected representatives against the public interest.


Some argue on religious or philosophical grounds that it is blameworthy to act against conscience, even if the judgement of conscience is likely to be erroneous (say because it is inadequately informed about the facts, or prevailing moral (humanist or religious), professional ethical, legal and human rights norms). Failure to acknowledge and accept that conscientious judgements can be seriously mistaken, may only promote situations where one's conscience is manipulated by others to provide unwarranted justifications for non-virtuous and selfish acts; indeed, insofar as it is appealed to as glorifying ideological content, and an associated extreme level of devotion, without adequate constraint of external, altruistic, normative justification, conscience may be considered morally blind and dangerous both to the individual concerned and humanity as a whole. Langston argues that philosophers of virtue ethics have unnecessarily neglected conscience for, once conscience is trained so that the principles and rules it applies are those one would want all others to live by, its practise cultivates and sustains the virtues; indeed, amongst people in what each society considers to be the highest state of moral development there is little disagreement about how to act. Emmanuel Levinas viewed conscience as a revelatory encountering of resistance to our selfish powers, developing morality by calling into question our naive sense of freedom of will to use such powers arbitrarily, or with violence, this process being more severe the more rigorously the goal of our self was to obtain control.
In other words, the welcoming of the Other, to Levinas, was the very essence of conscience properly conceived; it encouraged our ego to accept the fallibility of assuming things about other people, that selfish freedom of will "does not have the last word" and that realising this has a transcendent purpose: "I am not alone ... in conscience I have an experience that is not commensurate with any a priori  framework-a conceptless experience."


Conscientious acts and the law

In the late 13th and early 14th centuries, English litigants began to petition the Lord Chancellor of England for relief from unjust judgments.  As Keeper of the King's Conscience, the Chancellor intervened to allow for "merciful exceptions" to the King's laws, "to ensure that the King's conscience was right before God". The Chancellor's office evolved into the Court of Chancery and the Chancellor's decisions evolved into the body of law known as equity.



English humanist lawyers in the 16th and 17th centuries interpreted conscience as a collection of universal principles given to man by god at creation to be applied by reason; this gradually reforming the medieval Roman law-based system with forms of action, written pleadings, use of juries and patterns of litigation such as Demurrer and Assumpsit that displayed an increased concern for elements of right and wrong on the actual facts. A conscience vote in a parliament allows legislators to vote without restrictions from any political party to which they may belong. In his trial in Jerusalem Nazi war criminal Adolf Eichmann claimed he was simply following legal orders under paragraph 48 of the German Military Code which provided: "punishability of an action or omission is not excused on the ground that the person considered his behaviour required by his conscience or the prescripts of his religion". The United Nations Universal Declaration on Human Rights (UDHR) which is part of international customary law specifically refers to conscience in Articles 1 and 18. Likewise, the United Nations International Covenant on Civil and Political Rights (ICCPR) mentions conscience in Article 18.1..mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}

All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood

— United Nations, Universal Declaration on Human Rights Article 1

Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance

— United Nations, Universal Declaration on Human Rights Article 18

Everyone shall have the right to freedom of thought, conscience and religion. This right shall include freedom to have or to adopt a religion or belief of his choice, and freedom, either individually or in community with others and in public or private, to manifest his religion or belief in worship, observance, practice and teaching

— United Nations, International Covenant on Civil and Political Rights Article 18.1

 It has been argued that these articles provide international legal obligations protecting conscientious objectors from service in the military.


John Rawls in his A Theory of Justice defines a conscientious objector as an individual prepared to undertake, in public (and often despite widespread condemnation), an action of civil disobedience to a legal rule justifying it (also in public) by reference to contrary foundational social virtues (such as justice as liberty or fairness) and the principles of morality and law derived from them. Rawls considered civil disobedience should be viewed as an appeal, warning or admonishment (showing general respect and fidelity to the rule of law by the non-violence and transparency of methods adopted) that a law breaches a community's fundamental virtue of justice. Objections to Rawls' theory include first, its inability to accommodate conscientious objections to the society's basic appreciation of justice or to emerging moral or ethical principles (such as respect for the rights of the natural environment) which are not yet part of it and second, the difficulty of predictably and consistently determining that a majority decision is just or unjust. Conscientious objection (also called conscientious refusal or evasion) to obeying a law, should not arise from unreasoning, naive "traditional conscience", for to do so merely encourages infantile abdication of responsibility to calibrate the law against moral or human rights norms and disrespect for democratic institutions. Instead it should be based on "critical conscience' – seriously thought out, conceptually mature, personal moral or religious beliefs held to be fundamentally incompatible (that is, not merely inconsistent on the basis of selfish desires, whim or impulse), for example, either with all laws requiring conscription for military service, or legal compulsion to fight for or financially support the State in a particular war. A famous example arose when Henry David Thoreau the author of Walden was willingly jailed for refusing to pay a tax because he profoundly disagreed with a government policy and was frustrated by the corruption and injustice of the democratic machinery of the state. A more recent case concerned Kimberly Rivera, a private in the US Army and mother of four children who, having served three months in Iraq War decided the conflict was immoral and sought refugee status in Canada in 2012 (see List of Iraq War resisters), but was deported and arrested in the US.


"Unjust laws exist; shall we be content to obey them, or shall we endeavour to amend them, and obey them until we have succeeded, or shall we transgress them at once? ... A man has not everything to do but something; and because he cannot do everything, it is not necessary that he should do something wrong ... It is for no particular item in the tax bill that I refuse to pay it. I simply wish to refuse allegiance to the State, to withdraw and stand aloof from it effectually. I do not care to trace the course of my dollar if I could, till it buys a man, or a musket to shoot one with—the dollar is innocent—but I am concerned to trace the effects of my allegiance ... Must the citizen ever for a moment, or in the least degree, resign his conscience to the legislator? Why has every man a conscience, then?"


In the Second World War, Great Britain granted conscientious-objection status not just to complete pacifists, but to those who objected to fighting in that particular war; this was done partly out of genuine respect, but also to avoid the disgraceful and futile persecutions of conscientious objectors that occurred during the First World War.


Amnesty International organises campaigns to protect those arrested and or incarcerated as a prisoner of conscience because of their conscientious beliefs, particularly concerning intellectual, political and artistic freedom of expression and association. Aung San Suu Kyi of Burma, was the winner of the 2009 Amnesty International Ambassador of Conscience Award. In legislation, a conscience clause is a provision in a statute that excuses a health professional from complying with the law (for example legalising surgical or pharmaceutical abortion) if it is incompatible with religious or conscientious beliefs.
Expressed justifications for refusing to obey laws because of conscience vary. Many conscientious objectors are so for religious reasons—notably, members of the historic peace churches are pacifist by doctrine. Other objections can stem from a deep sense of responsibility toward humanity as a whole, or from the conviction that even acceptance of work under military orders acknowledges the principle of conscription that should be everywhere condemned before the world can ever become safe for real democracy. A conscientious objector, however, does not have a primary aim of changing the law. John Dewey considered that conscientious objectors were often the victims of "moral innocency" and inexpertness in moral training: "the moving force of events is always too much for conscience". The remedy was not to deplore the wickedness of those who manipulate world power, but to connect conscience with forces moving in another direction- to build institutions and social environments predicated on the rule of law, for example, "then will conscience itself have compulsive power instead of being forever the martyred and the coerced." As an example, Albert Einstein who had advocated conscientious objection during the First World War and had been a longterm supporter of War Resisters' International reasoned that "radical pacifism" could not be justified in the face of Nazi rearmament and advocated a world federalist organization with its own professional army.
Samuel Johnson pointed out that an appeal to conscience should not allow the law to bring unjust suffering upon another. Conscience, according to Johnson, was nothing more than a conviction felt by ourselves of something to be done or something to be avoided; in questions of simple unperplexed morality, conscience is very often a guide that may be trusted. But before conscience can conclusively determine what morally should be done, he thought that the state of the question should be thoroughly known. "No man's conscience", said Johnson "can tell him the right of another man ... it is a conscience very ill informed that violates the rights of one man, for the convenience of another."


Civil disobedience as nonviolent protest or civil resistance are also acts of conscience, but are designed by those who undertake them chiefly to change, by appealing to the majority and democratic processes, laws or government policies perceived to be incoherent with fundamental social virtues and principles (such as justice, equality or respect for intrinsic human dignity). Civil disobedience, in a properly functioning democracy, allows a minority who feel strongly that a law infringes their sense of justice (but have no capacity to obtain legislative amendments or a referendum on the issue) to make a potentially apathetic or uninformed majority take account of the intensity of opposing views. A notable example of civil resistance or satyagraha ("satya" in sanskrit means "truth and compassion", "agraha" means "firmness of will") involved Mahatma Gandhi making salt in India when that act was prohibited by a British statute, in order to create moral pressure for law reform. Rosa Parks similarly acted on conscience in 1955 in Montgomery, Alabama refusing a legal order to give up her seat to make room for a white passenger; her action (and the similar earlier act of 15-year-old Claudette Colvin) led to the Montgomery bus boycott. Rachel Corrie was a US citizen allegedly killed by a bulldozer operated by the Israel Defense Forces (IDF) while involved in direct action (based on the nonviolent principles of Martin Luther King Jr. and Mahatma Gandhi) to prevent demolition of the home of local Palestinian pharmacist Samir Nasrallah. Al Gore has argued "If you're a young person looking at the future of this planet and looking at what is being done right now, and not done, I believe we have reached the stage where it is time for civil disobedience to prevent the construction of new coal plants that do not have carbon capture and sequestration." In 2011, NASA climate scientist James E. Hansen, environmental leader Phil Radford and Professor Bill McKibben were arrested for opposing a tar sands oil pipeline and Canadian renewable energy professor Mark Jaccard was arrested for opposing mountain-top coal mining; in his book Storms of my Grandchildren Hansen calls for similar civil resistance on a global scale to help replace the 'business-as-usual' Kyoto Protocol cap and trade system, with a progressive carbon tax at emission source on the oil, gas and coal industries – revenue being paid as dividends to low carbon footprint families.


Notable historical examples of conscientious noncompliance in a different professional context included the manipulation of the visa process in 1939 by Japanese Consul-General Chiune Sugihara in Kaunas (the temporary capital of Lithuania between Germany and the Soviet Union) and by Raoul Wallenberg in Hungary in 1944 to allow Jews to escape almost certain death. Ho Feng-Shan the Chinese Consul-General in Vienna in 1939, defied orders from the Chinese ambassador in Berlin to issue Jews with visas for Shanghai. John Rabe a German member of the Nazi Party likewise saved thousands of Chinese from massacre by the Japanese military at Nanjing. The White Rose German student movement against the Nazis declared in their 4th leaflet: "We will not be silent. We are your bad conscience. The White Rose will not leave you in peace!" Conscientious noncompliance may be the only practical option for citizens wishing to affirm the existence of an international moral order or 'core' historical rights (such as the right to life, right to a fair trial and freedom of opinion) in states where non-violent protest or civil disobedience are met with prolonged arbitrary detention, torture, forced disappearance, murder or persecution.
The controversial Milgram experiment into obedience by Stanley Milgram showed that many people lack the psychological resources to openly resist authority, even when they are directed to act callously and inhumanely against an innocent victim.


World conscience

World conscience is the universalist idea that with ready global communication, all people on earth will no longer be morally estranged from one another, whether it be culturally, ethnically, or geographically; instead they will conceive ethics from the utopian point of view of the universe, eternity or infinity, rather than have their duties and obligations defined by forces arising solely within the restrictive boundaries of "blood and territory".


Often this derives from a spiritual or natural law perspective, that for world peace to be achieved, conscience, properly understood, should be generally considered as not necessarily linked (often destructively) to fundamentalist religious ideologies, but as an aspect of universal consciousness, access to which is the common heritage of humanity. Thinking predicated on the development of world conscience is common to members of the Global Ecovillage Network such as the Findhorn Foundation, international conservation organisations like Fauna and Flora International, as well as performers of world music such as Alan Stivell. Non-government organizations, particularly through their work in agenda-setting, policy-making and implementation of human rights-related policy, have been referred to as the conscience of the world


Edward O Wilson has developed the idea of consilience to encourage coherence of global moral and scientific knowledge supporting the premise that "only unified learning, universally shared, makes accurate foresight and wise choice possible". Thus, world conscience is a concept that overlaps with the Gaia hypothesis in advocating a balance of moral, legal, scientific and economic solutions to modern transnational problems such as global poverty and global warming, through strategies such as environmental ethics, climate ethics, natural conservation, ecology, cosmopolitanism, sustainability and sustainable development, biosequestration and legal protection of the biosphere and biodiversity. The NGO 350.org, for example, seeks to attract world conscience to the problems associated with elevation in atmospheric greenhouse gas concentrations.


The microcredit initiatives of Nobel Peace Prize winner Muhammad Yunus have been described as inspiring a "war on poverty that blends social conscience and business savvy".


The Green party politician Bob Brown (who was arrested by the Tasmanian state police for a conscientious act of civil disobedience during the Franklin Dam protest) expresses world conscience in these terms: "the universe, through us, is evolving towards experiencing, understanding and making choices about its future'; one example of policy outcomes from such thinking being a global tax (see Tobin tax) to alleviate global poverty and protect the biosphere, amounting to 1/10 of 1% placed on the worldwide speculative currency market. Such an approach sees world conscience best expressing itself through political reforms promoting democratically based globalisation or planetary democracy (for example internet voting for global governance organisations (see world government) based on the model of "one person, one vote, one value") which gradually will replace contemporary market-based globalisation.


The American cardiologist Bernard Lown and the Russian cardiologist Yevgeniy Chazov were motivated in conscience through studying the catastrophic public health consequences of nuclear war in establishing International Physicians for the Prevention of Nuclear War (IPPNW) which was awarded the Nobel Peace Prize in 1985 and continues to work to "heal an ailing planet".Worldwide expressions of conscience contributed to the decision of the French government to halt atmospheric nuclear tests at Mururoa in the Pacific in 1974 after 41 such explosions (although below-ground nuclear tests continued there into the 1990s).


A challenge to world conscience was provided by an influential 1968 article by Garrett Hardin that critically analyzed the dilemma in which multiple individuals, acting independently after rationally consulting self-interest (and, he claimed, the apparently low 'survival-of-the-fittest' value of conscience-led actions) ultimately destroy a shared limited resource, even though each acknowledges such an outcome is not in anyone's long-term interest. Hardin's conclusion that commons areas are practicably achievable only in conditions of low population density (and so their continuance requires state restriction on the freedom to breed), created controversy additionally through his direct deprecation of the role of conscience in achieving individual decisions, policies and laws that facilitate global justice and peace, as well as sustainability and sustainable development of world commons areas, for example including those officially designated such under United Nations treaties (see common heritage of humanity). Areas designated common heritage of humanity under international law include the Moon, Outer Space, deep sea bed, Antarctica, the world cultural and natural heritage (see World Heritage Convention) and the human genome. It will be a significant challenge for world conscience that as world oil, coal, mineral, timber, agricultural and water reserves are depleted, there will be increasing pressure to commercially exploit common heritage of mankind areas.


The philosopher Peter Singer has argued that the United Nations Millennium Development Goals represent the emergence of an ethics based not on national boundaries but on the idea of one world. Ninian Smart has similarly predicted that the increase in global travel and communication will gradually draw the world's religions towards a pluralistic and transcendental humanism characterized by an "open spirit" of empathy and compassion.


Noam Chomsky has argued that forces opposing the development of such a world conscience include free market ideologies that valorise corporate greed in nominal electoral democracies where advertising, shopping malls and indebtedness, shape citizens into apathetic consumers in relation to information and access necessary for democratic participation. John Passmore has argued that mystical considerations about the global expansion of all human consciousness, should take into account that if as a species we do become something much superior to what we are now, it will be as a consequence of conscience not only implanting a goal of moral perfectibility, but assisting us to remain periodically anxious, passionate and discontented, for these are necessary components of care and compassion. The Committee on Conscience of the US Holocaust Memorial Museum has targeted genocides such as those in Rwanda, Bosnia, Darfur, the Congo and Chechnya as challenges to the world's conscience. Oscar Arias Sanchez has criticised global arms industry spending as a failure of conscience by nation states: "When a country decides to invest in arms, rather than in education, housing, the environment, and health services for its people, it is depriving a whole generation of its right to prosperity and happiness. We have produced one firearm for every ten inhabitants of this planet, and yet we have not bothered to end hunger when such a feat is well within our reach. This is not a necessary or inevitable state of affairs. It is a deliberate choice" (see Campaign Against Arms Trade). US House of Representatives Speaker Nancy Pelosi, after meeting with the 14th Dalai Lama during the 2008 violent protests in Tibet and aftermath said: "The situation in Tibet is a challenge to the conscience of the world." Nelson Mandela, through his example and words, has been described as having shaped the conscience of the world.  The Right Livelihood Award is awarded yearly in Sweden to those people, mostly strongly motivated by conscience, who have made exemplary practical contributions to resolving the great challenges facing our planet and its people. In 2009, for example, along with Catherine Hamlin (obstetric fistula and see fistula foundation)), David Suzuki (promoting awareness of climate change) and Alyn Ware (nuclear disarmament), René Ngongo shared the Right Livelihood Award "for his courage in confronting the forces that are destroying the Congo Basin's rainforests and building political support for their conservation and sustainable use". Avaaz is one of the largest global on-line organizations launched in January 2007 to promote conscience-driven activism on issues such as climate change, human rights, animal rights, corruption, poverty, and conflict, thus "closing the gap between the world we have and the world most people everywhere want".


Notable examples of modern acts based on conscience

In a notable contemporary act of conscience, Christian bushwalker Brenda Hean protested against the flooding of Lake Pedder despite threats and that ultimately led to her death. Another was the campaign by Ken Saro-Wiwa against oil extraction by multinational corporations in Nigeria that led to his execution. So too was the act by the Tank Man, or the Unknown Rebel photographed holding his shopping bag in the path of tanks during the protests at Beijing's Tiananmen Square on 5 June 1989. The actions of United Nations Secretary General Dag Hammarskjöld to try to achieve peace in the Congo despite the (eventuating) threat to his life were strongly motivated by conscience as is reflected in his diary, Vägmärken (Markings). Another example involved the actions of Warrant Officer Hugh Thompson, Jr to try to prevent the My Lai massacre in the Vietnam War. Evan Pederick voluntarily confessed and was convicted of the Sydney Hilton bombing stating that his conscience could not tolerate the guilt and that "I guess I was quite unique in the prison system in that I had to keep proving my guilt, whereas everyone else said they were innocent." Vasili Arkhipov was a Russian naval officer on out-of-radio-contact Soviet submarine B-59 being depth-charged by US warships during the Cuban Missile Crisis whose dissent when two other officers decided to launch a nuclear torpedo (unanimous agreement to launch was required) may have averted a nuclear war. In 1963 Buddhist monk Thich Quang Duc performed a famous act of self-immolation to protest against alleged persecution of his faith by the Vietnamese Ngo Dinh Diem regime.


Conscience played a major role in the actions by anaesthetist Stephen Bolsin to whistleblow (see list of whistleblowers) on incompetent paediatric cardiac surgeons at the Bristol Royal Infirmary. Jeffrey Wigand was motivated by conscience to expose the Big Tobacco scandal, revealing that executives of the companies knew that cigarettes were addictive and approved the addition of carcinogenic ingredients to the cigarettes. David Graham, a Food and Drug Administration employee, was motivated by conscience to whistleblow that the arthritis pain-reliever Vioxx increased the risk of cardiovascular deaths although the manufacturer suppressed this information. Rick Piltz, from the U.S. global warming Science Program, blew the whistle on a White House official who ignored majority scientific opinion to edit a climate change report ("Our Changing Planet") to reflect the Bush administration's view that the problem was unlikely to exist. Muntadhar al-Zaidi, an Iraqi journalist, was imprisoned and allegedly tortured for his act of conscience in throwing his shoes at George W. Bush. Mordechai Vanunu, an Israeli former nuclear technician, acted on conscience to reveal details of Israel's nuclear weapons program to the British press in 1986; was kidnapped by Israeli agents, transported to Israel, convicted of treason and spent 18 years in prison, including more than 11 years in solitary confinement.


At the awards ceremony for the 200 metres at the 1968 Summer Olympics in Mexico City John Carlos, Tommie Smith and Peter Norman ignored death threats and official warnings to take part in an anti-racism protest that destroyed their respective careers. W. Mark Felt an agent of the United States Federal Bureau of Investigation who retired in 1973 as the Bureau's Associate Director, acted on conscience to provide reporters Bob Woodward and Carl Bernstein with information that resulted in the Watergate scandal. Conscience was a major factor in US Public Health Service officer Peter Buxtun revealing the Tuskegee syphilis experiment to the public. The 2008 attack by the Israeli military on civilian areas of Palestinian Gaza was described as a "stain on the world's conscience". Conscience was a major factor in the refusal of Aung San Suu Kyi to leave Burma despite house arrest and persecution by the military dictatorship in that country.
Conscience was a factor in Peter Galbraith's criticism of fraud in the 2009 Afghanistan election despite it costing him his United Nations job. Conscience motivated Bunnatine Greenhouse to expose irregularities in the contracting of the Halliburton company for work in Iraq. Naji al-Ali a popular cartoon artist in the Arab world, loved for his defense of the ordinary people, and for his criticism of repression and despotism by both the Israeli military and Yasser Arafat's PLO, was murdered for refusing to compromise with his conscience. The journalist Anna Politkovskaya provided (prior to her murder) an example of conscience in her opposition to the Second Chechen War and then-Russian President Vladimir Putin. Conscience motivated the Russian human rights activist Natalia Estemirova, who was abducted and murdered in Grozny, Chechnya in 2009. The Death of Neda Agha-Soltan arose from conscience-driven protests against the 2009 Iranian presidential election. Muslim lawyer Shirin Ebadi (winner of the 2003 Nobel Peace Prize) has been described as the 'conscience of the Islamic Republic' for her work in protecting the human rights of women and children in Iran. The human rights lawyer Gao Zhisheng, often referred to as the 'conscience of China' and who had previously been arrested and allegedly tortured after calling for respect for human rights and for constitutional reform, was abducted by Chinese security agents in February 2009. 2010 Nobel Peace Prize winner Liu Xiaobo in his final statement before being sentenced by a closed Chinese court to over a decade in jail as a political prisoner of conscience stated: "For hatred is corrosive of a person’s wisdom and conscience; the mentality of enmity can poison a nation’s spirit." Sergei Magnitsky, a lawyer in Russia, was arrested, held without trial for almost a year and died in custody, as a result of exposing corruption. On 6 October 2001 Laura Whittle was a naval gunner on HMAS Adelaide (FFG 01) under orders to implement a new border protection policy when they encountered the SIEV-4 (Suspected Illegal Entry Vessel-4) refugee boat in choppy seas. After being ordered to fire warning shots from her 50 calibre machinegun to make the boat turn back she saw it beginning to break up and sink with a father on board holding out his young daughter that she might be saved (see Children Overboard Affair). Whittle jumped without a life vest 12 metres into the sea to help save the refugees from drowning thinking "this isn't right; this isn't how things should be." In February 2012 journalist Marie Colvin was deliberately targeted and killed by the Syrian Army in Homs during the Syrian uprising and Siege of Homs, after she decided to stay at the "epicentre of the storm" in order to "expose what is happening". In October 2012 the Taliban organised the attempted murder of Malala Yousafzai a teenage girl who had been campaigning, despite their threats, for female education in Pakistan. In December 2012 the 2012 Delhi gang rape case was said to have stirred the collective conscience of India to civil disobedience and public protest at the lack of legal action against rapists in that country (see Rape in India) In June 2013 Edward Snowden revealed details of a US National Security Agency internet and electronic communication PRISM (surveillance program) because of a conscience-felt obligation to the freedom of humanity greater than obedience to the laws that bound his employment.


In literature, art, film, and music

The ancient epic of the Indian subcontinent, the Mahabharata of Vyasa, contains two pivotal moments of conscience. The first occurs when the warrior Arjuna being overcome with compassion against killing his opposing relatives in war, receives counsel (see Bhagavad-Gita) from Krishna about his spiritual duty ("work as though you are performing a sacrifice for the general good"). The second, at the end of the saga, is when king Yudhishthira having alone survived the moral tests of life, is offered eternal bliss, only to refuse it because a faithful dog is prevented from coming with him by purported divine rules and laws. The French author Montaigne (1533–1592) in one of the most celebrated of his essays ("On experience") expressed the benefits of living with a clear conscience: "Our duty is to compose our character, not to compose books, to win not battles and provinces, but order and tranquillity in our conduct. Our great and glorious masterpiece is to live properly". In his famous Japanese travel journal Oku no Hosomichi (Narrow Road to the Deep North) composed of mixed haiku poetry and prose, Matsuo Bashō (1644–94) in attempting to describe the eternal in this perishable world is often moved in conscience; for example by a thicket of summer grass being all that remains of the dreams and ambitions of ancient warriors. Chaucer's "Franklin's Tale" in The Canterbury Tales recounts how a young suitor releases a wife from a rash promise because of the respect in his conscience for the freedom to be truthful, gentle and generous.


 The critic A. C. Bradley discusses the central problem of Shakespeare's tragic character Hamlet as one where conscience in the form of moral scruples deters the young Prince with his "great anxiety to do right" from obeying his father's hell-bound ghost and murdering the usurping King ("is't not perfect conscience to quit him with this arm?" (v.ii.67)).


Bradley develops a theory about Hamlet's moral agony relating to a conflict between "traditional" and "critical" conscience: "The conventional moral ideas of his time, which he shared with the Ghost, told him plainly that he ought to avenge his father; but a deeper conscience in him, which was in advance of his time, contended with these explicit conventional ideas. It is because this deeper conscience remains below the surface that he fails to recognise it, and fancies he is hindered by cowardice or sloth or passion or what not; but it emerges into light in that speech to Horatio. And it is just because he has this nobler moral nature in him that we admire and love him". The opening words of Shakespeare's Sonnet 94 ("They that have pow'r to hurt, and will do none") have been admired as a description of conscience. So has John Donne's commencement of his poem s:Goodfriday, 1613. Riding Westward: "Let man's soul be a sphere, and then, in this, Th' intelligence that moves, devotion is;"


Anton Chekhov in his plays The Seagull, Uncle Vanya and Three Sisters describes the tortured emotional states of doctors who at some point in their careers have turned their back on conscience. In his short stories, Chekhov also explored how people misunderstood the voice of a tortured conscience. A promiscuous student, for example, in The Fit describes it as a "dull pain, indefinite, vague; it was like anguish and the most acute fear and despair ... in his breast, under the heart" and the young doctor examining the misunderstood agony of compassion experienced by the factory owner's daughter in From a Case Book calls it an "unknown, mysterious power ... in fact close at hand and watching him." Characteristically, Chekhov's own conscience drove him on the long journey to Sakhalin to record and alleviate the harsh conditions of the prisoners at that remote outpost. As Irina Ratushinskaya writes in the introduction to that work: "Abandoning everything, he travelled to the distant island of Sakhalin, the most feared place of exile and forced labour in Russia at that time. One cannot help but wonder why? Simply, because the lot of the people there was a bitter one, because nobody really knew about the lives and deaths of the exiles, because he felt that they stood in greater need of help that anyone else. A strange reason, maybe, but not for a writer who was the epitome of all the best traditions of a Russian man of letters. Russian literature has always focused on questions of conscience and was, therefore, a powerful force in the moulding of public opinion."


E. H. Carr writes of Dostoevsky's character the young student Raskolnikov in the novel Crime and Punishment who decides to murder a 'vile and loathsome' old woman money lender on the principle of transcending conventional morals: "the sequel reveals to us not the pangs of a stricken conscience (which a less subtle writer would have given us) but the tragic and fruitless struggle of a powerful intellect to maintain a conviction which is incompatible with the essential nature of man."

Hermann Hesse wrote his Siddhartha to describe how a young man in the time of the Buddha follows his conscience on a journey to discover a transcendent inner space where all things could be unified and simply understood, ending up discovering that personal truth through selfless service as a ferryman. J. R. R. Tolkien in his epic The Lord of the Rings describes how only the hobbit Frodo is pure enough in conscience to carry the ring of power through war-torn Middle-earth to destruction in the Cracks of Doom, Frodo determining at the end to journey without weapons, and being saved from failure by his earlier decision to spare the life of the creature Gollum. Conor Cruise O'Brien wrote that Albert Camus was the writer most representative of the Western consciousness and conscience in its relation to the non-Western world. Harper Lee's 1960 novel To Kill a Mockingbird portrays Atticus Finch (played by Gregory Peck in the classic film from the book (see To Kill a Mockingbird)) as a lawyer true to his conscience who sets an example to his children and community.


The Robert Bolt play A Man For All Seasons focuses on the conscience of Catholic lawyer Thomas More in his struggle with King Henry VIII ("the loyal subject is more bounden to be loyal to his conscience than to any other thing"). George Orwell wrote his novel Nineteen Eighty-Four on the isolated island of Jura, Scotland to describe how a man (Winston Smith) attempts to develop critical conscience in a totalitarian state which watches every action of the people and manipulates their thinking with a mixture of propaganda, endless war and thought control through language control (double think and newspeak) to the point where prisoners look up to and even love their torturers. In the Ministry of Love, Winston's torturer (O'Brien) states: "You are imagining that there is something called human nature which will be outraged by what we do and will turn against us. But we create human nature. Men are infinitely malleable".


A tapestry copy of Picasso's Guernica depicting a massacre of innocent women and children during the Spanish Civil War is displayed on the wall of the United Nations building in New York City, at the entrance to the Security Council room, demonstrably as a spur to the conscience of representatives from the nation states. Albert Tucker painted Man's Head to capture the moral disintegration, and lack of conscience, of a man convicted of kicking a dog to death.


The Impressionist painter Vincent van Gogh wrote in a letter to his brother Theo in 1878 that "one must never let the fire in one's soul die, for the time will inevitably come when it will be needed. And he who chooses poverty for himself and loves it possesses a great treasure and will hear the voice of his conscience address him every more clearly. He who hears that voice, which is God's greatest gift, in his innermost being and follows it, finds in it a friend at last, and he is never alone! ... That is what all great men have acknowledged in their works, all those who have thought a little more deeply and searched and worked and loved a little more than the rest, who have plumbed the depths of the sea of life."


The 1957 Ingmar Bergman film The Seventh Seal portrays the journey of a medieval knight (Max von Sydow) returning disillusioned from the crusades ("what is going to happen to those of us who want to believe, but aren't able to?") across a plague-ridden landscape, undertaking a game of chess with the personification of Death until he can perform one meaningful altruistic act of conscience (overturning the chess board to distract Death long enough for a family of jugglers to escape in their wagon). The 1942 Casablanca centers on the development of conscience in the cynical American Rick Blaine (Humphrey Bogart) in the face of oppression by the Nazis and the example of the resistance leader Victor Laszlo.The David Lean and Robert Bolt screenplay for Doctor Zhivago (an adaptation of Boris Pasternak's novel) focuses strongly on the conscience of a doctor-poet in the midst of the Russian Revolution (in the end "the walls of his heart were like paper").The 1982 Ridley Scott film Blade Runner focuses on the struggles of conscience between and within a bounty hunter (Rick Deckard (Harrison Ford)) and a renegade replicant android (Roy Batty (Rutger Hauer)) in a future society which refuses to accept that forms of artificial intelligence can have aspects of being such as conscience.


Johann Sebastian Bach wrote his last great choral composition the Mass in B minor (BWV 232) to express the alternating emotions of loneliness, despair, joy and rapture that arise as conscience reflects on a departed human life. Here JS Bach's use of counterpoint and contrapuntal settings, his dynamic discourse of melodically and rhythmically distinct voices seeking forgiveness of sins ("Qui tollis peccata mundi, miserere nobis") evokes a spiraling moral conversation of all humanity expressing his belief that "with devotional music, God is always present in his grace".


Ludwig van Beethoven's meditations on illness, conscience and mortality in the Late String Quartets led to his dedicating the third movement of String Quartet in A Minor (1825) Op. 132 (see String Quartet No. 15) as a "Hymn of Thanksgiving to God of a convalescent". John Lennon's work "Imagine" owes much of its popular appeal to its evocation of conscience against the atrocities created by war, religious fundamentalism and politics. The Beatles George Harrison-written track "The Inner Light" sets to Indian raga music a verse from the Tao Te Ching that "without going out of your door you can know the ways of heaven'. In the 1986 movie The Mission the guilty conscience and penance of the slave trader Mendoza is made more poignant by the haunting oboe music of Ennio Morricone ("On Earth as it is in Heaven") The song Sweet Lullaby by Deep Forest is based on a traditional Baegu lullaby from the Solomon Islands called "Rorogwela" in which a young orphan is comforted as an act of conscience by his older brother. The Dream Academy song 'Forest Fire' provided an early warning of the moral dangers of our 'black cloud' 'bringing down a different kind of weather ... letting the sunshine in, that's how the end begins."


The American Society of Journalists and Authors (ASJA) presents the Conscience-in-Media Award to journalists whom the society deems worthy of recognition for demonstrating "singular commitment to the highest principles of journalism at notable personal cost or sacrifice".


The Ambassador of Conscience Award, Amnesty International's most prestigious human rights award, takes its inspiration from a poem written by Irish Nobel prize-winning poet Seamus Heaney called "The Republic of Conscience".


See also

Notes

References

Further reading

External links







The history of life on Earth traces the processes by which living and extinct organisms evolved, from the earliest emergence of life to the present day. Earth formed about 4.54 ± 0.05 billion years ago (abbreviated as Ga, for gigaannum) and evidence suggests that life emerged prior to 3.7 Ga. The similarities among all known present-day species indicate that they have diverged through the process of evolution from a common ancestor.


The earliest clear evidence of life comes from biogenic carbon signatures and stromatolite fossils discovered in 3.7 billion-year-old metasedimentary rocks from western Greenland. In 2015, possible "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia. There is further evidence of possibly the oldest forms of life  in the form of fossilized microorganisms in hydrothermal vent precipitates from the Nuvvuagittuq Belt, that may have lived as early as 4.28 billion years ago, not long after the oceans formed 4.4 billion years ago, and after the Earth formed 4.54 ± 0.05 billion years ago. These earliest fossils, however, may have originated from non-biological processes.


Microbial mats of coexisting bacteria and archaea were the dominant form of life in the early Archean eon, and many of the major steps in early evolution are thought to have taken place in this environment. The evolution of photosynthesis by cyanobacteria, around 3.5 Ga, eventually led to a buildup of its waste product, oxygen, in the oceans. After free oxygen saturated all available reductant substances on the Earth's surface, it built up in the atmosphere, leading to the Great Oxygenation Event around 2.4 Ga. The earliest evidence of eukaryotes (complex cells with organelles) dates from 1.85 Ga, likely due to symbiogenesis between anaerobic archaea and aerobic proteobacteria in co-adaptation against the new oxidative stress. While eukaryotes may have been present earlier, their diversification accelerated when aerobic cellular respiration by the endosymbiont mitochondria provided a more abundant source of biological energy. Around 1.6 Ga, some eukaryotes gained the ability to photosynthesize via endosymbiosis with cyanobacteria, and gave rise to various algae that eventually overtook cyanobacteria as the dominant primary producers.


At around 1.7 Ga, multicellular organisms began to appear, with differentiated cells performing specialized functions. While early organisms reproduced asexually, the primary method of reproduction for the vast majority of macroscopic organisms, including almost all eukaryotes (which includes animals and plants), is sexual reproduction, the fusion of male and female reproductive cells (gametes) to create a zygote. The origin and evolution of sexual reproduction remain a puzzle for biologists, though it is thought to have evolved from a single-celled eukaryotic ancestor.


While microorganisms formed the earliest terrestrial ecosystems at least 2.7 Ga, the evolution of plants from freshwater green algae dates back to about 1 billion years ago. Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event as early tree Archaeopteris drew down CO2 levels, leading to global cooling and lowered sea levels, while their roots increased rock weathering and nutrient run-offs which may have triggered algal bloom anoxic events.


Bilateria, animals having a left and a right side that are mirror images of each other, appeared by 555 Ma (million years ago). Ediacara biota appeared during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 Ma during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land.


The Permian–Triassic extinction event killed most complex species of its time, 252 Ma. During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates; one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods. After the Cretaceous–Paleogene extinction event 66 Ma killed off the non-avian dinosaurs, mammals increased rapidly in size and diversity. Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.


Only a very small percentage of species have been identified: one estimate claims that Earth may have 1 trillion species, because "identifying every microbial species on Earth presents a huge challenge." Only 1.75–1.8 million species have been named and 1.8 million documented in a central database. The currently living species represent less than one percent of all species that have ever lived on Earth.


Earliest history of Earth

The oldest meteorite fragments found on Earth are about 4.54 billion years old; this, coupled primarily with the dating of ancient lead deposits, has put the estimated age of Earth at around that time. The Moon has the same composition as Earth's crust but does not contain an iron-rich core like the Earth's. Many scientists think that about 60–110 million years (period of late stage accretion) after the formation of Solar System, the proto-Earth collided with Theia, a co-orbital, Mars-sized protoplanet originating from the inner Solar System. This collision ejected massive amounts of proto-Earth silicate mantle material into orbit that accreted to form the Moon. Another hypothesis is that the Earth and Moon started to coalesce at the same time but the Earth, having a much stronger gravity than the early Moon, attracted almost all the iron particles in the area.


Until 2001, the oldest rocks found on Earth were about 3.8 billion years old, leading scientists to estimate that the Earth's surface had been molten until then. Accordingly, they named this part of Earth's history the Hadean. However, analysis of zircons formed 4.4 Ga indicates that Earth's crust solidified about 100 million years after the planet's formation and that the planet quickly acquired oceans and an atmosphere, which may have been capable of supporting life.


Evidence from the Moon indicates that, from 4 to 3.8 Ga, it suffered a Late Heavy Bombardment by debris that was left over from the formation of the Solar System, and the Earth should have experienced an even heavier bombardment due to its stronger gravity. While there is no direct evidence of conditions on Earth 4 to 3.8 Ga, there is no reason to think that the Earth was not also affected by this late heavy bombardment. This event may well have stripped away any previous atmosphere and oceans; in this case gases and water from comet impacts may have contributed to their replacement, although outgassing from volcanoes on Earth would have supplied at least half. However, if subsurface microbial life had evolved by this point, it would have survived the bombardment.


Earliest evidence for life on Earth

The earliest identified organisms were minute and relatively featureless, and their fossils looked like small rods that are very difficult to tell apart from structures that arise through abiotic physical processes. The oldest undisputed evidence of life on Earth, interpreted as fossilized bacteria, dates to 3 Ga. Other finds in rocks dated to about 3.5 Ga have been interpreted as bacteria, with geochemical evidence also seeming to show the presence of life 3.8 Ga. However, these analyses were closely scrutinized, and non-biological processes were found which could produce all of the "signatures of life" that had been reported. While this does not prove that the structures found had a non-biological origin, they cannot be taken as clear evidence for the presence of life. Geochemical signatures from rocks deposited 3.4 Ga have been interpreted as evidence for life.


Evidence for fossilized microorganisms considered to be 3.77 billion to 4.28 billion years old was found in the Nuvvuagittuq Greenstone Belt in Quebec, Canada, although the evidence is disputed as inconclusive.


Origins of life on Earth

Most biologists reason that all living organisms on Earth must share a single last universal ancestor, because it would be virtually impossible that two or more separate lineages could have independently developed the many complex biochemical mechanisms common to all living organisms.


According to a different scenario a single last universal ancestor, e.g. a "first cell" or a first individual precursor cell has never existed. Instead, the early biochemical evolution of life led to diversification through the development of a multiphenotypic population of pre-cells from which the precursor cells (protocells) of the three domains of life emerged. Thus, the formation of cells was a successive process. See § Metabolism first: Pre-cells, successive cellularisation, below.


Independent emergence on Earth

Life on Earth is based on carbon and water. Carbon provides stable frameworks for complex chemicals and can be easily extracted from the environment, especially from carbon dioxide. There is no other chemical element whose properties are similar enough to carbon's to be called an analogue; silicon, the element directly below carbon on the periodic table, does not form very many complex stable molecules, and because most of its compounds are water-insoluble and because silicon dioxide is a hard and abrasive solid in contrast to carbon dioxide at temperatures associated with living things, it would be more difficult for organisms to extract. The elements boron and phosphorus have more complex chemistries but suffer from other limitations relative to carbon. Water is an excellent solvent and has two other useful properties: the fact that ice floats enables aquatic organisms to survive beneath it in winter; and its molecules have electrically negative and positive ends, which enables it to form a wider range of compounds than other solvents can. Other good solvents, such as ammonia, are liquid only at such low temperatures that chemical reactions may be too slow to sustain life, and lack water's other advantages. Organisms based on alternative biochemistry may, however, be possible on other planets.


Research on how life might have emerged from non-living chemicals focuses on three possible starting points: self-replication, an organism's ability to produce offspring that are very similar to itself; metabolism, its ability to feed and repair itself; and external cell membranes, which allow food to enter and waste products to leave, but exclude unwanted substances. Research on abiogenesis still has a long way to go, since theoretical and empirical approaches are only beginning to make contact with each other.


Replication first: RNA world

Even the simplest members of the three modern domains of life use DNA to record their "recipes" and a complex array of RNA and protein molecules to "read" these instructions and use them for growth, maintenance and self-replication. The discovery that some RNA molecules can catalyze both their own replication and the construction of proteins led to the hypothesis of earlier life-forms based entirely on RNA. These ribozymes could have formed an RNA world in which there were individuals but no species, as mutations and horizontal gene transfers would have meant that offspring were likely to have different genomes from their parents, and evolution occurred at the level of genes rather than organisms. RNA would later have been replaced by DNA, which can build longer, more stable genomes, strengthening heritability and expanding the capabilities of individual organisms. Ribozymes remain as the main components of ribosomes, the "protein factories" in modern cells. Evidence suggests the first RNA molecules formed on Earth prior to 4.17 Ga.


Although short self-replicating RNA molecules have been artificially produced in laboratories, doubts have been raised about whether natural non-biological synthesis of RNA is possible. The earliest "ribozymes" may have been formed of simpler nucleic acids such as PNA, TNA or GNA, which would have been replaced later by RNA.


In 2003, it was proposed that porous metal sulfide precipitates would assist RNA synthesis at about 100 °C (212 °F) and ocean-bottom pressures near hydrothermal vents. Under this hypothesis, lipid membranes would be the last major cell components to appear and, until then, the protocells would be confined to the pores.


Membranes first: Lipid world

It has been suggested that double-walled "bubbles" of lipids like those that form the external membranes of cells may have been an essential first step. Experiments that simulated the conditions of the early Earth have reported the formation of lipids, and these can spontaneously form liposomes, double-walled "bubbles", and then reproduce themselves. Although they are not intrinsically information-carriers as nucleic acids are, they would be subject to natural selection for longevity and reproduction. Nucleic acids such as RNA might then have formed more easily within the liposomes than outside.


The clay hypothesis

RNA is complex and there are doubts about whether it can be produced non-biologically in the wild. Some clays, notably montmorillonite, have properties that make them plausible accelerators for the emergence of an RNA world: they grow by self-replication of their crystalline pattern; they are subject to an analogue of natural selection, as the clay "species" that grows fastest in a particular environment rapidly becomes dominant; and they can catalyze the formation of RNA molecules. Although this idea has not become the scientific consensus, it still has active supporters.


Research in 2003 reported that montmorillonite could also accelerate the conversion of fatty acids into "bubbles" and that the "bubbles" could encapsulate RNA attached to the clay. These "bubbles" can then grow by absorbing additional lipids and then divide. The formation of the earliest cells may have been aided by similar processes.


A similar hypothesis presents self-replicating iron-rich clays as the progenitors of nucleotides, lipids and amino acids.


Metabolism first: Iron–sulfur world





A series of experiments starting in 1997 showed that early stages in the formation of proteins from inorganic materials including carbon monoxide and hydrogen sulfide could be achieved by using iron sulfide and nickel sulfide as catalysts. Most of the steps required temperatures of about 100 °C (212 °F) and moderate pressures, although one stage required 250 °C (482 °F) and a pressure equivalent to that found under 7 kilometres (4.3 mi) of rock. Hence it was suggested that self-sustaining synthesis of proteins could have occurred near hydrothermal vents.

Metabolism first: Pre–cells (successive cellularization)

In this scenario, the biochemical evolution of life led to diversification through the development of a multiphenotypic population of pre-cells, i.e. evolving entities of primordial life with different characteristics and widespread horizontal gene transfer.


From this pre-cell population the founder groups A, B, C and then, from them, the precursor cells (here named proto-cells) of the three domains of life arose successively, leading first to the domain Bacteria, then to the domain Archea and finally to the domain Eucarya.


For the development of cells (cellularization), the pre-cells had to be protected from their surroundings by envelopes (i.e. membranes, walls). For instance, the development of rigid cell walls by the invention of peptidoglycan in bacteria (domain Bacteria) may have been a prerequisite for their successful survival, radiation and colonization of virtually all habitats of the geosphere and hydrosphere.


This scenario may explain the quasi-random distribution of evolutionarily important features among the three domains and, at the same time, the existence of the most basic biochemical features (genetic code, set of protein amino acids etc.) in all three domains (unity of life), as well as the close relationship between the Archaea and the Eucarya. A scheme of the pre-cell scenario is shown in the adjacent figure, where important evolutionary improvements are indicated by numbers.


Prebiotic environments

Geothermal springs

Wet-dry cycles at geothermal springs are shown to solve the problem of hydrolysis and promote the polymerization and vesicle encapsulation of biopolymers. The temperatures of geothermal springs are suitable for biomolecules. Silica minerals and metal sulfides in these environments have photocatalytic properties to catalyze biomolecules. Solar UV exposure also promotes the synthesis of biomolecules like RNA nucleotides. An analysis of hydrothermal veins at a 3.5 Gya (giga years ago or 1 billion years) geothermal spring setting were found to have elements required for the origin of life, which are potassium, boron, hydrogen, sulfur, phosphorus, zinc, nitrogen, and oxygen. Mulkidjanian and colleagues find that such environments have identical ionic concentrations to the cytoplasm of modern cells. Fatty acids in acidic or slightly alkaline geothermal springs assemble into vesicles after wet-dry cycles as there is a lower concentration of ionic solutes at geothermal springs since they are freshwater environments, in contrast to seawater which has a higher concentration of ionic solutes. For organic compounds to be present at geothermal springs, they would have likely been transported by carbonaceous meteors. The molecules that fell from the meteors were then accumulated in geothermal springs. Geothermal springs can accumulate aqueous phosphate in the form of phosphoric acid. Based on lab-run models, these concentrations of phosphate are insufficient to facilitate biosynthesis. As for the evolutionary implications, freshwater heterotrophic cells that depended upon synthesized organic compounds later evolved photosynthesis because of the continuous exposure to sunlight as well as their cell walls with ion pumps to maintain their intracellular metabolism after they entered the oceans.


Deep sea hydrothermal vents

Catalytic mineral particles and transition metal sulfides at these environments are capable of catalyzing organic compounds. Scientists simulated laboratory conditions that were identical to white smokers and successfully oligomerized RNA, measured to be 4 units long. Long chain fatty acids can be synthesized via Fischer-Tropsch synthesis. Another experiment that replicated conditions also similar white smokers, with long chain fatty acids present resulted in the assembly of vesicles. Exergonic reactions at hydrothermal vents are suggested to have been a source of free energy that promoted chemical reactions, synthesis of organic molecules, and are inducive to chemical gradients. In small rock pore systems, membranous structures between alkaline seawater and the acidic ocean would be conducive to natural proton gradients. Nucleobase synthesis could occur by following universally conserved biochemical pathways by using metal ions as catalysts. RNA molecules of 22 bases can be polymerized in alkaline hydrothermal vent pores. Thin pores are shown to only accumulate long polynucleotides whereas thick pores accumulate both short and long polynucleotides. Small mineral cavities or mineral gels could have been a compartment for abiogenic processes. A genomic analysis supports this hypothesis as they found 355 genes that likely traced to LUCA upon 6.1 million sequenced prokaryotic genes. They reconstruct LUCA as a thermophilic anaerobe with a Wood-Ljungdahl pathway, implying an origin of life at white smokers. LUCA would also have exhibited other biochemical pathways such as gluconeogenesis, reverse incomplete Krebs cycle, glycolysis, and the pentose phosphate pathway, including biochemical reactions such as reductive amination and transamination.


Carbonate-rich lakes

One theory traces the origins of life to the abundant carbonate-rich lakes which would have dotted the early Earth. Phosphate would have been an essential cornerstone to the origin of life since it is a critical component of nucleotides, phospholipids, and adenosine triphosphate. Phosphate is often depleted in natural environments due to its uptake by microbes and its affinity for calcium ions. In a process called 'apatite precipitation', free phosphate ions react with the calcium ions abundant in water to precipitate out of solution as apatite minerals. When attempting to simulate prebiotic phosphorylation, scientists have only found success when using phosphorus levels far above modern day natural concentrations.


This problem of low phosphate is solved in carbonate-rich environments. When in the presence of carbonate, calcium readily reacts to form calcium carbonate instead of apatite minerals. With the free calcium ions removed from solution, phosphate ions are no longer precipitated from solution. This is specifically seen in lakes with no inflow, since no new calcium is introduced into the water body. After all of the calcium is sequestered into calcium carbonate (calcite), phosphate concentrations are able to increase to levels necessary for facilitating biomolecule creation.


Though carbonate-rich lakes have alkaline chemistry in modern times, models suggest that carbonate lakes had a pH low enough for prebiotic synthesis when placed in the acidifying context of Earth's early carbon dioxide rich atmosphere. Rainwater rich in carbonic acid weathered the rock on the surface of the Earth at rates far greater than today. With high phosphate influx, no phosphate precipitation, and no microbial usage of phosphate at this time, models show phosphate reached concentrations approximately 100 times greater than they are today. Modeled pH and phosphate levels of early Earth carbonate-rich lakes nearly match the conditions used in current laboratory experiments on the origin of life.


Similar to the process predicted by geothermal hot spring hypotheses, changing lake levels and wave action deposited phosphorus-rich brine onto dry shore and marginal pools. This drying of the solution promotes polymerization reactions and removes enough water to promote phosphorylation, a process integral to biological energy storage and transfer. When washed away by further precipitation and wave action, researchers concluded these newly formed biomolecules may have washed back into the lake - allowing the first prebiotic syntheses on Earth to occur.


Life "seeded" from elsewhere

The Panspermia hypothesis does not explain how life arose originally, but simply examines the possibility of its coming from somewhere other than Earth. The idea that life on Earth was "seeded" from elsewhere in the Universe dates back at least to the Greek philosopher Anaximander in the sixth century BCE. In the twentieth century it was proposed by the physical chemist Svante Arrhenius, by the astronomers Fred Hoyle and Chandra Wickramasinghe, and by molecular biologist Francis Crick and chemist Leslie Orgel.


There are three main versions of the "seeded from elsewhere" hypothesis: from elsewhere in the Solar System via fragments knocked into space by a large meteor impact, in which case the most credible sources are Mars and Venus; by alien visitors, possibly as a result of accidental contamination by microorganisms that they brought with them; and from outside the Solar System but by natural means.


Experiments in low Earth orbit, such as EXOSTACK, have demonstrated that some microorganism spores can survive the shock of being catapulted into space and some can survive exposure to outer space radiation for at least 5.7 years. Meteorite ALH84001, which was once part of the Martian crust, shows evidence of carbonate-globules with texture and size indicative of terrestrial bacterial activity. Scientists are divided over the likelihood of life arising independently on Mars, or on other planets in our galaxy.


Environmental and evolutionary impact of microbial mats

Microbial mats are multi-layered, multi-species colonies of bacteria and other organisms that are generally only a few millimeters thick, but still contain a wide range of chemical environments, each of which favors a different set of microorganisms. To some extent each mat forms its own food chain, as the by-products of each group of microorganisms generally serve as "food" for adjacent groups.


Stromatolites are stubby pillars built as microorganisms in mats slowly migrate upwards to avoid being smothered by sediment deposited on them by water. There has been vigorous debate about the validity of alleged stromatolite fossils from before 3 Ga, with critics arguing that they could have been formed by non-biological processes. In 2006, another find of stromatolites was reported from the same part of Australia, in rocks dated to 3.5 Ga.


In modern underwater mats the top layer often consists of photosynthesizing cyanobacteria which create an oxygen-rich environment, while the bottom layer is oxygen-free and often dominated by hydrogen sulfide emitted by the organisms living there. Oxygen is toxic to organisms that are not adapted to it, but greatly increases the metabolic efficiency of oxygen-adapted organisms; oxygenic photosynthesis by bacteria in mats increased biological productivity by a factor of between 100 and 1,000. The source of hydrogen atoms used by oxygenic photosynthesis is water, which is much more plentiful than the geologically produced reducing agents required by the earlier non-oxygenic photosynthesis. From this point onwards life itself produced significantly more of the resources it needed than did geochemical processes.


Oxygen became a significant component of Earth's atmosphere about 2.4 Ga. Although eukaryotes may have been present much earlier, the oxygenation of the atmosphere was a prerequisite for the evolution of the most complex eukaryotic cells, from which all multicellular organisms are built. The boundary between oxygen-rich and oxygen-free layers in microbial mats would have moved upwards when photosynthesis shut down overnight, and then downwards as it resumed on the next day. This would have created selection pressure for organisms in this intermediate zone to acquire the ability to tolerate and then to use oxygen, possibly via endosymbiosis, where one organism lives inside another and both of them benefit from their association.


Cyanobacteria have the most complete biochemical "toolkits" of all the mat-forming organisms. Hence they are the most self-sufficient, well-adapted to strike out on their own both as floating mats and as the first of the phytoplankton, provide the basis of most marine food chains.


Diversification of eukaryotes

Chromatin, nucleus, endomembrane system, and mitochondria

Eukaryotes may have been present long before the oxygenation of the atmosphere, but most modern eukaryotes require oxygen, which is used by their mitochondria to fuel the production of ATP, the internal energy supply of all known cells. In the 1970s, a vigorous debate concluded that eukaryotes emerged as a result of a sequence of endosymbiosis between prokaryotes. For example: a predatory microorganism invaded a large prokaryote, probably an archaean, but instead of killing its prey, the attacker took up residence and evolved into mitochondria; one of these chimeras later tried to swallow a photosynthesizing cyanobacterium, but the victim survived inside the attacker and the new combination became the ancestor of plants; and so on. After each endosymbiosis, the partners eventually eliminated unproductive duplication of genetic functions by rearranging their genomes, a process which sometimes involved transfer of genes between them. Another hypothesis proposes that mitochondria were originally sulfur- or hydrogen-metabolizing endosymbionts, and became oxygen-consumers later. On the other hand, mitochondria might have been part of eukaryotes' original equipment.


There is a debate about when eukaryotes first appeared: the presence of steranes in Australian shales may indicate eukaryotes at 2.7 Ga; however, an analysis in 2008 concluded that these chemicals infiltrated the rocks less than 2.2 Ga and prove nothing about the origins of eukaryotes. Fossils of the algae Grypania have been reported in 1.85 billion-year-old rocks (originally dated to 2.1 Ga but later revised), indicating that eukaryotes with organelles had already evolved. A diverse collection of fossil algae were found in rocks dated between 1.5 and 1.4 Ga. The earliest known fossils of fungi date from 1.43 Ga.


Plastids

Plastids, the superclass of organelles of which chloroplasts are the best-known exemplar, are thought to have originated from endosymbiotic cyanobacteria. The symbiosis evolved around 1.5 Ga and enabled eukaryotes to carry out oxygenic photosynthesis. Three evolutionary lineages of photosynthetic plastids have since emerged: chloroplasts in green algae and plants, rhodoplasts in red algae and cyanelles in the glaucophytes. Not long after this primary endosymbiosis of plastids, rhodoplasts, and chloroplasts were passed down to other bikonts, establishing a eukaryotic assemblage of phytoplankton by the end of the Neoproterozoic Eon.


Sexual reproduction and multicellular organisms

Evolution of sexual reproduction

The defining characteristics of sexual reproduction in eukaryotes are meiosis and fertilization, resulting in genetic recombination, giving offspring 50% of their genes from each parent. By contrast, in asexual reproduction there is no recombination, but occasional horizontal gene transfer. Bacteria also exchange DNA by bacterial conjugation, enabling the spread of resistance to antibiotics and other toxins, and the ability to utilize new metabolites. However, conjugation is not a means of reproduction, and is not limited to members of the same species – there are cases where bacteria transfer DNA to plants and animals.


On the other hand, bacterial transformation is clearly an adaptation for transfer of DNA between bacteria of the same species. This is a complex process involving the products of numerous bacterial genes and can be regarded as a bacterial form of sex. This process occurs naturally in at least 67 prokaryotic species (in seven different phyla). Sexual reproduction in eukaryotes may have evolved from bacterial transformation.


The disadvantages of sexual reproduction are well-known: the genetic reshuffle of recombination may break up favorable combinations of genes; and since males do not directly increase the number of offspring in the next generation, an asexual population can out-breed and displace in as little as 50 generations a sexual population that is equal in every other respect. Nevertheless, the great majority of animals, plants, fungi and protists reproduce sexually. There is strong evidence that sexual reproduction arose early in the history of eukaryotes and that the genes controlling it have changed very little since then. How sexual reproduction evolved and survived is an unsolved puzzle.


The Red Queen hypothesis suggests that sexual reproduction provides protection against parasites, because it is easier for parasites to evolve means of overcoming the defenses of genetically identical clones than those of sexual species that present moving targets, and there is some experimental evidence for this. However, there is still doubt about whether it would explain the survival of sexual species if multiple similar clone species were present, as one of the clones may survive the attacks of parasites for long enough to out-breed the sexual species. Furthermore, contrary to the expectations of the Red Queen hypothesis, Kathryn A. Hanley et al. found that the prevalence, abundance and mean intensity of mites was significantly higher in sexual geckos than in asexuals sharing the same habitat. In addition, biologist Matthew Parker, after reviewing numerous genetic studies on plant disease resistance, failed to find a single example consistent with the concept that pathogens are the primary selective agent responsible for sexual reproduction in the host.


Alexey Kondrashov's deterministic mutation hypothesis (DMH) assumes that each organism has more than one harmful mutation and that the combined effects of these mutations are more harmful than the sum of the harm done by each individual mutation. If so, sexual recombination of genes will reduce the harm that bad mutations do to offspring and at the same time eliminate some bad mutations from the gene pool by isolating them in individuals that perish quickly because they have an above-average number of bad mutations. However, the evidence suggests that the DMH's assumptions are shaky because many species have on average less than one harmful mutation per individual and no species that has been investigated shows evidence of synergy between harmful mutations.


The random nature of recombination causes the relative abundance of alternative traits to vary from one generation to another. This genetic drift is insufficient on its own to make sexual reproduction advantageous, but a combination of genetic drift and natural selection may be sufficient. When chance produces combinations of good traits, natural selection gives a large advantage to lineages in which these traits become genetically linked. On the other hand, the benefits of good traits are neutralized if they appear along with bad traits. Sexual recombination gives good traits the opportunities to become linked with other good traits, and mathematical models suggest this may be more than enough to offset the disadvantages of sexual reproduction. Other combinations of hypotheses that are inadequate on their own are also being examined.


The adaptive function of sex remains a major unresolved issue in biology. The competing models to explain it were reviewed by John A. Birdsell and Christopher Wills. The hypotheses discussed above all depend on the possible beneficial effects of random genetic variation produced by genetic recombination. An alternative view is that sex arose and is maintained as a process for repairing DNA damage, and that the genetic variation produced is an occasionally beneficial byproduct.


Multicellularity

The simplest definitions of "multicellular", for example "having multiple cells", could include colonial cyanobacteria like Nostoc. Even a technical definition such as "having the same genome but different types of cell" would still include some genera of the green algae Volvox, which have cells that specialize in reproduction. Multicellularity evolved independently in organisms as diverse as sponges and other animals, fungi, plants, brown algae, cyanobacteria, slime molds and myxobacteria. For the sake of brevity, this article focuses on the organisms that show the greatest specialization of cells and variety of cell types, although this approach to the evolution of biological complexity could be regarded as "rather anthropocentric".


The initial advantages of multicellularity may have included: more efficient sharing of nutrients that are digested outside the cell, increased resistance to predators, many of which attacked by engulfing; the ability to resist currents by attaching to a firm surface; the ability to reach upwards to filter-feed or to obtain sunlight for photosynthesis; the ability to create an internal environment that gives protection against the external one; and even the opportunity for a group of cells to behave "intelligently" by sharing information. These features would also have provided opportunities for other organisms to diversify, by creating more varied environments than flat microbial mats could.


Multicellularity with differentiated cells is beneficial to the organism as a whole but disadvantageous from the point of view of individual cells, most of which lose the opportunity to reproduce themselves. In an asexual multicellular organism, rogue cells which retain the ability to reproduce may take over and reduce the organism to a mass of undifferentiated cells. Sexual reproduction eliminates such rogue cells from the next generation and therefore appears to be a prerequisite for complex multicellularity.


The available evidence indicates that eukaryotes evolved much earlier but remained inconspicuous until a rapid diversification around 1 Ga. The only respect in which eukaryotes clearly surpass bacteria and archaea is their capacity for variety of forms, and sexual reproduction enabled eukaryotes to exploit that advantage by producing organisms with multiple cells that differed in form and function.


By comparing the composition of transcription factor families and regulatory network motifs between unicellular organisms and multicellular organisms, scientists found there are many novel transcription factor families and three novel types of regulatory network motifs in multicellular organisms, and novel family transcription factors are preferentially wired into these novel network motifs which are essential for multicellular development. These results propose a plausible mechanism for the contribution of novel-family transcription factors and novel network motifs to the origin of multicellular organisms at transcriptional regulatory level.


Fossil evidence

Fungi-like fossils were found in vesicular basalt dating back to the Paleoproterozoic Era around 2.4 billion years ago. The controversial Francevillian biota fossils, dated to 2.1 Ga, are the earliest known fossil organisms that are clearly multicellular, if they are indeed fossils. They may have had differentiated cells. Another early multicellular fossil, Qingshania, dated to 1.7 Ga, appears to consist of virtually identical cells. The red algae called Bangiomorpha, dated at 1.2 Ga, is the earliest known organism that certainly has differentiated, specialized cells, and is also the oldest known sexually reproducing organism. The 1.43 billion-year-old fossils interpreted as fungi appear to have been multicellular with differentiated cells. The "string of beads" organism Horodyskia, found in rocks dated from 1.5 Ga to 900 Ma, may have been an early metazoan; however, it has also been interpreted as a colonial foraminifera.


Emergence of animals

Deuterostomes


Ecdysozoa


Spiralia


Xenacoelomorpha


Cnidaria


Placozoa


Ctenophora


Porifera


Animals are multicellular eukaryotes, and are distinguished from plants, algae, and fungi by lacking cell walls. All animals are motile, if only at certain life stages. All animals except sponges have bodies differentiated into separate tissues, including muscles, which move parts of the animal by contracting, and nerve tissue, which transmits and processes signals. In November 2019, researchers reported the discovery of Caveasphaera, a multicellular organism found in 609-million-year-old rocks, that is not easily defined as an animal or non-animal, which may be related to one of the earliest instances of animal evolution. Fossil studies of Caveasphaera have suggested that animal-like embryonic development arose much earlier than the oldest clearly defined animal fossils. and may be consistent with studies suggesting that animal evolution may have begun about 750 million years ago.


Nonetheless, the earliest widely accepted animal fossils are the rather modern-looking cnidarians (the group that includes jellyfish, sea anemones and Hydra), possibly from around 580 Ma, although fossils from the Doushantuo Formation can only be dated approximately. Their presence implies that the cnidarian and bilaterian lineages had already diverged.


The Ediacara biota, which flourished for the last 40 million years before the start of the Cambrian, were the first animals more than a very few centimeters long. Many were flat and had a "quilted" appearance, and seemed so strange that there was a proposal to classify them as a separate kingdom, Vendozoa. Others, however, have been interpreted as early molluscs (Kimberella), echinoderms (Arkarua), and arthropods (Spriggina, Parvancorina). There is still debate about the classification of these specimens, mainly because the diagnostic features which allow taxonomists to classify more recent organisms, such as similarities to living organisms, are generally absent in the Ediacarans. However, there seems little doubt that Kimberella was at least a triploblastic bilaterian animal, in other words, an animal significantly more complex than the cnidarians.


The small shelly fauna are a very mixed collection of fossils found between the Late Ediacaran and Middle Cambrian periods. The earliest, Cloudina, shows signs of successful defense against predation and may indicate the start of an evolutionary arms race. Some tiny Early Cambrian shells almost certainly belonged to molluscs, while the owners of some "armor plates", Halkieria and Microdictyon, were eventually identified when more complete specimens were found in Cambrian lagerstätten that preserved soft-bodied animals.


In the 1970s there was already a debate about whether the emergence of the modern phyla was "explosive" or gradual but hidden by the shortage of Precambrian animal fossils. A re-analysis of fossils from the Burgess Shale lagerstätte increased interest in the issue when it revealed animals, such as Opabinia, which did not fit into any known phylum. At the time these were interpreted as evidence that the modern phyla had evolved very rapidly in the Cambrian explosion and that the Burgess Shale's "weird wonders" showed that the Early Cambrian was a uniquely experimental period of animal evolution. Later discoveries of similar animals and the development of new theoretical approaches led to the conclusion that many of the "weird wonders" were evolutionary "aunts" or "cousins" of modern groups—for example that Opabinia was a member of the lobopods, a group which includes the ancestors of the arthropods, and that it may have been closely related to the modern tardigrades. Nevertheless, there is still much debate about whether the Cambrian explosion was really explosive and, if so, how and why it happened and why it appears unique in the history of animals.


Deuterostomes and the first vertebrates

Most of the animals at the heart of the Cambrian explosion debate were protostomes, one of the two main groups of complex animals. The other major group, the deuterostomes, contains invertebrates such as starfish and sea urchins (echinoderms), as well as chordates (see below). Many echinoderms have hard calcite "shells", which are fairly common from the Early Cambrian small shelly fauna onwards. Other deuterostome groups are soft-bodied, and most of the significant Cambrian deuterostome fossils come from the Chengjiang fauna, a lagerstätte in China. The chordates are another major deuterostome group: animals with a distinct dorsal nerve cord. Chordates include soft-bodied invertebrates such as tunicates as well as vertebrates—animals with a backbone. While tunicate fossils predate the Cambrian explosion, the Chengjiang fossils Haikouichthys and Myllokunmingia appear to be true vertebrates, and Haikouichthys had distinct vertebrae, which may have been slightly mineralized. Vertebrates with jaws, such as the acanthodians, first appeared in the Late Ordovician.


Colonization of land

Adaptation to life on land is a major challenge: all land organisms need to avoid drying-out and all those above microscopic size must create special structures to withstand gravity; respiration and gas exchange systems have to change; reproductive systems cannot depend on water to carry eggs and sperm towards each other. Although the earliest good evidence of land plants and animals dates back to the Ordovician period (488 to 444 Ma), and a number of microorganism lineages made it onto land much earlier, modern land ecosystems only appeared in the Late Devonian, about 385 to 359 Ma. In May 2017, evidence of the earliest known life on land may have been found in 3.48-billion-year-old geyserite and other related mineral deposits (often found around hot springs and geysers) uncovered in the Pilbara Craton of Western Australia. In July 2018, scientists reported that the earliest life on land may have been bacteria living on land 3.22 billion years ago. In May 2019, scientists reported the discovery of a fossilized fungus, named Ourasphaira giraldae, in the Canadian Arctic, that may have grown on land a billion years ago, well before plants were living on land.


Evolution of terrestrial antioxidants

Oxygen began to accumulate in Earth's atmosphere over 3 Ga, as a by-product of photosynthesis in cyanobacteria (blue-green algae). However, oxygen produces destructive chemical oxidation which was toxic to most previous organisms. Protective endogenous antioxidant enzymes and exogenous dietary antioxidants helped to prevent oxidative damage. For example, brown algae accumulate inorganic mineral antioxidants such as rubidium, vanadium, zinc, iron, copper, molybdenum, selenium and iodine, concentrated more than 30,000 times more than in seawater. Most marine mineral antioxidants act in the cells as essential trace elements in redox and antioxidant metalloenzymes.


When plants and animals began to enter rivers and land about 500 Ma, environmental deficiency of these marine mineral antioxidants was a challenge to the evolution of terrestrial life. Terrestrial plants slowly optimized the production of new endogenous antioxidants such as ascorbic acid, polyphenols, flavonoids, tocopherols, etc.


A few of these appeared more recently, in the last 200–50 Ma, in fruits and flowers of angiosperm plants. In fact, angiosperms (the dominant type of plant today) and most of their antioxidant pigments evolved during the Late Jurassic period. Plants employ antioxidants to defend their structures against reactive oxygen species produced during photosynthesis. Animals are exposed to the same oxidants, and they have evolved endogenous enzymatic antioxidant systems. Iodine in the form of the iodide ion I− is the most primitive and abundant electron-rich essential element in the diet of marine and terrestrial organisms; it acts as an electron donor and has this ancestral antioxidant function in all iodide-concentrating cells, from primitive marine algae to terrestrial vertebrates.


Evolution of soil

Before the colonization of land there was no soil, a combination of mineral particles and decomposed organic matter. Land surfaces were either bare rock or shifting sand produced by weathering. Water and dissolved nutrients would have drained away very quickly. In the Sub-Cambrian peneplain in Sweden, for example, maximum depth of kaolinitization by Neoproterozoic weathering is about 5 m, while nearby kaolin deposits developed in the Mesozoic are much thicker. It has been argued that in the late Neoproterozoic sheet wash was a dominant process of erosion of surface material due to the lack of plants on land.


Films of cyanobacteria, which are not plants but use the same photosynthesis mechanisms, have been found in modern deserts in areas unsuitable for vascular plants. This suggests that microbial mats may have been the first organisms to colonize dry land, possibly in the Precambrian. Mat-forming cyanobacteria could have gradually evolved resistance to desiccation as they spread from the seas to intertidal zones and then to land. Lichens, which are symbiotic combinations of a fungus (almost always an ascomycete) and one or more photosynthesizers (green algae or cyanobacteria), are also important colonizers of lifeless environments, and their ability to break down rocks contributes to soil formation where plants cannot survive. The earliest known ascomycete fossils date from 423 to 419 Ma in the Silurian.


Soil formation would have been very slow until the appearance of burrowing animals, which mix the mineral and organic components of soil and whose feces are a major source of organic components. Burrows have been found in Ordovician sediments, and are attributed to annelids (worms) or arthropods.


Plants and the Late Devonian wood crisis

In aquatic algae, almost all cells are capable of photosynthesis and are nearly independent. Life on land requires plants to become internally more complex and specialized: photosynthesis is most efficient at the top; roots extract water and nutrients from the ground; and the intermediate parts support and transport.


Spores of land plants resembling liverworts have been found in Middle Ordovician rocks from 476 Ma. Middle Silurian rocks from 430 Ma contain fossils of true plants, including clubmosses such as Baragwanathia; most were under 10 centimetres (3.9 in) high, and some appear closely related to vascular plants, the group that includes trees.


By the Late Devonian 370 Ma, abundant trees such as Archaeopteris bound the soil so firmly that they changed river systems from mostly braided to mostly meandering. This caused the "Late Devonian wood crisis" because:


Land invertebrates

Animals had to change their feeding and excretory systems, and most land animals developed internal fertilization of their eggs. The difference in refractive index between water and air required changes in their eyes. On the other hand, in some ways movement and breathing became easier, and the better transmission of high-frequency sounds in the air encouraged the development of hearing.


The oldest animal with evidence of air-breathing, although not being the oldest myriapod fossil record, is Pneumodesmus, an archipolypodan millipede from the Early Devonian, about 414 Ma. Its air-breathing, terrestrial nature is evidenced by the presence of spiracles, the openings to tracheal systems. However, some earlier trace fossils from the Cambrian-Ordovician boundary about 490 Ma are interpreted as the tracks of large amphibious arthropods on coastal sand dunes, and may have been made by euthycarcinoids, which are thought to be evolutionary "aunts" of myriapods. Other trace fossils from the Late Ordovician a little over 445 Ma probably represent land invertebrates, and there is clear evidence of numerous arthropods on coasts and alluvial plains shortly before the Silurian-Devonian boundary, about 415 Ma, including signs that some arthropods ate plants. Arthropods were well pre-adapted to colonize land, because their existing jointed exoskeletons provided protection against desiccation, support against gravity and a means of locomotion that was not dependent on water.


The fossil record of other major invertebrate groups on land is poor: none at all for non-parasitic flatworms, nematodes or nemerteans; some parasitic nematodes have been fossilized in amber; annelid worm fossils are known from the Carboniferous, but they may still have been aquatic animals; the earliest fossils of gastropods on land date from the Late Carboniferous, and this group may have had to wait until leaf litter became abundant enough to provide the moist conditions they need.


The earliest confirmed fossils of flying insects date from the Late Carboniferous, but it is thought that insects developed the ability to fly in the Early Carboniferous or even Late Devonian. This gave them a wider range of ecological niches for feeding and breeding, and a means of escape from predators and from unfavorable changes in the environment. About 99% of modern insect species fly or are descendants of flying species.


Amphibians

Osteolepiformes


Panderichthyidae


Obruchevichthidae


Acanthostega


Ichthyostega


Tulerpeton


Early labyrinthodonts


Anthracosauria


Amniotes


Tetrapods, vertebrates with four limbs, evolved from other rhipidistian fish over a relatively short timespan during the Late Devonian (370 to 360 Ma). The early groups are grouped together as Labyrinthodontia. They retained aquatic, fry-like tadpoles, a system still seen in modern amphibians.


Iodine and T4/T3 stimulate the amphibian metamorphosis and the evolution of nervous systems transforming the aquatic, vegetarian tadpole into a "more evolved" terrestrial, carnivorous frog with better neurological, visuospatial, olfactory and cognitive abilities for hunting. The new hormonal action of T3 was made possible by the formation of T3-receptors in the cells of vertebrates. First, about 600–500 million years ago, the alpha T3-receptors with a metamorphosing action appeared in primitive chordates and then, about 250–150 million years ago, the beta T3-receptors with metabolic and thermogenetic actions appeared in birds and mammals.


From the 1950s to the early 1980s it was thought that tetrapods evolved from fish that had already acquired the ability to crawl on land, possibly in order to go from a pool that was drying out to one that was deeper. However, in 1987, nearly complete fossils of Acanthostega from about 363 Ma showed that this Late Devonian transitional animal had legs and both lungs and gills, but could never have survived on land: its limbs and its wrist and ankle joints were too weak to bear its weight; its ribs were too short to prevent its lungs from being squeezed flat by its weight; its fish-like tail fin would have been damaged by dragging on the ground. The current hypothesis is that Acanthostega, which was about 1 metre (3.3 ft) long, was a wholly aquatic predator that hunted in shallow water. Its skeleton differed from that of most fish, in ways that enabled it to raise its head to breathe air while its body remained submerged, including: its jaws show modifications that would have enabled it to gulp air; the bones at the back of its skull are locked together, providing strong attachment points for muscles that raised its head; the head is not joined to the shoulder girdle and it has a distinct neck.


The Devonian proliferation of land plants may help to explain why air breathing would have been an advantage: leaves falling into streams and rivers would have encouraged the growth of aquatic vegetation; this would have attracted grazing invertebrates and small fish that preyed on them; they would have been attractive prey but the environment was unsuitable for the big marine predatory fish; air-breathing would have been necessary because these waters would have been short of oxygen, since warm water holds less dissolved oxygen than cooler marine water and since the decomposition of vegetation would have used some of the oxygen.


Later discoveries revealed earlier transitional forms between Acanthostega and completely fish-like animals. Unfortunately, there is then a gap (Romer's gap) of about 30 Ma between the fossils of ancestral tetrapods and Middle Carboniferous fossils of vertebrates that look well-adapted for life on land, during which only some fossils are found, which had five digits in the terminating point of the four limbs, showing true or crown tetrapods appeared in the gap around 350 Ma. Some of the fossils after this gap look as if the animals which they belonged to were early relatives of modern amphibians, all of which need to keep their skins moist and to lay their eggs in water, while others are accepted as early relatives of the amniotes, whose waterproof skin and egg membranes enable them to live and breed far from water. The Carboniferous Rainforest Collapse may have paved the way for amniotes to become dominant over amphibians.


Reptiles

Early synapsids (extinct)


Extinct pelycosaurs


Extinct therapsids


Extinct mammaliaforms


Mammals


Anapsids; whether turtles belong here is debated


Captorhinidae and Protorothyrididae (extinct)


Araeoscelidia (extinct)


Squamata (lizards and snakes)


Extinct archosaurs


Crocodilians


Pterosaurs (extinct)


Extincttheropods


Birds


Sauropods(extinct)


Ornithischians (extinct)


Amniotes, whose eggs can survive in dry environments, probably evolved in the Late Carboniferous period (330 to 298.9 Ma). The earliest fossils of the two surviving amniote groups, synapsids and sauropsids, date from around 313 Ma. The synapsid pelycosaurs and their descendants the therapsids are the most common land vertebrates in the best-known Permian (298.9 to 251.9 Ma) fossil beds. However, at the time these were all in temperate zones at middle latitudes, and there is evidence that hotter, drier environments nearer the Equator were dominated by sauropsids and amphibians.


The Permian–Triassic extinction event wiped out almost all land vertebrates, as well as the great majority of other life. During the slow recovery from this catastrophe, estimated to have taken 30 million years, a previously obscure sauropsid group became the most abundant and diverse terrestrial vertebrates: a few fossils of archosauriformes ("ruling lizard forms") have been found in Late Permian rocks, but, by the Middle Triassic, archosaurs were the dominant land vertebrates. Dinosaurs distinguished themselves from other archosaurs in the Late Triassic, and became the dominant land vertebrates of the Jurassic and Cretaceous periods (201.4 to 66 Ma).


Birds

During the Late Jurassic, birds evolved from small, predatory theropod dinosaurs. The first birds inherited teeth and long, bony tails from their dinosaur ancestors, but some had developed horny, toothless beaks by the very Late Jurassic and short pygostyle tails by the Early Cretaceous.


Mammals

While the archosaurs and dinosaurs were becoming more dominant in the Triassic, the mammaliaform successors of the therapsids evolved into small, mainly nocturnal insectivores. This ecological role may have promoted the evolution of mammals, for example nocturnal life may have accelerated the development of endothermy ("warm-bloodedness") and hair or fur. By 195 Ma in the Early Jurassic there were animals that were very like today's mammals in a number of respects. Unfortunately, there is a gap in the fossil record throughout the Middle Jurassic. However, fossil teeth discovered in Madagascar indicate that the split between the lineage leading to monotremes and the one leading to other living mammals had occurred by 167 Ma. After dominating land vertebrate niches for about 150 Ma, the non-avian dinosaurs perished in the Cretaceous–Paleogene extinction event (66 Ma) along with many other groups of organisms. Mammals throughout the time of the dinosaurs had been restricted to a narrow range of taxa, sizes and shapes, but increased rapidly in size and diversity after the extinction, with bats taking to the air within 13 million years, and cetaceans to the sea within 15 million years.


Flowering plants

Gnetales(gymnosperm)


Welwitschia(gymnosperm)


Ephedra(gymnosperm)


Bennettitales


Angiosperms(flowering plants)


Angiosperms(flowering plants)


Cycads(gymnosperm)


Bennettitales


Ginkgo


Gnetales(gymnosperm)


Conifers(gymnosperm)


The first flowering plants appeared around 130 Ma. The 250,000 to 400,000 species of flowering plants outnumber all other ground plants combined, and are the dominant vegetation in most terrestrial ecosystems. There is fossil evidence that flowering plants diversified rapidly in the Early Cretaceous, from 130 to 90 Ma, and that their rise was associated with that of pollinating insects. Among modern flowering plants Magnolia are thought to be close to the common ancestor of the group. However, paleontologists have not succeeded in identifying the earliest stages in the evolution of flowering plants.


Social insects

The social insects are remarkable because the great majority of individuals in each colony are sterile. This appears contrary to basic concepts of evolution such as natural selection and the selfish gene. In fact, there are very few eusocial insect species: only 15 out of approximately 2,600 living families of insects contain eusocial species, and it seems that eusociality has evolved independently only 12 times among arthropods, although some eusocial lineages have diversified into several families. Nevertheless, social insects have been spectacularly successful; for example although ants and termites account for only about 2% of known insect species, they form over 50% of the total mass of insects. Their ability to control a territory appears to be the foundation of their success.


The sacrifice of breeding opportunities by most individuals has long been explained as a consequence of these species' unusual haplodiploid method of sex determination, which has the paradoxical consequence that two sterile worker daughters of the same queen share more genes with each other than they would with their offspring if they could breed. However, E. O. Wilson and Bert Hölldobler argue that this explanation is faulty: for example, it is based on kin selection, but there is no evidence of nepotism in colonies that have multiple queens. Instead, they write, eusociality evolves only in species that are under strong pressure from predators and competitors, but in environments where it is possible to build "fortresses"; after colonies have established this security, they gain other advantages through co-operative foraging. In support of this explanation they cite the appearance of eusociality in bathyergid mole rats, which are not haplodiploid.


The earliest fossils of insects have been found in Early Devonian rocks from about 400 Ma, which preserve only a few varieties of flightless insect. The Mazon Creek lagerstätten from the Late Carboniferous, about 300 Ma, include about 200 species, some gigantic by modern standards, and indicate that insects had occupied their main modern ecological niches as herbivores, detritivores and insectivores. Social termites and ants first appeared in the Early Cretaceous, and advanced social bees have been found in Late Cretaceous rocks but did not become abundant until the Middle Cenozoic.


Humans

The idea that, along with other life forms, modern-day humans evolved from an ancient, common ancestor was proposed by Robert Chambers in 1844 and taken up by Charles Darwin in 1871. Modern humans evolved from a lineage of upright-walking apes that has been traced back over 6 Ma to Sahelanthropus. The first known stone tools were made about 2.5 Ma, apparently by Australopithecus garhi, and were found near animal bones that bear scratches made by these tools. The earliest hominines had chimpanzee-sized brains, but there has been a fourfold increase in the last 3 Ma; a statistical analysis suggests that hominine brain sizes depend almost completely on the date of the fossils, while the species to which they are assigned has only slight influence. There is a long-running debate about whether modern humans evolved all over the world simultaneously from existing advanced hominines or are descendants of a single small population in Africa, which then migrated all over the world less than 200,000 years ago and replaced previous hominine species. There is also debate about whether anatomically modern humans had an intellectual, cultural and technological "Great Leap Forward" under 40,000–50,000 years ago and, if so, whether this was due to neurological changes that are not visible in fossils.


Mass extinctions

Life on Earth has suffered occasional mass extinctions at least since 542 Ma. Although they were disasters at the time, mass extinctions have sometimes accelerated the evolution of life on Earth. When dominance of particular ecological niches passes from one group of organisms to another, it is rarely because the new dominant group is "superior" to the old and usually because an extinction event eliminates the old dominant group and makes way for the new one.


The fossil record appears to show that the gaps between mass extinctions are becoming longer and that the average and background rates of extinction are decreasing. Both of these phenomena could be explained in one or more ways:


Biodiversity in the fossil record, which is "...the number of distinct genera alive at any given time; that is, those whose first occurrence predates and whose last occurrence postdates that time" shows a different trend: a fairly swift rise from 542 to 400 Ma; a slight decline from 400 to 200 Ma, in which the devastating Permian–Triassic extinction event is an important factor; and a swift rise from 200 Ma to the present.


See also

Footnotes

References

Bibliography

Further reading

External links




The alkali metals consist of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). Together with hydrogen they constitute group 1, which lies in the s-block of the periodic table. All alkali metals have their outermost electron in an s-orbital: this shared electron configuration results in them having very similar characteristic properties. Indeed, the alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterised homologous behaviour. This family of elements is also known as the lithium family after its leading element.


The alkali metals are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen (and in the case of lithium, nitrogen). Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free elements. Caesium, the fifth alkali metal, is the most reactive of all the metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.


All of the discovered alkali metals occur in nature as their compounds: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in minute traces in nature as an intermediate step in some obscure side branches of the natural decay chains. Experiments have been conducted to attempt the synthesis of element 119, which is likely to be the next member of the group; none were successful. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.


Most alkali metals have many different applications. One of the best-known applications of the pure elements is the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks form the basis of the second. A common application of the compounds of sodium is the sodium-vapour lamp, which emits light very efficiently. Table salt, or sodium chloride, has been used since antiquity. Lithium finds use as a psychiatric medication and as an anode in lithium batteries. Sodium, potassium and possibly lithium are essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.


History

Sodium compounds have been known since ancient times; salt (sodium chloride) has been an important commodity in human activities. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence which led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri-Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did not include either alkali in his list of chemical elements in 1789.


Pure potassium was first isolated in 1807 in England by Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by the use of electrolysis of the molten salt with the newly invented voltaic pile. Previous attempts at electrolysis of the aqueous salt were unsuccessful due to potassium's extreme reactivity.: 68  Potassium was the first metal that was isolated by electrolysis. Later that same year, Davy reported extraction of sodium from the similar substance caustic soda (NaOH, lye) by a similar technique, demonstrating the elements, and thus the salts, to be different.


Petalite (.mw-parser-output .template-chem2-su{display:inline-block;font-size:80%;line-height:1;vertical-align:-0.35em}.mw-parser-output .template-chem2-su>span{display:block;text-align:left}.mw-parser-output sub.template-chem2-sub{font-size:80%;vertical-align:-0.35em}.mw-parser-output sup.template-chem2-sup{font-size:80%;vertical-align:0.65em}LiAlSi4O10) was discovered in 1800 by the Brazilian chemist José Bonifácio de Andrada in a mine on the island of Utö, Sweden. However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jacob Berzelius, detected the presence of a new element while analysing petalite ore. This new element was noted by him to form compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline than the other alkali metals. Berzelius gave the unknown material the name lithion/lithina, from the Greek word λιθoς (transliterated as lithos, meaning "stone"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood. He named the metal inside the material lithium. Lithium, sodium, and potassium were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang Döbereiner in 1850 as having similar properties.


Rubidium and caesium were the first elements to be discovered using the spectroscope, invented in 1859 by Robert Bunsen and Gustav Kirchhoff. The next year, they discovered caesium in the mineral water from Bad Dürkheim, Germany. Their discovery of rubidium came the following year in Heidelberg, Germany, finding it in the mineral lepidolite. The names of rubidium and caesium come from the most prominent lines in their emission spectra: a bright red line for rubidium (from the Latin word rubidus, meaning dark red or bright red), and a sky-blue line for caesium (derived from the Latin word caesius, meaning sky-blue).


Around 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music, where notes an octave apart have similar musical functions. His version put all the alkali metals then known (lithium to caesium), as well as copper, silver, and thallium (which show the +1 oxidation state characteristic of the alkali metals), together into a group. His table placed hydrogen with the halogens.


After 1869, Dmitri Mendeleev proposed his periodic table placing lithium at the top of a group with sodium, potassium, rubidium, caesium, and thallium. Two years later, Mendeleev revised his table, placing hydrogen in group 1 above lithium, and also moving thallium to the boron group. In this 1871 version, copper, silver, and gold were placed twice, once as part of group IB, and once as part of a "group VIII" encompassing today's groups 8 to 11. After the introduction of the 18-column table, the group IB elements were moved to their current position in the d-block, while alkali metals were left in group IA. Later the group's name was changed to group 1 in 1988. The trivial name "alkali metals" comes from the fact that the hydroxides of the group 1 elements are all strong alkalis when dissolved in water.


There were at least four erroneous and incomplete discoveries before Marguerite Perey of the Curie Institute in Paris, France discovered francium in 1939 by purifying a sample of actinium-227, which had been reported to have a decay energy of 220 keV. However, Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one that was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure that she later revised to 1%.


The next element below francium (eka-francium) in the periodic table would be ununennium (Uue), element 119.: 1729–1730  The synthesis of ununennium was first attempted in 1985 by bombarding a target of einsteinium-254 with calcium-48 ions at the superHILAC accelerator at the Lawrence Berkeley National Laboratory in Berkeley, California. No atoms were identified, leading to a limiting yield of 300 nb.


It is highly unlikely that this reaction will be able to create any atoms of ununennium in the near future, given the extremely difficult task of making sufficient amounts of einsteinium-254, which is favoured for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms, to make a large enough target to increase the sensitivity of the experiment to the required level; einsteinium has not been found in nature and has only been produced in laboratories, and in quantities smaller than those needed for effective synthesis of superheavy elements. However, given that ununennium is only the first period 8 element on the extended periodic table, it may well be discovered in the near future through other reactions, and indeed an attempt to synthesise it is currently ongoing in Japan. Currently, none of the period 8 elements has been discovered yet, and it is also possible, due to drip instabilities, that only the lower period 8 elements, up to around element 128, are physically possible. No attempts at synthesis have been made for any heavier alkali metals: due to their extremely high atomic number, they would require new, more powerful methods and technology to make.: 1737–1739 


Occurrence

In the Solar System

The Oddo–Harkins rule holds that elements with even atomic numbers are more common that those with odd atomic numbers, with the exception of hydrogen and beryllium. This rule argues that elements with odd atomic numbers have one unpaired proton and are more likely to capture another, thus increasing their atomic number. In elements with even atomic numbers, protons are paired, with each member of the pair offsetting the spin of the other, enhancing stability. All the alkali metals have odd atomic numbers and they are not as common as the elements with even atomic numbers adjacent to them (the noble gases and the alkaline earth metals) in the Solar System. The heavier alkali metals are also less abundant than the lighter ones as the alkali metals from rubidium onward can only be synthesised in supernovae and not in stellar nucleosynthesis. Lithium is also much less abundant than sodium and potassium as it is poorly synthesised in both Big Bang nucleosynthesis and in stars: the Big Bang could only produce trace quantities of lithium, beryllium and boron due to the absence of a stable nucleus with 5 or 8 nucleons, and stellar nucleosynthesis could only pass this bottleneck by the triple-alpha process, fusing three helium nuclei to form carbon, and skipping over those three elements.


On Earth

The Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the Solar System. In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements. The mass of the Earth is approximately 5.98×1024 kg. It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements. Due to planetary differentiation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.


The alkali metals, due to their high reactivity, do not occur naturally in pure form in nature. They are lithophiles and therefore remain close to the Earth's surface because they combine readily with oxygen and so associate strongly with silica, forming relatively low-density minerals that do not sink down into the Earth's core. Potassium, rubidium and caesium are also incompatible elements due to their large ionic radii.


Sodium and potassium are very abundant on Earth, both being among the ten most common elements in Earth's crust; sodium makes up approximately 2.6% of the Earth's crust measured by weight, making it the sixth most abundant element overall and the most abundant alkali metal. Potassium makes up approximately 1.5% of the Earth's crust and is the seventh most abundant element. Sodium is found in many different minerals, of which the most common is ordinary salt (sodium chloride), which occurs in vast quantities dissolved in seawater. Other solid deposits include halite, amphibole, cryolite, nitratine, and zeolite. Many of these solid deposits occur as a result of ancient seas evaporating, which still occurs now in places such as Utah's Great Salt Lake and the Dead Sea.: 69  Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium.: 69 


Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size.: 69  Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that lithium concentration in seawater is approximately 0.14 to 0.25 parts per million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide.: 69 


Rubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals.: 70  Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten, but is much less abundant than rubidium.


Francium-223, the only naturally occurring isotope of francium, is the product of the alpha decay of actinium-227 and can be found in trace amounts in uranium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1018 uranium atoms. It has been calculated that there are at most 30 grams of francium in the earth's crust at any time, due to its extremely short half-life of 22 minutes.


Properties

Physical and chemical

The physical and chemical properties of the alkali metals can be readily explained by their having an ns1 valence electron configuration, which results in weak metallic bonding. Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation.: 74  They all crystallise in the body-centered cubic crystal structure,: 73  and have distinctive flame colours because their outer s electron is very easily excited.: 75  Indeed, these flame test colours are the most common way of identifying them since all their salts with common ions are soluble.: 75  The ns1 configuration also results in the alkali metals having very large atomic and ionic radii, as well as very high thermal and electrical conductivity.: 75  Their chemistry is dominated by the loss of their lone valence electron in the outermost s-orbital to form the +1 oxidation state, due to the ease of ionising this electron and the very high second ionisation energy.: 76  Most of the chemistry has been observed only for the first five members of the group. The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited. What little is known about francium shows that it is very close in behaviour to caesium, as expected. The physical properties of francium are even sketchier because the bulk element has never been observed; hence any data that may be found in the literature are certainly speculative extrapolations.


The alkali metals are more similar to each other than the elements in any other group are to each other. Indeed, the similarity is so great that it is quite difficult to separate potassium, rubidium, and caesium, due to their similar ionic radii; lithium and sodium are more distinct. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation.: 75  In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium. One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others.: 75  This is because the Li+ ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionising it in the gas phase.: 75 


The stable alkali metals are all silver-coloured metals except for caesium, which has a pale golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold).: 74  Additionally, the heavy alkaline earth metals calcium, strontium, and barium, as well as the divalent lanthanides europium and ytterbium, are pale yellow, though the colour is much less prominent than it is for caesium.: 74  Their lustre tarnishes rapidly in air due to oxidation.


All the alkali metals are highly reactive and are never found in elemental forms in nature. Because of this, they are usually stored in mineral oil or kerosene (paraffin oil). They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (LiF). The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care. The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium if the same number of moles of each metal is used. The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron. Not only do the alkali metals react with water, but also with proton donors like alcohols and phenols, gaseous ammonia, and alkynes, the last demonstrating the phenomenal degree of their reactivity. Their great power as reducing agents makes them very useful in liberating other metals from their oxides or halides.: 76 


The second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations.: 28  The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations. The alkalide anions have filled s-subshells, which gives them enough stability to exist. All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials. Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions. A particularly striking example of an alkalide is "inverse sodium hydride", H+Na− (both ions being complexed), as opposed to the usual sodium hydride, Na+H−: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.


In aqueous solution, the alkali metal ions form aqua ions of the formula +, where n is the solvation number. Their coordination numbers and shapes agree well with those expected from their ionic radii. In aqueous solution the water molecules directly attached to the metal ion are said to belong to the first coordination sphere, also known as the first, or primary, solvation shell. The bond between a water molecule and the metal ion is a dative covalent bond, with the oxygen atom donating both electrons to the bond. Each coordinated water molecule may be attached by hydrogen bonds to other water molecules. The latter are said to reside in the second coordination sphere. However, for the alkali metal cations, the second coordination sphere is not well-defined as the +1 charge on the cation is not high enough to polarise the water molecules in the primary solvation shell enough for them to form strong hydrogen bonds with those in the second coordination sphere, producing a more stable entity.: 25  The solvation number for Li+ has been experimentally determined to be 4, forming the tetrahedral +: while solvation numbers of 3 to 6 have been found for lithium aqua ions, solvation numbers less than 4 may be the result of the formation of contact ion pairs, and the higher solvation numbers may be interpreted in terms of water molecules that approach + through a face of the tetrahedron, though molecular dynamic simulations may indicate the existence of an octahedral hexaaqua ion. There are also probably six water molecules in the primary solvation sphere of the sodium ion, forming the octahedral + ion.: 126–127  While it was previously thought that the heavier alkali metals also formed octahedral hexaaqua ions, it has since been found that potassium and rubidium probably form the + and + ions, which have the square antiprismatic structure, and that caesium forms the 12-coordinate + ion.


Lithium

The chemistry of lithium shows several differences from that of the rest of the group as the small Li+ cation polarises anions and gives its compounds a more covalent character. Lithium and magnesium have a diagonal relationship due to their similar atomic radii, so that they show some similarities. For example, lithium forms a stable nitride, a property common among all the alkaline earth metals (magnesium's group) but unique among the alkali metals. In addition, among their respective groups, only lithium and magnesium form organometallic compounds with significant covalent character (e.g. LiMe and MgMe2).


Lithium fluoride is the only alkali metal halide that is poorly soluble in water, and lithium hydroxide is the only alkali metal hydroxide that is not deliquescent. Conversely, lithium perchlorate and other lithium salts with large anions that cannot be polarised are much more stable than the analogous compounds of the other alkali metals, probably because Li+ has a high solvation energy.: 76  This effect also means that most simple lithium salts are commonly encountered in hydrated form, because the anhydrous forms are extremely hygroscopic: this allows salts like lithium chloride and lithium bromide to be used in dehumidifiers and air-conditioners.: 76 


Francium

Francium is also predicted to show some differences due to its high atomic weight, causing its electrons to travel at considerable fractions of the speed of light and thus making relativistic effects more prominent. In contrast to the trend of decreasing electronegativities and ionisation energies of the alkali metals, francium's electronegativity and ionisation energy are predicted to be higher than caesium's due to the relativistic stabilisation of the 7s electrons; also, its atomic radius is expected to be abnormally low. Thus, contrary to expectation, caesium is the most reactive of the alkali metals, not francium.: 1729  All known physical properties of francium also deviate from the clear trends going from lithium to caesium, such as the first ionisation energy, electron affinity, and anion polarisability, though due to the paucity of known data about francium many sources give extrapolated values, ignoring that relativistic effects make the trend from lithium to caesium become inapplicable at francium. Some of the few properties of francium that have been predicted taking relativity into account are the electron affinity (47.2 kJ/mol) and the enthalpy of dissociation of the Fr2 molecule (42.1 kJ/mol). The CsFr molecule is polarised as Cs+Fr−, showing that the 7s subshell of francium is much more strongly affected by relativistic effects than the 6s subshell of caesium. Additionally, francium superoxide (FrO2) is expected to have significant covalent character, unlike the other alkali metal superoxides, because of bonding contributions from the 6p electrons of francium.


Nuclear

All the alkali metals have odd atomic numbers; hence, their isotopes must be either odd–odd (both proton and neutron number are odd) or odd–even (proton number is odd, but neutron number is even). Odd–odd nuclei have even mass numbers, whereas odd–even nuclei have odd mass numbers. Odd–odd primordial nuclides are rare because most odd–odd nuclei are highly unstable with respect to beta decay, because the decay products are even–even, and are therefore more strongly bound, due to nuclear pairing effects.


Due to the great rarity of odd–odd nuclei, almost all the primordial isotopes of the alkali metals are odd–even (the exceptions being the light stable isotope lithium-6 and the long-lived radioisotope potassium-40). For a given odd mass number, there can be only a single beta-stable nuclide, since there is not a difference in binding energy between even–odd and odd–even comparable to that between even–even and odd–odd, leaving other nuclides of the same mass number (isobars) free to beta decay toward the lowest-mass nuclide. An effect of the instability of an odd number of either type of nucleons is that odd-numbered elements, such as the alkali metals, tend to have fewer stable isotopes than even-numbered elements. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number and all but one also have an even number of neutrons. Beryllium is the single exception to both rules, due to its low atomic number.


All of the alkali metals except lithium and caesium have at least one naturally occurring radioisotope: sodium-22 and sodium-24 are trace radioisotopes produced cosmogenically, potassium-40 and rubidium-87 have very long half-lives and thus occur naturally, and all isotopes of francium are radioactive. Caesium was also thought to be radioactive in the early 20th century, although it has no naturally occurring radioisotopes. (Francium had not been discovered yet at that time.) The natural long-lived radioisotope of potassium, potassium-40, makes up about 0.012% of natural potassium, and thus natural potassium is weakly radioactive. This natural radioactivity became a basis for a mistaken claim of the discovery for element 87 (the next alkali metal after caesium) in 1925. Natural rubidium is similarly slightly radioactive, with 27.83% being the long-lived radioisotope rubidium-87.: 74 


Caesium-137, with a half-life of 30.17 years, is one of the two principal medium-lived fission products, along with strontium-90, which are responsible for most of the radioactivity of spent nuclear fuel after several years of cooling, up to several hundred years after use. It constitutes most of the radioactivity still left from the Chernobyl accident. Caesium-137 undergoes high-energy beta decay and eventually becomes stable barium-137. It is a strong emitter of gamma radiation. Caesium-137 has a very low rate of neutron capture and cannot be feasibly disposed of in this way, but must be allowed to decay. Caesium-137 has been used as a tracer in hydrologic studies, analogous to the use of tritium. Small amounts of caesium-134 and caesium-137 were released into the environment during nearly all nuclear weapon tests and some nuclear accidents, most notably the Goiânia accident and the Chernobyl disaster. As of 2005, caesium-137 is the principal source of radiation in the zone of alienation around the Chernobyl nuclear power plant. Its chemical properties as one of the alkali metals make it one of the most problematic of the short-to-medium-lifetime fission products because it easily moves and spreads in nature due to the high water solubility of its salts, and is taken up by the body, which mistakes it for its essential congeners sodium and potassium.: 114 


Periodic trends

The alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation.: 75  In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.


Atomic and ionic radii

The atomic radii of the alkali metals increase going down the group. Because of the shielding effect, when an atom has more than one electron shell, each electron feels electric repulsion from the other electrons as well as electric attraction from the nucleus. In the alkali metals, the outermost electron only feels a net charge of +1, as some of the nuclear charge (which is equal to the atomic number) is cancelled by the inner electrons; the number of inner electrons of an alkali metal is always one less than the nuclear charge. Therefore, the only factor which affects the atomic radius of the alkali metals is the number of electron shells. Since this number increases down the group, the atomic radius must also increase down the group.


The ionic radii of the alkali metals are much smaller than their atomic radii. This is because the outermost electron of the alkali metals is in a different electron shell than the inner electrons, and thus when it is removed the resulting atom has one fewer electron shell and is smaller. Additionally, the effective nuclear charge has increased, and thus the electrons are attracted more strongly towards the nucleus and the ionic radius decreases.


First ionisation energy

The first ionisation energy of an element or molecule is the energy required to move the most loosely held electron from one mole of gaseous atoms of the element or molecules to form one mole of gaseous ions with electric charge +1. The factors affecting the first ionisation energy are the nuclear charge, the amount of shielding by the inner electrons and the distance from the most loosely held electron from the nucleus, which is always an outer electron in main group elements. The first two factors change the effective nuclear charge the most loosely held electron feels. Since the outermost electron of alkali metals always feels the same effective nuclear charge (+1), the only factor which affects the first ionisation energy is the distance from the outermost electron to the nucleus. Since this distance increases down the group, the outermost electron feels less attraction from the nucleus and thus the first ionisation energy decreases. This trend is broken in francium due to the relativistic stabilisation and contraction of the 7s orbital, bringing francium's valence electron closer to the nucleus than would be expected from non-relativistic calculations. This makes francium's outermost electron feel more attraction from the nucleus, increasing its first ionisation energy slightly beyond that of caesium.: 1729 


The second ionisation energy of the alkali metals is much higher than the first as the second-most loosely held electron is part of a fully filled electron shell and is thus difficult to remove.


Reactivity

The reactivities of the alkali metals increase going down the group. This is the result of a combination of two factors: the first ionisation energies and atomisation energies of the alkali metals. Because the first ionisation energy of the alkali metals decreases down the group, it is easier for the outermost electron to be removed from the atom and participate in chemical reactions, thus increasing reactivity down the group. The atomisation energy measures the strength of the metallic bond of an element, which falls down the group as the atoms increase in radius and thus the metallic bond must increase in length, making the delocalised electrons further away from the attraction of the nuclei of the heavier alkali metals. Adding the atomisation and first ionisation energies gives a quantity closely related to (but not equal to) the activation energy of the reaction of an alkali metal with another substance. This quantity decreases going down the group, and so does the activation energy; thus, chemical reactions can occur faster and the reactivity increases down the group.


Electronegativity

Electronegativity is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself. If the bond between sodium and chlorine in sodium chloride were covalent, the pair of shared electrons would be attracted to the chlorine because the effective nuclear charge on the outer electrons is +7 in chlorine but is only +1 in sodium. The electron pair is attracted so close to the chlorine atom that they are practically transferred to the chlorine atom (an ionic bond). However, if the sodium atom was replaced by a lithium atom, the electrons will not be attracted as close to the chlorine atom as before because the lithium atom is smaller, making the electron pair more strongly attracted to the closer effective nuclear charge from lithium. Hence, the larger alkali metal atoms (further down the group) will be less electronegative as the bonding pair is less strongly attracted towards them. As mentioned previously, francium is expected to be an exception.


Because of the higher electronegativity of lithium, some of its compounds have a more covalent character. For example, lithium iodide (LiI) will dissolve in organic solvents, a property of most covalent compounds. Lithium fluoride (LiF) is the only alkali halide that is not soluble in water, and lithium hydroxide (LiOH) is the only alkali metal hydroxide that is not deliquescent.


Melting and boiling points

The melting point of a substance is the point where it changes state from solid to liquid while the boiling point of a substance (in liquid state) is the point where the vapour pressure of the liquid equals the environmental pressure surrounding the liquid and all the liquid changes state to gas. As a metal is heated to its melting point, the metallic bonds keeping the atoms in place weaken so that the atoms can move around, and the metallic bonds eventually break completely at the metal's boiling point. Therefore, the falling melting and boiling points of the alkali metals indicate that the strength of the metallic bonds of the alkali metals decreases down the group. This is because metal atoms are held together by the electromagnetic attraction from the positive ions to the delocalised electrons. As the atoms increase in size going down the group (because their atomic radius increases), the nuclei of the ions move further away from the delocalised electrons and hence the metallic bond becomes weaker so that the metal can more easily melt and boil, thus lowering the melting and boiling points. The increased nuclear charge is not a relevant factor due to the shielding effect.


Density

The alkali metals all have the same crystal structure (body-centred cubic) and thus the only relevant factors are the number of atoms that can fit into a certain volume and the mass of one of the atoms, since density is defined as mass per unit volume. The first factor depends on the volume of the atom and thus the atomic radius, which increases going down the group; thus, the volume of an alkali metal atom increases going down the group. The mass of an alkali metal atom also increases going down the group. Thus, the trend for the densities of the alkali metals depends on their atomic weights and atomic radii; if figures for these two factors are known, the ratios between the densities of the alkali metals can then be calculated. The resultant trend is that the densities of the alkali metals increase down the table, with an exception at potassium. Due to having the lowest atomic weight and the largest atomic radius of all the elements in their periods, the alkali metals are the least dense metals in the periodic table. Lithium, sodium, and potassium are the only three metals in the periodic table that are less dense than water: in fact, lithium is the least dense known solid at room temperature.: 75 


Compounds

The alkali metals form complete series of compounds with all usually encountered anions, which well illustrate group trends. These compounds can be described as involving the alkali metals losing electrons to acceptor species and forming monopositive ions.: 79  This description is most accurate for alkali halides and becomes less and less accurate as cationic and anionic charge increase, and as the anion becomes larger and more polarisable. For instance, ionic bonding gives way to metallic bonding along the series NaCl, Na2O, Na2S, Na3P, Na3As, Na3Sb, Na3Bi, Na.: 81 


Hydroxides

All the alkali metals react vigorously or explosively with cold water, producing an aqueous solution of a strongly basic alkali metal hydroxide and releasing hydrogen gas. This reaction becomes more vigorous going down the group: lithium reacts steadily with effervescence, but sodium and potassium can ignite, and rubidium and caesium sink in water and generate hydrogen gas so rapidly that shock waves form in the water that may shatter glass containers. When an alkali metal is dropped into water, it produces an explosion, of which there are two separate stages. The metal reacts with the water first, breaking the hydrogen bonds in the water and producing hydrogen gas; this takes place faster for the more reactive heavier alkali metals. Second, the heat generated by the first part of the reaction often ignites the hydrogen gas, causing it to burn explosively into the surrounding air. This secondary hydrogen gas explosion produces the visible flame above the bowl of water, lake or other body of water, not the initial reaction of the metal with water (which tends to happen mostly under water). The alkali metal hydroxides are the most basic known hydroxides.: 87 


Recent research has suggested that the explosive behavior of alkali metals in water is driven by a Coulomb explosion rather than solely by rapid generation of hydrogen itself. All alkali metals melt as a part of the reaction with water. Water molecules ionise the bare metallic surface of the liquid metal, leaving a positively charged metal surface and negatively charged water ions. The attraction between the charged metal and water ions will rapidly increase the surface area, causing an exponential increase of ionisation. When the repulsive forces within the liquid metal surface exceeds the forces of the surface tension, it vigorously explodes.


The hydroxides themselves are the most basic hydroxides known, reacting with acids to give salts and with alcohols to give oligomeric alkoxides. They easily react with carbon dioxide to form carbonates or bicarbonates, or with hydrogen sulfide to form sulfides or bisulfides, and may be used to separate thiols from petroleum. They react with amphoteric oxides: for example, the oxides of aluminium, zinc, tin, and lead react with the alkali metal hydroxides to give aluminates, zincates, stannates, and plumbates. Silicon dioxide is acidic, and thus the alkali metal hydroxides can also attack silicate glass.: 87 


Intermetallic compounds

The alkali metals form many intermetallic compounds with each other and the elements from groups 2 to 13 in the periodic table of varying stoichiometries,: 81  such as the sodium amalgams with mercury, including Na5Hg8 and Na3Hg. Some of these have ionic characteristics: taking the alloys with gold, the most electronegative of metals, as an example, NaAu and KAu are metallic, but RbAu and CsAu are semiconductors.: 81  NaK is an alloy of sodium and potassium that is very useful because it is liquid at room temperature, although precautions must be taken due to its extreme reactivity towards water and air. The eutectic mixture melts at −12.6 °C. An alloy of 41% caesium, 47% sodium, and 12% potassium has the lowest known melting point of any metal or alloy, −78 °C.


Compounds with the group 13 elements

The intermetallic compounds of the alkali metals with the heavier group 13 elements (aluminium, gallium, indium, and thallium), such as NaTl, are poor conductors or semiconductors, unlike the normal alloys with the preceding elements, implying that the alkali metal involved has lost an electron to the Zintl anions involved. Nevertheless, while the elements in group 14 and beyond tend to form discrete anionic clusters, group 13 elements tend to form polymeric ions with the alkali metal cations located between the giant ionic lattice. For example, NaTl consists of a polymeric anion (—Tl−—)n with a covalent diamond cubic structure with Na+ ions located between the anionic lattice. The larger alkali metals cannot fit similarly into an anionic lattice and tend to force the heavier group 13 elements to form anionic clusters.


Boron is a special case, being the only nonmetal in group 13. The alkali metal borides tend to be boron-rich, involving appreciable boron–boron bonding involving deltahedral structures,: 147–8  and are thermally unstable due to the alkali metals having a very high vapour pressure at elevated temperatures. This makes direct synthesis problematic because the alkali metals do not react with boron below 700 °C, and thus this must be accomplished in sealed containers with the alkali metal in excess. Furthermore, exceptionally in this group, reactivity with boron decreases down the group: lithium reacts completely at 700 °C, but sodium at 900 °C and potassium not until 1200 °C, and the reaction is instantaneous for lithium but takes hours for potassium. Rubidium and caesium borides have not even been characterised. Various phases are known, such as LiB10, NaB6, NaB15, and KB6. Under high pressure the boron–boron bonding in the lithium borides changes from following Wade's rules to forming Zintl anions like the rest of group 13.


Compounds with the group 14 elements

Lithium and sodium react with carbon to form acetylides, Li2C2 and Na2C2, which can also be obtained by reaction of the metal with acetylene. Potassium, rubidium, and caesium react with graphite; their atoms are intercalated between the hexagonal graphite layers, forming graphite intercalation compounds of formulae MC60 (dark grey, almost black), MC48 (dark grey, almost black), MC36 (blue), MC24 (steel blue), and MC8 (bronze) (M = K, Rb, or Cs). These compounds are over 200 times more electrically conductive than pure graphite, suggesting that the valence electron of the alkali metal is transferred to the graphite layers (e.g. M+C−8). Upon heating of KC8, the elimination of potassium atoms results in the conversion in sequence to KC24, KC36, KC48 and finally KC60. KC8 is a very strong reducing agent and is pyrophoric and explodes on contact with water. While the larger alkali metals (K, Rb, and Cs) initially form MC8, the smaller ones initially form MC6, and indeed they require reaction of the metals with graphite at high temperatures around 500 °C to form. Apart from this, the alkali metals are such strong reducing agents that they can even reduce buckminsterfullerene to produce solid fullerides MnC60; sodium, potassium, rubidium, and caesium can form fullerides where n = 2, 3, 4, or 6, and rubidium and caesium additionally can achieve n = 1.: 285 


When the alkali metals react with the heavier elements in the carbon group (silicon, germanium, tin, and lead), ionic substances with cage-like structures are formed, such as the silicides M4Si4 (M = K, Rb, or Cs), which contains M+ and tetrahedral Si4−4 ions. The chemistry of alkali metal germanides, involving the germanide ion Ge4− and other cluster (Zintl) ions such as Ge2−4, Ge4−9, Ge2−9, and 6−, is largely analogous to that of the corresponding silicides.: 393  Alkali metal stannides are mostly ionic, sometimes with the stannide ion (Sn4−), and sometimes with more complex Zintl ions such as Sn4−9, which appears in tetrapotassium nonastannide (K4Sn9). The monatomic plumbide ion (Pb4−) is unknown, and indeed its formation is predicted to be energetically unfavourable; alkali metal plumbides have complex Zintl ions, such as Pb4−9. These alkali metal germanides, stannides, and plumbides may be produced by reducing germanium, tin, and lead with sodium metal in liquid ammonia.: 394 


Nitrides and pnictides

Lithium, the lightest of the alkali metals, is the only alkali metal which reacts with nitrogen at standard conditions, and its nitride is the only stable alkali metal nitride. Nitrogen is an unreactive gas because breaking the strong triple bond in the dinitrogen molecule (N2) requires a lot of energy. The formation of an alkali metal nitride would consume the ionisation energy of the alkali metal (forming M+ ions), the energy required to break the triple bond in N2 and the formation of N3− ions, and all the energy released from the formation of an alkali metal nitride is from the lattice energy of the alkali metal nitride. The lattice energy is maximised with small, highly charged ions; the alkali metals do not form highly charged ions, only forming ions with a charge of +1, so only lithium, the smallest alkali metal, can release enough lattice energy to make the reaction with nitrogen exothermic, forming lithium nitride. The reactions of the other alkali metals with nitrogen would not release enough lattice energy and would thus be endothermic, so they do not form nitrides at standard conditions. Sodium nitride (Na3N) and potassium nitride (K3N), while existing, are extremely unstable, being prone to decomposing back into their constituent elements, and cannot be produced by reacting the elements with each other at standard conditions. Steric hindrance forbids the existence of rubidium or caesium nitride.: 417  However, sodium and potassium form colourless azide salts involving the linear N−3 anion; due to the large size of the alkali metal cations, they are thermally stable enough to be able to melt before decomposing.: 417 


All the alkali metals react readily with phosphorus and arsenic to form phosphides and arsenides with the formula M3Pn (where M represents an alkali metal and Pn represents a pnictogen – phosphorus, arsenic, antimony, or bismuth). This is due to the greater size of the P3− and As3− ions, so that less lattice energy needs to be released for the salts to form. These are not the only phosphides and arsenides of the alkali metals: for example, potassium has nine different known phosphides, with formulae K3P, K4P3, K5P4, KP, K4P6, K3P7, K3P11, KP10.3, and KP15. While most metals form arsenides, only the alkali and alkaline earth metals form mostly ionic arsenides. The structure of Na3As is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal, and this indicates that even with these electropositive metals the bonding cannot be straightforwardly ionic. Other alkali metal arsenides not conforming to the formula M3As are known, such as LiAs, which has a metallic lustre and electrical conductivity indicating the presence of some metallic bonding. The antimonides are unstable and reactive as the Sb3− ion is a strong reducing agent; reaction of them with acids form the toxic and unstable gas stibine (SbH3). Indeed, they have some metallic properties, and the alkali metal antimonides of stoichiometry MSb involve antimony atoms bonded in a spiral Zintl structure. Bismuthides are not even wholly ionic; they are intermetallic compounds containing partially metallic and partially ionic bonds.


Oxides and chalcogenides

All the alkali metals react vigorously with oxygen at standard conditions. They form various types of oxides, such as simple oxides (containing the O2− ion), peroxides (containing the O2−2 ion, where there is a single bond between the two oxygen atoms), superoxides (containing the O−2 ion), and many others. Lithium burns in air to form lithium oxide, but sodium reacts with oxygen to form a mixture of sodium oxide and sodium peroxide. Potassium forms a mixture of potassium peroxide and potassium superoxide, while rubidium and caesium form the superoxide exclusively. Their reactivity increases going down the group: while lithium, sodium and potassium merely burn in air, rubidium and caesium are pyrophoric (spontaneously catch fire in air).


The smaller alkali metals tend to polarise the larger anions (the peroxide and superoxide) due to their small size. This attracts the electrons in the more complex anions towards one of its constituent oxygen atoms, forming an oxide ion and an oxygen atom. This causes lithium to form the oxide exclusively on reaction with oxygen at room temperature. This effect becomes drastically weaker for the larger sodium and potassium, allowing them to form the less stable peroxides. Rubidium and caesium, at the bottom of the group, are so large that even the least stable superoxides can form. Because the superoxide releases the most energy when formed, the superoxide is preferentially formed for the larger alkali metals where the more complex anions are not polarised. The oxides and peroxides for these alkali metals do exist, but do not form upon direct reaction of the metal with oxygen at standard conditions. In addition, the small size of the Li+ and O2− ions contributes to their forming a stable ionic lattice structure. Under controlled conditions, however, all the alkali metals, with the exception of francium, are known to form their oxides, peroxides, and superoxides. The alkali metal peroxides and superoxides are powerful oxidising agents. Sodium peroxide and potassium superoxide react with carbon dioxide to form the alkali metal carbonate and oxygen gas, which allows them to be used in submarine air purifiers; the presence of water vapour, naturally present in breath, makes the removal of carbon dioxide by potassium superoxide even more efficient. All the stable alkali metals except lithium can form red ozonides (MO3) through low-temperature reaction of the powdered anhydrous hydroxide with ozone: the ozonides may be then extracted using liquid ammonia. They slowly decompose at standard conditions to the superoxides and oxygen, and hydrolyse immediately to the hydroxides when in contact with water.: 85  Potassium, rubidium, and caesium also form sesquioxides M2O3, which may be better considered peroxide disuperoxides, .: 85 


Rubidium and caesium can form a great variety of suboxides with the metals in formal oxidation states below +1.: 85  Rubidium can form Rb6O and Rb9O2 (copper-coloured) upon oxidation in air, while caesium forms an immense variety of oxides, such as the ozonide CsO3 and several brightly coloured suboxides, such as Cs7O (bronze), Cs4O (red-violet), Cs11O3 (violet), Cs3O (dark green), CsO, Cs3O2, as well as Cs7O2. The last of these may be heated under vacuum to generate Cs2O.


The alkali metals can also react analogously with the heavier chalcogens (sulfur, selenium, tellurium, and polonium), and all the alkali metal chalcogenides are known (with the exception of francium's). Reaction with an excess of the chalcogen can similarly result in lower chalcogenides, with chalcogen ions containing chains of the chalcogen atoms in question. For example, sodium can react with sulfur to form the sulfide (Na2S) and various polysulfides with the formula Na2Sx (x from 2 to 6), containing the S2−x ions. Due to the basicity of the Se2− and Te2− ions, the alkali metal selenides and tellurides are alkaline in solution; when reacted directly with selenium and tellurium, alkali metal polyselenides and polytellurides are formed along with the selenides and tellurides with the Se2−x and Te2−x ions. They may be obtained directly from the elements in liquid ammonia or when air is not present, and are colourless, water-soluble compounds that air oxidises quickly back to selenium or tellurium.: 766  The alkali metal polonides are all ionic compounds containing the Po2− ion; they are very chemically stable and can be produced by direct reaction of the elements at around 300–400 °C.: 766 


Halides, hydrides, and pseudohalides

The alkali metals are among the most electropositive elements on the periodic table and thus tend to bond ionically to the most electronegative elements on the periodic table, the halogens (fluorine, chlorine, bromine, iodine, and astatine), forming salts known as the alkali metal halides. The reaction is very vigorous and can sometimes result in explosions.: 76  All twenty stable alkali metal halides are known; the unstable ones are not known, with the exception of sodium astatide, because of the great instability and rarity of astatine and francium. The most well-known of the twenty is certainly sodium chloride, otherwise known as common salt. All of the stable alkali metal halides have the formula MX where M is an alkali metal and X is a halogen. They are all white ionic crystalline solids that have high melting points. All the alkali metal halides are soluble in water except for lithium fluoride (LiF), which is insoluble in water due to its very high lattice enthalpy. The high lattice enthalpy of lithium fluoride is due to the small sizes of the Li+ and F− ions, causing the electrostatic interactions between them to be strong: a similar effect occurs for magnesium fluoride, consistent with the diagonal relationship between lithium and magnesium.: 76 


The alkali metals also react similarly with hydrogen to form ionic alkali metal hydrides, where the hydride anion acts as a pseudohalide: these are often used as reducing agents, producing hydrides, complex metal hydrides, or hydrogen gas.: 83  Other pseudohalides are also known, notably the cyanides. These are isostructural to the respective halides except for lithium cyanide, indicating that the cyanide ions may rotate freely.: 322  Ternary alkali metal halide oxides, such as Na3ClO, K3BrO (yellow), Na4Br2O, Na4I2O, and K4Br2O, are also known.: 83  The polyhalides are rather unstable, although those of rubidium and caesium are greatly stabilised by the feeble polarising power of these extremely large cations.: 835 


Coordination complexes

Alkali metal cations do not usually form coordination complexes with simple Lewis bases due to their low charge of just +1 and their relatively large size; thus the Li+ ion forms most complexes and the heavier alkali metal ions form less and less (though exceptions occur for weak complexes).: 90  Lithium in particular has a very rich coordination chemistry in which it exhibits coordination numbers from 1 to 12, although octahedral hexacoordination is its preferred mode.: 90–1  In aqueous solution, the alkali metal ions exist as octahedral hexahydrate complexes +, with the exception of the lithium ion, which due to its small size forms tetrahedral tetrahydrate complexes +; the alkali metals form these complexes because their ions are attracted by electrostatic forces of attraction to the polar water molecules. Because of this, anhydrous salts containing alkali metal cations are often used as desiccants. Alkali metals also readily form complexes with crown ethers (e.g. 12-crown-4 for Li+, 15-crown-5 for Na+, 18-crown-6 for K+, and 21-crown-7 for Rb+) and cryptands due to electrostatic attraction.


Ammonia solutions

The alkali metals dissolve slowly in liquid ammonia, forming ammoniacal solutions of solvated metal cation M+ and solvated electron e−, which react to form hydrogen gas and the alkali metal amide (MNH2, where M represents an alkali metal): this was first noted by Humphry Davy in 1809 and rediscovered by W. Weyl in 1864. The process may be speeded up by a catalyst. Similar solutions are formed by the heavy divalent alkaline earth metals calcium, strontium, barium, as well as the divalent lanthanides, europium and ytterbium. The amide salt is quite insoluble and readily precipitates out of solution, leaving intensely coloured ammonia solutions of the alkali metals. In 1907, Charles A. Kraus identified the colour as being due to the presence of solvated electrons, which contribute to the high electrical conductivity of these solutions. At low concentrations (below 3 M), the solution is dark blue and has ten times the conductivity of aqueous sodium chloride; at higher concentrations (above 3 M), the solution is copper-coloured and has approximately the conductivity of liquid metals like mercury. In addition to the alkali metal amide salt and solvated electrons, such ammonia solutions also contain the alkali metal cation (M+), the neutral alkali metal atom (M), diatomic alkali metal molecules (M2) and alkali metal anions (M−). These are unstable and eventually become the more thermodynamically stable alkali metal amide and hydrogen gas. Solvated electrons are powerful reducing agents and are often used in chemical synthesis.


Organometallic

Organolithium

Being the smallest alkali metal, lithium forms the widest variety of and most stable organometallic compounds, which are bonded covalently. Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi)x where R is the organic group. As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles. For use as bases, butyllithiums are often used and are commercially available. An example of an organolithium compound is methyllithium ((CH3Li)x), which exists in tetrameric (x = 4, tetrahedral) and hexameric (x = 6, octahedral) forms. Organolithium compounds, especially n-butyllithium, are useful reagents in organic synthesis, as might be expected given lithium's diagonal relationship with magnesium, which plays an important role in the Grignard reaction.: 102  For example, alkyllithiums and aryllithiums may be used to synthesise aldehydes and ketones by reaction with metal carbonyls. The reaction with nickel tetracarbonyl, for example, proceeds through an unstable acyl nickel carbonyl complex which then undergoes electrophilic substitution to give the desired aldehyde (using H+ as the electrophile) or ketone (using an alkyl halide) product.: 105 


Alkyllithiums and aryllithiums may also react with N,N-disubstituted amides to give aldehydes and ketones, and symmetrical ketones by reacting with carbon monoxide. They thermally decompose to eliminate a β-hydrogen, producing alkenes and lithium hydride: another route is the reaction of ethers with alkyl- and aryllithiums that act as strong bases.: 105  In non-polar solvents, aryllithiums react as the carbanions they effectively are, turning carbon dioxide to aromatic carboxylic acids (ArCO2H) and aryl ketones to tertiary carbinols (Ar'2C(Ar)OH). Finally, they may be used to synthesise other organometallic compounds through metal-halogen exchange.: 106 


Heavier alkali metals

Unlike the organolithium compounds, the organometallic compounds of the heavier alkali metals are predominantly ionic. The application of organosodium compounds in chemistry is limited in part due to competition from organolithium compounds, which are commercially available and exhibit more convenient reactivity. The principal organosodium compound of commercial importance is sodium cyclopentadienide. Sodium tetraphenylborate can also be classified as an organosodium compound since in the solid state sodium is bound to the aryl groups. Organometallic compounds of the higher alkali metals are even more reactive than organosodium compounds and of limited utility. A notable reagent is Schlosser's base, a mixture of n-butyllithium and potassium tert-butoxide. This reagent reacts with propene to form the compound allylpotassium (KCH2CHCH2). cis-2-Butene and trans-2-butene equilibrate when in contact with alkali metals. Whereas isomerisation is fast with lithium and sodium, it is slow with the heavier alkali metals. The heavier alkali metals also favour the sterically congested conformation. Several crystal structures of organopotassium compounds have been reported, establishing that they, like the sodium compounds, are polymeric. Organosodium, organopotassium, organorubidium and organocaesium compounds are all mostly ionic and are insoluble (or nearly so) in nonpolar solvents.


Alkyl and aryl derivatives of sodium and potassium tend to react with air. They cause the cleavage of ethers, generating alkoxides. Unlike alkyllithium compounds, alkylsodiums and alkylpotassiums cannot be made by reacting the metals with alkyl halides because Wurtz coupling occurs:: 265 


As such, they have to be made by reacting alkylmercury compounds with sodium or potassium metal in inert hydrocarbon solvents. While methylsodium forms tetramers like methyllithium, methylpotassium is more ionic and has the nickel arsenide structure with discrete methyl anions and potassium cations.: 265 


The alkali metals and their hydrides react with acidic hydrocarbons, for example cyclopentadienes and terminal alkynes, to give salts. Liquid ammonia, ether, or hydrocarbon solvents are used, the most common of which being tetrahydrofuran. The most important of these compounds is sodium cyclopentadienide, NaC5H5, an important precursor to many transition metal cyclopentadienyl derivatives.: 265  Similarly, the alkali metals react with cyclooctatetraene in tetrahydrofuran to give alkali metal cyclooctatetraenides; for example, dipotassium cyclooctatetraenide (K2C8H8) is an important precursor to many metal cyclooctatetraenyl derivatives, such as uranocene.: 266  The large and very weakly polarising alkali metal cations can stabilise large, aromatic, polarisable radical anions, such as the dark-green sodium naphthalenide, Na+−, a strong reducing agent.: 266 


Representative reactions of alkali metals

Reaction with oxygen

Upon reacting with oxygen, alkali metals form oxides, peroxides, superoxides and suboxides. However, the first three are more common. The table below shows the types of compounds formed in reaction with oxygen. The compound in brackets represents the minor product of combustion.


The alkali metal peroxides are ionic compounds that are unstable in water. The peroxide anion is weakly bound to the cation, and it is hydrolysed, forming stronger covalent bonds.


The other oxygen compounds are also unstable in water.


Reaction with sulfur

With sulfur, they form sulfides and polysulfides.


Because alkali metal sulfides are essentially salts of a weak acid and a strong base, they form basic solutions.


Reaction with nitrogen

Lithium is the only metal that combines directly with nitrogen at room temperature.


Li3N can react with water to liberate ammonia.


Reaction with hydrogen

With hydrogen, alkali metals form saline hydrides that hydrolyse in water.


Reaction with carbon

Lithium is the only metal that reacts directly with carbon to give dilithium acetylide. Na and K can react with acetylene to give acetylides.


2Na + 2C2H2 →150 oC 2NaC2H + H2{\displaystyle {\ce {2Na\ +\ 2C2H2\ ->\ 2NaC2H\ +\ H2}}}

2Na + 2NaC2H →220 oC 2Na2C2 + H2{\displaystyle {\ce {2Na\ +\ 2NaC2H\ ->\ 2Na2C2\ +\ H2}}}

Reaction with water

On reaction with water, they generate hydroxide ions and hydrogen gas. This reaction is vigorous and highly exothermic and the hydrogen resulted may ignite in air or even explode in the case of Rb and Cs.


Reaction with other salts

The alkali metals are very good reducing agents. They can reduce metal cations that are less electropositive. Titanium is produced industrially by the reduction of titanium tetrachloride with Na at 400 °C (van Arkel–de Boer process).


Reaction with organohalide compounds

Alkali metals react with halogen derivatives to generate hydrocarbon via the Wurtz reaction.


Alkali metals in liquid ammonia

Alkali metals dissolve in liquid ammonia or other donor solvents like aliphatic amines or hexamethylphosphoramide to give blue solutions. These solutions are believed to contain free electrons.


Due to the presence of solvated electrons, these solutions are very powerful reducing agents used in organic synthesis.


Reaction 1) is known as Birch reduction.
Other reductions that can be carried by these solutions are:


Extensions

Although francium is the heaviest alkali metal that has been discovered, there has been some theoretical work predicting the physical and chemical characteristics of hypothetical heavier alkali metals. Being the first period 8 element, the undiscovered element ununennium (element 119) is predicted to be the next alkali metal after francium and behave much like their lighter congeners; however, it is also predicted to differ from the lighter alkali metals in some properties.: 1729–1730  Its chemistry is predicted to be closer to that of potassium or rubidium: 1729–1730  instead of caesium or francium. This is unusual as periodic trends, ignoring relativistic effects would predict ununennium to be even more reactive than caesium and francium. This lowered reactivity is due to the relativistic stabilisation of ununennium's valence electron, increasing ununennium's first ionisation energy and decreasing the metallic and ionic radii; this effect is already seen for francium.: 1729–1730  This assumes that ununennium will behave chemically as an alkali metal, which, although likely, may not be true due to relativistic effects. The relativistic stabilisation of the 8s orbital also increases ununennium's electron affinity far beyond that of caesium and francium; indeed, ununennium is expected to have an electron affinity higher than all the alkali metals lighter than it. Relativistic effects also cause a very large drop in the polarisability of ununennium.: 1729–1730  On the other hand, ununennium is predicted to continue the trend of melting points decreasing going down the group, being expected to have a melting point between 0 °C and 30 °C.: 1724 


The stabilisation of ununennium's valence electron and thus the contraction of the 8s orbital cause its atomic radius to be lowered to 240 pm,: 1729–1730  very close to that of rubidium (247 pm), so that the chemistry of ununennium in the +1 oxidation state should be more similar to the chemistry of rubidium than to that of francium. On the other hand, the ionic radius of the Uue+ ion is predicted to be larger than that of Rb+, because the 7p orbitals are destabilised and are thus larger than the p-orbitals of the lower shells. Ununennium may also show the +3: 1729–1730  and +5 oxidation states, which are not seen in any other alkali metal,: 28  in addition to the +1 oxidation state that is characteristic of the other alkali metals and is also the main oxidation state of all the known alkali metals: this is because of the destabilisation and expansion of the 7p3/2 spinor, causing its outermost electrons to have a lower ionisation energy than what would otherwise be expected.: 28 : 1729–1730  Indeed, many ununennium compounds are expected to have a large covalent character, due to the involvement of the 7p3/2 electrons in the bonding.


Not as much work has been done predicting the properties of the alkali metals beyond ununennium. Although a simple extrapolation of the periodic table (by the Aufbau principle) would put element 169, unhexennium, under ununennium, Dirac-Fock calculations predict that the next element after ununennium with alkali-metal-like properties may be element 165, unhexpentium, which is predicted to have the electron configuration  5g18 6f14 7d10 8s2 8p1/22 9s1.: 1729–1730  This element would be intermediate in properties between an alkali metal and a group 11 element, and while its physical and atomic properties would be closer to the former, its chemistry may be closer to that of the latter. Further calculations show that unhexpentium would follow the trend of increasing ionisation energy beyond caesium, having an ionisation energy comparable to that of sodium, and that it should also continue the trend of decreasing atomic radii beyond caesium, having an atomic radius comparable to that of potassium.: 1729–1730  However, the 7d electrons of unhexpentium may also be able to participate in chemical reactions along with the 9s electron, possibly allowing oxidation states beyond +1, whence the likely transition metal behaviour of unhexpentium.: 1732–1733  Due to the alkali and alkaline earth metals both being s-block elements, these predictions for the trends and properties of ununennium and unhexpentium also mostly hold quite similarly for the corresponding alkaline earth metals unbinilium (Ubn) and unhexhexium (Uhh).: 1729–1733  Unsepttrium, element 173, may be an even better heavier homologue of ununennium; with a predicted electron configuration of  6g1, it returns to the alkali-metal-like situation of having one easily removed electron far above a closed p-shell in energy, and is expected to be even more reactive than caesium.


The probable properties of further alkali metals beyond unsepttrium have not been explored yet as of 2019, and they may or may not be able to exist. In periods 8 and above of the periodic table, relativistic and shell-structure effects become so strong that extrapolations from lighter congeners become completely inaccurate. In addition, the relativistic and shell-structure effects (which stabilise the s-orbitals and destabilise and expand the d-, f-, and g-orbitals of higher shells) have opposite effects, causing even larger difference between relativistic and non-relativistic calculations of the properties of elements with such high atomic numbers.: 1732–1733  Interest in the chemical properties of ununennium, unhexpentium, and unsepttrium stems from the fact that they are located close to the expected locations of islands of stability, centered at elements 122 (306Ubb) and 164 (482Uhq).


Pseudo-alkali metals

Many other substances are similar to the alkali metals in their tendency to form monopositive cations. Analogously to the pseudohalogens, they have sometimes been called "pseudo-alkali metals". These substances include some elements and many more polyatomic ions; the polyatomic ions are especially similar to the alkali metals in their large size and weak polarising power.


Hydrogen

The element hydrogen, with one electron per neutral atom, is usually placed at the top of Group 1 of the periodic table because of its electron configuration. But hydrogen is not normally considered to be an alkali metal. Metallic hydrogen, which only exists at very high pressures, is known for its electrical and magnetic properties, not its chemical properties. Under typical conditions, pure hydrogen exists as a diatomic gas consisting of two atoms per molecule (H2); however, the alkali metals form diatomic molecules (such as dilithium, Li2) only at high temperatures, when they are in the gaseous state.


Hydrogen, like the alkali metals, has one valence electron and reacts easily with the halogens, but the similarities mostly end there because of the small size of a bare proton H+ compared to the alkali metal cations. Its placement above lithium is primarily due to its electron configuration. It is sometimes placed above fluorine due to their similar chemical properties, though the resemblance is likewise not absolute.


The first ionisation energy of hydrogen (1312.0 kJ/mol) is much higher than that of the alkali metals. As only one additional electron is required to fill in the outermost shell of the hydrogen atom, hydrogen often behaves like a halogen, forming the negative hydride ion, and is very occasionally considered to be a halogen on that basis. (The alkali metals can also form negative ions, known as alkalides, but these are little more than laboratory curiosities, being unstable.) An argument against this placement is that formation of hydride from hydrogen is endothermic, unlike the exothermic formation of halides from halogens. The radius of the H− anion also does not fit the trend of increasing size going down the halogens: indeed, H− is very diffuse because its single proton cannot easily control both electrons.: 15–6  It was expected for some time that liquid hydrogen would show metallic properties; while this has been shown to not be the case, under extremely high pressures, such as those found at the cores of Jupiter and Saturn, hydrogen does become metallic and behaves like an alkali metal; in this phase, it is known as metallic hydrogen. The electrical resistivity of liquid metallic hydrogen at 3000 K is approximately equal to that of liquid rubidium and caesium at 2000 K at the respective pressures when they undergo a nonmetal-to-metal transition.


The 1s1 electron configuration of hydrogen, while analogous to that of the alkali metals (ns1), is unique because there is no 1p subshell. Hence it can lose an electron to form the hydron H+, or gain one to form the hydride ion H−.: 43  In the former case it resembles superficially the alkali metals; in the latter case, the halogens, but the differences due to the lack of a 1p subshell are important enough that neither group fits the properties of hydrogen well.: 43  Group 14 is also a good fit in terms of thermodynamic properties such as ionisation energy and electron affinity, but hydrogen cannot be tetravalent. Thus none of the three placements are entirely satisfactory, although group 1 is the most common placement (if one is chosen) because of the electron configuration and the fact that the hydron is by far the most important of all monatomic hydrogen species, being the foundation of acid-base chemistry. As an example of hydrogen's unorthodox properties stemming from its unusual electron configuration and small size, the hydrogen ion is very small (radius around 150 fm compared to the 50–220 pm size of most other atoms and ions) and so is nonexistent in condensed systems other than in association with other atoms or molecules. Indeed, transferring of protons between chemicals is the basis of acid-base chemistry.: 43  Also unique is hydrogen's ability to form hydrogen bonds, which are an effect of charge-transfer, electrostatic, and electron correlative contributing phenomena. While analogous lithium bonds are also known, they are mostly electrostatic. Nevertheless, hydrogen can take on the same structural role as the alkali metals in some molecular crystals, and has a close relationship with the lightest alkali metals (especially lithium).


Ammonium and derivatives

The ammonium ion (NH+4) has very similar properties to the heavier alkali metals, acting as an alkali metal intermediate between potassium and rubidium, and is often considered a close relative. For example, most alkali metal salts are soluble in water, a property which ammonium salts share. Ammonium is expected to behave stably as a metal (NH+4 ions in a sea of delocalised electrons) at very high pressures (though less than the typical pressure where transitions from insulating to metallic behaviour occur around, 100 GPa), and could possibly occur inside the ice giants Uranus and Neptune, which may have significant impacts on their interior magnetic fields. It has been estimated that the transition from a mixture of ammonia and dihydrogen molecules to metallic ammonium may occur at pressures just below 25 GPa. Under standard conditions, ammonium can form a metallic amalgam with mercury.


Other "pseudo-alkali metals" include the alkylammonium cations, in which some of the hydrogen atoms in the ammonium cation are replaced by alkyl or aryl groups. In particular, the quaternary ammonium cations (NR+4) are very useful since they are permanently charged, and they are often used as an alternative to the expensive Cs+ to stabilise very large and very easily polarisable anions such as HI−2.: 812–9  Tetraalkylammonium hydroxides, like alkali metal hydroxides, are very strong bases that react with atmospheric carbon dioxide to form carbonates.: 256  Furthermore, the nitrogen atom may be replaced by a phosphorus, arsenic, or antimony atom (the heavier nonmetallic pnictogens), creating a phosphonium (PH+4) or arsonium (AsH+4) cation that can itself be substituted similarly; while stibonium (SbH+4) itself is not known, some of its organic derivatives are characterised.


Cobaltocene and derivatives

Cobaltocene, Co(C5H5)2, is a metallocene, the cobalt analogue of ferrocene. It is a dark purple solid. Cobaltocene has 19 valence electrons, one more than usually found in organotransition metal complexes, such as its very stable relative, ferrocene, in accordance with the 18-electron rule. This additional electron occupies an orbital that is antibonding with respect to the Co–C bonds. Consequently, many chemical reactions of Co(C5H5)2 are characterized by its tendency to lose this "extra" electron, yielding a very stable 18-electron cation known as cobaltocenium. Many cobaltocenium salts coprecipitate with caesium salts, and cobaltocenium hydroxide is a strong base that absorbs atmospheric carbon dioxide to form cobaltocenium carbonate.: 256  Like the alkali metals, cobaltocene is a strong reducing agent, and decamethylcobaltocene is stronger still due to the combined inductive effect of the ten methyl groups. Cobalt may be substituted by its heavier congener rhodium to give rhodocene, an even stronger reducing agent. Iridocene (involving iridium) would presumably be still more potent, but is not very well-studied due to its instability.


Thallium

Thallium is the heaviest stable element in group 13 of the periodic table. At the bottom of the periodic table, the inert-pair effect is quite strong, because of the relativistic stabilisation of the 6s orbital and the decreasing bond energy as the atoms increase in size so that the amount of energy released in forming two more bonds is not worth the high ionisation energies of the 6s electrons.: 226–7  It displays the +1 oxidation state: 28  that all the known alkali metals display,: 28  and thallium compounds with thallium in its +1 oxidation state closely resemble the corresponding potassium or silver compounds stoichiometrically due to the similar ionic radii of the Tl+ (164 pm), K+ (152 pm) and Ag+ (129 pm) ions. It was sometimes considered an alkali metal in continental Europe (but not in England) in the years immediately following its discovery,: 126  and was placed just after caesium as the sixth alkali metal in Dmitri Mendeleev's 1869 periodic table and Julius Lothar Meyer's 1868 periodic table. Mendeleev's 1871 periodic table and Meyer's 1870 periodic table put thallium in its current position in the boron group and left the space below caesium blank. However, thallium also displays the oxidation state +3,: 28  which no known alkali metal displays: 28  (although ununennium, the undiscovered seventh alkali metal, is predicted to possibly display the +3 oxidation state).: 1729–1730  The sixth alkali metal is now considered to be francium. While Tl+ is stabilised by the inert-pair effect, this inert pair of 6s electrons is still able to participate chemically, so that these electrons are stereochemically active in aqueous solution. Additionally, the thallium halides (except TlF) are quite insoluble in water, and TlI has an unusual structure because of the presence of the stereochemically active inert pair in thallium.


Copper, silver, and gold

The group 11 metals (or coinage metals), copper, silver, and gold, are typically categorised as transition metals given they can form ions with incomplete d-shells. Physically, they have the relatively low melting points and high electronegativity values associated with post-transition metals. "The filled d subshell and free s electron of Cu, Ag, and Au contribute to their high electrical and thermal conductivity. Transition metals to the left of group 11 experience interactions between s electrons and the partially filled d subshell that lower electron mobility." Chemically, the group 11 metals behave like main-group metals in their +1 valence states, and are hence somewhat related to the alkali metals: this is one reason for their previously being labelled as "group IB", paralleling the alkali metals' "group IA". They are occasionally classified as post-transition metals. Their spectra are analogous to those of the alkali metals. Their monopositive ions are paramagnetic and contribute no colour to their salts, like those of the alkali metals.


In Mendeleev's 1871 periodic table, copper, silver, and gold are listed twice, once under group VIII (with the iron triad and platinum group metals), and once under group IB. Group IB was nonetheless parenthesised to note that it was tentative. Mendeleev's main criterion for group assignment was the maximum oxidation state of an element: on that basis, the group 11 elements could not be classified in group IB, due to the existence of copper(II) and gold(III) compounds being known at that time. However, eliminating group IB would make group I the only main group (group VIII was labelled a transition group) to lack an A–B bifurcation. Soon afterward, a majority of chemists chose to classify these elements in group IB and remove them from group VIII for the resulting symmetry: this was the predominant classification until the rise of the modern medium-long 18-column periodic table, which separated the alkali metals and group 11 metals.


The coinage metals were traditionally regarded as a subdivision of the alkali metal group, due to them sharing the characteristic s1 electron configuration of the alkali metals (group 1: p6s1; group 11: d10s1). However, the similarities are largely confined to the stoichiometries of the +1 compounds of both groups, and not their chemical properties.: 1177  This stems from the filled d subshell providing a much weaker shielding effect on the outermost s electron than the filled p subshell, so that the coinage metals have much higher first ionisation energies and smaller ionic radii than do the corresponding alkali metals.: 1177  Furthermore, they have higher melting points, hardnesses, and densities, and lower reactivities and solubilities in liquid ammonia, as well as having more covalent character in their compounds.: 1177  Finally, the alkali metals are at the top of the electrochemical series, whereas the coinage metals are almost at the very bottom.: 1177  The coinage metals' filled d shell is much more easily disrupted than the alkali metals' filled p shell, so that the second and third ionisation energies are lower, enabling higher oxidation states than +1 and a richer coordination chemistry, thus giving the group 11 metals clear transition metal character.: 1177  Particularly noteworthy is gold forming ionic compounds with rubidium and caesium, in which it forms the auride ion (Au−) which also occurs in solvated form in liquid ammonia solution: here gold behaves as a pseudohalogen because its 5d106s1 configuration has one electron less than the quasi-closed shell 5d106s2 configuration of mercury.: 1177 


Production and isolation

The production of pure alkali metals is somewhat complicated due to their extreme reactivity with commonly used substances, such as water. From their silicate ores, all the stable alkali metals may be obtained the same way: sulfuric acid is first used to dissolve the desired alkali metal ion and aluminium(III) ions from the ore (leaching), whereupon basic precipitation removes aluminium ions from the mixture by precipitating it as the hydroxide. The remaining insoluble alkali metal carbonate is then precipitated selectively; the salt is then dissolved in hydrochloric acid to produce the chloride. The result is then left to evaporate and the alkali metal can then be isolated. Lithium and sodium are typically isolated through electrolysis from their liquid chlorides, with calcium chloride typically added to lower the melting point of the mixture. The heavier alkali metals, however, are more typically isolated in a different way, where a reducing agent (typically sodium for potassium and magnesium or calcium for the heaviest alkali metals) is used to reduce the alkali metal chloride. The liquid or gaseous product (the alkali metal) then undergoes fractional distillation for purification. Most routes to the pure alkali metals require the use of electrolysis due to their high reactivity; one of the few which does not is the pyrolysis of the corresponding alkali metal azide, which yields the metal for sodium, potassium, rubidium, and caesium and the nitride for lithium.: 77 


Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride.


Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 °C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of potassium chloride or potassium hydroxide, found extensively in places such as Canada, Russia, Belarus, Germany, Israel, United States, and Jordan, in a method similar to how sodium was produced in the late 1800s and early 1900s. It can also be produced from seawater. However, these methods are problematic because the potassium metal tends to dissolve in its molten chloride and vaporises significantly at the operating temperatures, potentially forming the explosive superoxide. As a result, pure potassium metal is now produced by reducing molten potassium chloride with sodium metal at 850 °C.: 74 


Although sodium is less reactive than potassium, this process works because at such high temperatures potassium is more volatile than sodium and can easily be distilled off, so that the equilibrium shifts towards the right to produce more potassium gas and proceeds almost to completion.: 74 


Metals like sodium are obtained by electrolysis of molten salts. Rb & Cs obtained mainly as by products of Li processing. To make pure caesium, ores of caesium and rubidium are crushed and heated to 650 °C with sodium metal, generating an alloy that can then be separated via a fractional distillation technique. Because metallic caesium is too reactive to handle, it is normally offered as caesium azide (CsN3). Caesium hydroxide is formed when caesium interacts aggressively with water and ice (CsOH).


Rubidium is the 16th most abundant element in the earth's crust; however, it is quite rare. Some minerals found in North America, South Africa, Russia, and Canada contain rubidium. Some potassium minerals (lepidolites, biotites, feldspar, carnallite) contain it, together with caesium. Pollucite, carnallite, leucite, and lepidolite are all minerals that contain rubidium. As a by-product of lithium extraction, it is commercially obtained from lepidolite. Rubidium is also found in potassium rocks and brines, which is a commercial supply. The majority of rubidium is now obtained as a byproduct of refining lithium. Rubidium is used in vacuum tubes as a getter, a material that combines with and removes trace gases from vacuum tubes.

For several years in the 1950s and 1960s, a by-product of the potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium while the rest was potassium and a small fraction of caesium. Today the largest producers of caesium, for example the Tanco Mine in Manitoba, Canada, produce rubidium as by-product from pollucite. Today, a common method for separating rubidium from potassium and caesium is the fractional crystallisation of a rubidium and caesium alum (Cs, Rb)Al(SO4)2·12H2O, which yields pure rubidium alum after approximately 30 recrystallisations. The limited applications and the lack of a mineral rich in rubidium limit the production of rubidium compounds to 2 to 4 tonnes per year. Caesium, however, is not produced from the above reaction. Instead, the mining of pollucite ore is the main method of obtaining pure caesium, extracted from the ore mainly by three methods: acid digestion, alkaline decomposition, and direct reduction. Both metals are produced as by-products of lithium production: after 1958, when interest in lithium's thermonuclear properties increased sharply, the production of rubidium and caesium also increased correspondingly.: 71  Pure rubidium and caesium metals are produced by reducing their chlorides with calcium metal at 750 °C and low pressure.: 74 


As a result of its extreme rarity in nature, most francium is synthesised in the nuclear reaction 197Au + 18O → 210Fr + 5 n, yielding francium-209, francium-210, and francium-211. The greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, which were synthesised using the nuclear reaction given above. When the only natural isotope francium-223 is specifically required, it is produced as the alpha daughter of actinium-227, itself produced synthetically from the neutron irradiation of natural radium-226, one of the daughters of natural uranium-238.


Applications

Lithium, sodium, and potassium have many useful applications, while rubidium and caesium are very notable in academic contexts but do not have many applications yet.: 68  Lithium is the key ingredient for a range of lithium-based batteries, and lithium oxide can help process silica. Lithium stearate is a thickener and can be used to make lubricating greases; it is produced from lithium hydroxide, which is also used to absorb carbon dioxide in space capsules and submarines.: 70  Lithium chloride is used as a brazing alloy for aluminium parts. In medicine, some lithium salts are used as mood-stabilising pharmaceuticals. Metallic lithium is used in alloys with magnesium and aluminium to give very tough and light alloys.: 70 


Sodium compounds have many applications, the most well-known being sodium chloride as table salt. Sodium salts of fatty acids are used as soap. Pure sodium metal also has many applications, including use in sodium-vapour lamps, which produce very efficient light compared to other types of lighting, and can help smooth the surface of other metals. Being a strong reducing agent, it is often used to reduce many other metals, such as titanium and zirconium, from their chlorides. Furthermore, it is very useful as a heat-exchange liquid in fast breeder nuclear reactors due to its low melting point, viscosity, and cross-section towards neutron absorption.: 74  Sodium-ion batteries may provide cheaper alternatives to their equivalent lithium-based cells. Both sodium and potassium are commonly used as GRAS counterions to create more water-soluble and hence more bioavailable salt forms of acidic pharmaceuticals.


Potassium compounds are often used as fertilisers: 73  as potassium is an important element for plant nutrition. Potassium hydroxide is a very strong base, and is used to control the pH of various substances. Potassium nitrate and potassium permanganate are often used as powerful oxidising agents.: 73  Potassium superoxide is used in breathing masks, as it reacts with carbon dioxide to give potassium carbonate and oxygen gas. Pure potassium metal is not often used, but its alloys with sodium may substitute for pure sodium in fast breeder nuclear reactors.: 74 


Rubidium and caesium are often used in atomic clocks. Caesium atomic clocks are extraordinarily accurate; if a clock had been made at the time of the dinosaurs, it would be off by less than four seconds (after 80 million years). For that reason, caesium atoms are used as the definition of the second. Rubidium ions are often used in purple fireworks, and caesium is often used in drilling fluids in the petroleum industry.


Francium has no commercial applications, but because of francium's relatively simple atomic structure, among other things, it has been used in spectroscopy experiments, leading to more information regarding energy levels and the coupling constants of the weak interaction. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels, similar to those predicted by quantum theory.


Biological role and precautions

Metals

Pure alkali metals are dangerously reactive with air and water and must be kept away from heat, fire, oxidising agents, acids, most organic compounds, halocarbons, plastics, and moisture. They also react with carbon dioxide and carbon tetrachloride, so that normal fire extinguishers are counterproductive when used on alkali metal fires. Some Class D dry powder extinguishers designed for metal fires are effective, depriving the fire of oxygen and cooling the alkali metal.


Experiments are usually conducted using only small quantities of a few grams in a fume hood. Small quantities of lithium may be disposed of by reaction with cool water, but the heavier alkali metals should be dissolved in the less reactive isopropanol. The alkali metals must be stored under mineral oil or an inert atmosphere. The inert atmosphere used may be argon or nitrogen gas, except for lithium, which reacts with nitrogen. Rubidium and caesium must be kept away from air, even under oil, because even a small amount of air diffused into the oil may trigger formation of the dangerously explosive peroxide; for the same reason, potassium should not be stored under oil in an oxygen-containing atmosphere for longer than 6 months.


Ions

The bioinorganic chemistry of the alkali metal ions has been extensively reviewed.
Solid state crystal structures have been determined for many complexes of alkali metal ions in small peptides, nucleic acid constituents, carbohydrates and ionophore complexes.


Lithium naturally only occurs in traces in biological systems and has no known biological role, but does have effects on the body when ingested. Lithium carbonate is used as a mood stabiliser in psychiatry to treat bipolar disorder (manic-depression) in daily doses of about 0.5 to 2 grams, although there are side-effects. Excessive ingestion of lithium causes drowsiness, slurred speech and vomiting, among other symptoms, and poisons the central nervous system, which is dangerous as the required dosage of lithium to treat bipolar disorder is only slightly lower than the toxic dosage. Its biochemistry, the way it is handled by the human body and studies using rats and goats suggest that it is an essential trace element, although the natural biological function of lithium in humans has yet to be identified.


Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The Dietary Reference Intake for sodium is 1.5 grams per day, but most people in the United States consume more than 2.3 grams per day, the minimum amount that promotes hypertension; this in turn causes 7.6 million premature deaths worldwide.


Potassium is the major cation (positive ion) inside animal cells, while sodium is the major cation outside animal cells. The concentration differences of these charged particles causes a difference in electric potential between the inside and outside of cells, known as the membrane potential. The balance between potassium and sodium is maintained by ion transporter proteins in the cell membrane. The cell membrane potential created by potassium and sodium ions allows the cell to generate an action potential—a "spike" of electrical discharge. The ability of cells to produce electrical discharge is critical for body functions such as neurotransmission, muscle contraction, and heart function. Disruption of this balance may thus be fatal: for example, ingestion of large amounts of potassium compounds can lead to hyperkalemia strongly influencing the cardiovascular system. Potassium chloride is used in the United States for lethal injection executions.


Due to their similar atomic radii, rubidium and caesium in the body mimic potassium and are taken up similarly. Rubidium has no known biological role, but may help stimulate metabolism, and, similarly to caesium, replace potassium in the body causing potassium deficiency. Partial substitution is quite possible and rather non-toxic: a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons. Rats can survive up to 50% substitution of potassium by rubidium. Rubidium (and to a much lesser extent caesium) can function as temporary cures for hypokalemia; while rubidium can adequately physiologically substitute potassium in some systems, caesium is never able to do so. There is only very limited evidence in the form of deficiency symptoms for rubidium being possibly essential in goats; even if this is true, the trace amounts usually present in food are more than enough.


Caesium compounds are rarely encountered by most people, but most caesium compounds are mildly toxic. Like rubidium, caesium tends to substitute potassium in the body, but is significantly larger and is therefore a poorer substitute. Excess caesium can lead to hypokalemia, arrhythmia, and acute cardiac arrest, but such amounts would not ordinarily be encountered in natural sources. As such, caesium is not a major chemical environmental pollutant. The median lethal dose (LD50) value for caesium chloride in mice is 2.3 g per kilogram, which is comparable to the LD50 values of potassium chloride and sodium chloride. Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment.


Radioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries. Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers. This led to four deaths and serious injuries from radiation exposure. Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health. Radioisotopes of francium would presumably be dangerous as well due to their high decay energy and short half-life, but none have been produced in large enough amounts to pose any serious risk.


Notes

References

Fluorine is a chemical element; it has symbol F and atomic number 9. It is the lightest halogen and exists at standard conditions as pale yellow diatomic gas. Fluorine is extremely reactive as it reacts with all other elements except for the light noble gases. In its elemental form it is highly toxic.


Among the elements, fluorine ranks 24th in cosmic abundance and 13th in crustal abundance. Fluorite, the primary mineral source of fluorine, which gave the element its name, was first described in 1529; as it was added to metal ores to lower their melting points for smelting, the Latin verb fluo meaning 'to flow' gave the mineral its name. Proposed as an element in 1810, fluorine proved difficult and dangerous to separate from its compounds, and several early experimenters died or sustained injuries from their attempts. Only in 1886 did French chemist Henri Moissan isolate elemental fluorine using low-temperature electrolysis, a process still employed for modern production. Industrial production of fluorine gas for uranium enrichment, its largest application, began during the Manhattan Project in World War II.


Owing to the expense of refining pure fluorine, most commercial applications use fluorine compounds, with about half of mined fluorite used in steelmaking. The rest of the fluorite is converted into hydrogen fluoride en route to various organic fluorides, or into cryolite, which plays a key role in aluminium refining. The carbon–fluorine bond is usually very stable. Organofluorine compounds are widely used as refrigerants, electrical insulation, and PTFE (Teflon). Pharmaceuticals such as atorvastatin and fluoxetine contain C−F bonds. The fluoride ion from dissolved fluoride salts inhibits dental cavities and so finds use in toothpaste and water fluoridation. Global fluorochemical sales amount to more than US$15 billion a year.


Fluorocarbon gases are generally greenhouse gases with global-warming potentials 100 to 23,500 times that of carbon dioxide, and SF6 has the highest global warming potential of any known substance. Organofluorine compounds often persist in the environment due to the strength of the carbon–fluorine bond. Fluorine has no known metabolic role in mammals; a few plants and marine sponges synthesize organofluorine poisons (most often monofluoroacetates) that help deter predation.


Characteristics

Electron configuration

Fluorine atoms have nine electrons, one fewer than neon, and electron configuration 1s22s22p5: two electrons in a filled inner shell and seven in an outer shell requiring one more to be filled. The outer electrons are ineffective at nuclear shielding, and experience a high effective nuclear charge of 9 − 2 = 7; this affects the atom's physical properties.


Fluorine's first ionization energy is third-highest among all elements, behind helium and neon, which complicates the removal of electrons from neutral fluorine atoms. It also has a high electron affinity, second only to chlorine, and tends to capture an electron to become isoelectronic with the noble gas neon; it has the highest electronegativity of any reactive element. Fluorine atoms have a small covalent radius of around 60 picometers, similar to those of its period neighbors oxygen and neon.


Reactivity

The bond energy of difluorine is much lower than that of either Cl2 or Br2 and similar to the easily cleaved peroxide bond; this, along with high electronegativity, accounts for fluorine's easy dissociation, high reactivity, and strong bonds to non-fluorine atoms. Conversely, bonds to other atoms are very strong because of fluorine's high electronegativity. Unreactive substances like powdered steel, glass fragments, and asbestos fibers react quickly with cold fluorine gas; wood and water spontaneously combust under a fluorine jet.


Reactions of elemental fluorine with metals require varying conditions. Alkali metals cause explosions and alkaline earth metals display vigorous activity in bulk; to prevent passivation from the formation of metal fluoride layers, most other metals such as aluminium and iron must be powdered, and noble metals require pure fluorine gas at 300–450 °C (572–842 °F). Some solid nonmetals (sulfur, phosphorus) react vigorously in liquid fluorine. Hydrogen sulfide and sulfur dioxide combine readily with fluorine, the latter sometimes explosively; sulfuric acid exhibits much less activity, requiring elevated temperatures.


Hydrogen, like some of the alkali metals, reacts explosively with fluorine. Carbon, as lamp black, reacts at room temperature to yield tetrafluoromethane. Graphite combines with fluorine above 400 °C (752 °F) to produce non-stoichiometric carbon monofluoride; higher temperatures generate gaseous fluorocarbons, sometimes with explosions. Carbon dioxide and carbon monoxide react at or just above room temperature, whereas paraffins and other organic chemicals generate strong reactions: even completely substituted haloalkanes such as carbon tetrachloride, normally incombustible, may explode. Although nitrogen trifluoride is stable, nitrogen requires an electric discharge at elevated temperatures for reaction with fluorine to occur, due to the very strong triple bond in elemental nitrogen; ammonia may react explosively. Oxygen does not combine with fluorine under ambient conditions, but can be made to react using electric discharge at low temperatures and pressures; the products tend to disintegrate into their constituent elements when heated. Heavier halogens react readily with fluorine as does the noble gas radon; of the other noble gases, only xenon and krypton react, and only under special conditions. Argon does not react with fluorine gas; however, it does form a compound with fluorine, argon fluorohydride.


Phases

At room temperature, fluorine is a gas of diatomic molecules, pale yellow when pure (sometimes described as yellow-green). It has a characteristic halogen-like pungent and biting odor detectable at 20 ppb. Fluorine condenses into a bright yellow liquid at −188 °C (−306.4 °F), a transition temperature similar to those of oxygen and nitrogen.


Fluorine has two solid forms, α- and β-fluorine. The latter crystallizes at −220 °C (−364.0 °F) and is transparent and soft, with the same disordered cubic structure of freshly crystallized solid oxygen, unlike the orthorhombic systems of other solid halogens. Further cooling to −228 °C (−378.4 °F) induces a phase transition into opaque and hard α-fluorine, which has a monoclinic structure with dense, angled layers of molecules. The transition from β- to α-fluorine is more exothermic than the condensation of fluorine, and can be violent.


Isotopes

Natural fluorine consists entirely of the stable isotope 19F. It has a high magnetogyric ratio and exceptional sensitivity to magnetic fields; because it is also the only isotope, it is highly suited for NMR and use in magnetic resonance imaging. Eighteen radioisotopes with mass numbers 13–31 have been synthesized, of which 18F is the most stable with a half-life of 109.734 minutes. 18F is a natural trace radioisotope produced by cosmic ray spallation of atmospheric argon as well as by reaction of protons with natural oxygen: 18O + p → 18F + n. Other radioisotopes have half-lives less than 70 seconds; most decay in less than half a second. The isotopes 17F and 18F undergo β+ decay and electron capture, lighter isotopes decay by proton emission, and those heavier than 19F undergo β− decay (the heaviest ones with delayed neutron emission). Two metastable isomers of fluorine are known, 18mF, with a half-life of 162(7) nanoseconds, and 26mF, with a half-life of 2.2(1) milliseconds.


Occurrence

Universe

Among the lighter elements, fluorine's abundance value of 400 ppb (parts per billion) – 24th among elements in the universe – is exceptionally low: other elements from carbon to magnesium are twenty or more times as common. This is because stellar nucleosynthesis processes bypass fluorine, and any fluorine atoms otherwise created have high nuclear cross sections, allowing collisions with hydrogen or helium to generate oxygen or neon respectively.


Beyond this transient existence, three explanations have been proposed for the presence of fluorine:


Earth

Fluorine is the 13th most abundant element in Earth's crust at 600–700 ppm (parts per million) by mass. Though believed not to occur naturally, elemental fluorine has been shown to be present as an occlusion in antozonite, a variant of fluorite. Most fluorine exists as fluoride-containing minerals. Fluorite, fluorapatite and cryolite are the most industrially significant. Fluorite (CaF2), also known as fluorspar, abundant worldwide, is the main source of fluoride, and hence fluorine. China and Mexico are the major suppliers. Fluorapatite (Ca5(PO4)3F), which contains most of the world's fluoride, is an inadvertent source of fluoride as a byproduct of fertilizer production. Cryolite (Na3AlF6), used in the production of aluminium, is the most fluorine-rich mineral. Economically viable natural sources of cryolite have been exhausted, and most is now synthesized commercially.


Other minerals such as topaz contain fluorine. Fluorides, unlike other halides, are insoluble and do not occur in commercially favorable concentrations in saline waters. Trace quantities of organofluorines of uncertain origin have been detected in volcanic eruptions and geothermal springs. The existence of gaseous fluorine in crystals, suggested by the smell of crushed antozonite, is contentious; a 2012 study reported the presence of 0.04% F2 by weight in antozonite, attributing these inclusions to radiation from the presence of tiny amounts of uranium.


History

Early discoveries

In 1529, Georgius Agricola described fluorite as an additive used to lower the melting point of metals during smelting. He penned the Latin word fluorēs (fluor, flow) for fluorite rocks. The name later evolved into fluorspar (still commonly used) and then fluorite. The composition of fluorite was later determined to be calcium difluoride.


Hydrofluoric acid was used in glass etching from 1720 onward. Andreas Sigismund Marggraf first characterized it in 1764 when he heated fluorite with sulfuric acid, and the resulting solution corroded its glass container. Swedish chemist Carl Wilhelm Scheele repeated the experiment in 1771, and named the acidic product fluss-spats-syran (fluorspar acid). In 1810, the French physicist André-Marie Ampère suggested that hydrogen and an element analogous to chlorine constituted hydrofluoric acid. He also proposed in a letter to Sir Humphry Davy dated August 26, 1812 that this then-unknown substance may be named fluorine from fluoric compounds and the -ine suffix of other halogens. This word, often with modifications, is used in most European languages; however, Greek, Russian, and some others, following Ampère's later suggestion, use the name ftor or derivatives, from the Greek φθόριος (phthorios, destructive). The New Latin name fluorum gave the element its current symbol F; Fl was used in early papers.


Isolation

Initial studies on fluorine were so dangerous that several 19th-century experimenters were deemed "fluorine martyrs" after misfortunes with hydrofluoric acid. Isolation of elemental fluorine was hindered by the extreme corrosiveness of both elemental fluorine itself and hydrogen fluoride, as well as the lack of a simple and suitable electrolyte. Edmond Frémy postulated that electrolysis of pure hydrogen fluoride to generate fluorine was feasible and devised a method to produce anhydrous samples from acidified potassium bifluoride; instead, he discovered that the resulting (dry) hydrogen fluoride did not conduct electricity. Frémy's former student Henri Moissan persevered, and after much trial and error found that a mixture of potassium bifluoride and dry hydrogen fluoride was a conductor, enabling electrolysis. To prevent rapid corrosion of the platinum in his electrochemical cells, he cooled the reaction to extremely low temperatures in a special bath and forged cells from a more resistant mixture of platinum and iridium, and used fluorite stoppers. In 1886, after 74 years of effort by many chemists, Moissan isolated elemental fluorine.


In 1906, two months before his death, Moissan received the Nobel Prize in Chemistry, with the following citation:


n recognition of the great services rendered by him in his investigation and isolation of the element fluorine ... The whole world has admired the great experimental skill with which you have studied that savage beast among the elements.

Later uses

The Frigidaire division of General Motors (GM) experimented with chlorofluorocarbon refrigerants in the late 1920s, and Kinetic Chemicals was formed as a joint venture between GM and DuPont in 1930 hoping to market Freon-12 (CCl2F2) as one such refrigerant. It replaced earlier and more toxic compounds, increased demand for kitchen refrigerators, and became profitable; by 1949 DuPont had bought out Kinetic and marketed several other Freon compounds. Polytetrafluoroethylene (PTFE, Teflon) was serendipitously discovered in 1938 by Roy J. Plunkett while working on refrigerants at Kinetic, and its superlative chemical and thermal resistance lent it to accelerated commercialization and mass production by 1941.


Large-scale production of elemental fluorine began during World War II. Germany used high-temperature electrolysis to make tons of the planned incendiary chlorine trifluoride and the Manhattan Project used huge quantities to produce uranium hexafluoride for uranium enrichment. Since UF6 is as corrosive as fluorine, gaseous diffusion plants required special materials: nickel for membranes, fluoropolymers for seals, and liquid fluorocarbons as coolants and lubricants. This burgeoning nuclear industry later drove post-war fluorochemical development.


Compounds

Fluorine has a rich chemistry, encompassing organic and inorganic domains. It combines with metals, nonmetals, metalloids, and most noble gases. Fluorine's high electron affinity results in a preference for ionic bonding; when it forms covalent bonds, these are polar, and almost always single.


Oxidation states

In compounds, fluorine almost exclusively assumes an oxidation state of −1. Fluorine in F2 is defined to have oxidation state 0. The unstable species F−2 and F−3, which decompose at around 40 K, have intermediate oxidation states; F+4 and a few related species are predicted to be stable.


Metals

Alkali metals form ionic and highly soluble monofluorides; these have the cubic arrangement of sodium chloride and analogous chlorides. Alkaline earth difluorides possess strong ionic bonds but are insoluble in water, with the exception of beryllium difluoride, which also exhibits some covalent character and has a quartz-like structure. Rare earth elements and many other metals form mostly ionic trifluorides.


Covalent bonding first comes to prominence in the tetrafluorides: those of zirconium, hafnium and several actinides are ionic with high melting points, while those of titanium, vanadium, and niobium are polymeric, melting or decomposing at no more than 350 °C (662 °F). Pentafluorides continue this trend with their linear polymers and oligomeric complexes. Thirteen metal hexafluorides are known, all octahedral, and are mostly volatile solids but for liquid MoF6 and ReF6, and gaseous WF6. Rhenium heptafluoride, the only characterized metal heptafluoride, is a low-melting molecular solid with pentagonal bipyramidal molecular geometry. Gold heptafluoride is a low-temperature complex of molecular F2 with AuF5, with NPA calculations indicating that the fluorine in the F2 ligand is nearly neutral while those in the AuF5 portion of the molecule have strong negative partial charges. This is consistent with the F2 ligand representing fluorine in the zero oxidation state. Metal fluorides with more fluorine atoms are particularly reactive.


Hydrogen

Hydrogen and fluorine combine to yield hydrogen fluoride, in which discrete molecules form clusters by hydrogen bonding, resembling water more than hydrogen chloride. It boils at a much higher temperature than heavier hydrogen halides and unlike them is miscible with water. Hydrogen fluoride readily hydrates on contact with water to form aqueous hydrogen fluoride, also known as hydrofluoric acid. Unlike the other hydrohalic acids, which are strong, hydrofluoric acid is a weak acid at low concentrations. However, it can attack glass, something the other acids cannot do.


Other reactive nonmetals

Binary fluorides of metalloids and p-block nonmetals are generally covalent and volatile, with varying reactivities. Period 3 and heavier nonmetals can form hypervalent fluorides.


Boron trifluoride is planar and possesses an incomplete octet. It functions as a Lewis acid and combines with Lewis bases like ammonia to form adducts. Carbon tetrafluoride is tetrahedral and inert; its group analogues, silicon and germanium tetrafluoride, are also tetrahedral but behave as Lewis acids. The pnictogens form trifluorides that increase in reactivity and basicity with higher molecular weight, although nitrogen trifluoride resists hydrolysis and is not basic. The pentafluorides of phosphorus, arsenic, and antimony are more reactive than their respective trifluorides, with antimony pentafluoride the strongest neutral Lewis acid known, only behind gold pentafluoride.


Chalcogens have diverse fluorides: unstable difluorides have been reported for oxygen (the only known compound with oxygen in an oxidation state of +2), sulfur, and selenium; tetrafluorides and hexafluorides exist for sulfur, selenium, and tellurium. The latter are stabilized by more fluorine atoms and lighter central atoms, so sulfur hexafluoride is especially inert. Chlorine, bromine, and iodine can each form mono-, tri-, and pentafluorides, but only iodine heptafluoride has been characterized among possible interhalogen heptafluorides. Many of them are powerful sources of fluorine atoms, and industrial applications using chlorine trifluoride require precautions similar to those using fluorine.


Noble gases

Noble gases, having complete electron shells, defied reaction with other elements until 1962 when Neil Bartlett reported synthesis of xenon hexafluoroplatinate; xenon difluoride, tetrafluoride, hexafluoride, and multiple oxyfluorides have been isolated since then. Among other noble gases, krypton forms a difluoride, and radon and fluorine generate a solid suspected to be radon difluoride. Binary fluorides of lighter noble gases are exceptionally unstable: argon and hydrogen fluoride combine under extreme conditions to give argon fluorohydride. Helium has no long-lived fluorides, and no neon fluoride has ever been observed; helium fluorohydride has been detected for milliseconds at high pressures and low temperatures.


Organic compounds

The carbon–fluorine bond is organic chemistry's strongest, and gives stability to organofluorines. It is almost non-existent in nature, but is used in artificial compounds. Research in this area is usually driven by commercial applications; the compounds involved are diverse and reflect the complexity inherent in organic chemistry.


Discrete molecules

The substitution of hydrogen atoms in an alkane by progressively more fluorine atoms gradually alters several properties: melting and boiling points are lowered, density increases, solubility in hydrocarbons decreases and overall stability increases. Perfluorocarbons, in which all hydrogen atoms are substituted, are insoluble in most organic solvents, reacting at ambient conditions only with sodium in liquid ammonia.


The term perfluorinated compound is used for what would otherwise be a perfluorocarbon if not for the presence of a functional group, often a carboxylic acid. These compounds share many properties with perfluorocarbons such as stability and hydrophobicity, while the functional group augments their reactivity, enabling them to adhere to surfaces or act as surfactants. Fluorosurfactants, in particular, can lower the surface tension of water more than their hydrocarbon-based analogues. Fluorotelomers, which have some unfluorinated carbon atoms near the functional group, are also regarded as perfluorinated.


Polymers

Polymers exhibit the same stability increases afforded by fluorine substitution (for hydrogen) in discrete molecules; their melting points generally increase too. Polytetrafluoroethylene (PTFE), the simplest fluoropolymer and perfluoro analogue of polyethylene with structural unit –CF2–, demonstrates this change as expected, but its very high melting point makes it difficult to mold. Various PTFE derivatives are less temperature-tolerant but easier to mold: fluorinated ethylene propylene replaces some fluorine atoms with trifluoromethyl groups, perfluoroalkoxy alkanes do the same with trifluoromethoxy groups, and Nafion contains perfluoroether side chains capped with sulfonic acid groups. Other fluoropolymers retain some hydrogen atoms; polyvinylidene fluoride has half the fluorine atoms of PTFE and polyvinyl fluoride has a quarter, but both behave much like perfluorinated polymers.


Production

Elemental fluorine and virtually all fluorine compounds are produced from hydrogen fluoride or its aqueous solution, hydrofluoric acid. Hydrogen fluoride is produced in kilns by the endothermic reaction of fluorite (CaF2) with sulfuric acid:


The gaseous HF can then be absorbed in water or liquefied.


About 20% of manufactured HF is a byproduct of fertilizer production, which produces hexafluorosilicic acid (H2SiF6), which can be degraded to release HF thermally and by hydrolysis:


Industrial routes to F2

Moissan's method is used to produce industrial quantities of fluorine, via the electrolysis of a potassium bifluoride/hydrogen fluoride mixture: hydrogen ions are reduced at a steel container cathode and fluoride ions are oxidized at a carbon block anode, under 8–12 volts, to generate hydrogen and fluorine gas respectively. Temperatures are elevated, KF•2HF melting at 70 °C (158 °F) and being electrolyzed at 70–130 °C (158–266 °F). KF, acts to provide electrical conductivity, enabling electrolysis of the virtually non-conductive HF. Fluorine can be stored in steel cylinders that have passivated interiors, at temperatures below 200 °C (392 °F); otherwise nickel can be used. Regulator valves and pipework are made of nickel, the latter possibly using Monel instead. Frequent passivation, along with the strict exclusion of water and greases, must be undertaken. In the laboratory, glassware may carry fluorine gas under low pressure and anhydrous conditions; some sources instead recommend nickel-Monel-PTFE systems.


Laboratory routes

While preparing for a 1986 conference to celebrate the centennial of Moissan's achievement, Karl O. Christe reasoned that chemical fluorine generation should be feasible since some metal fluoride anions have no stable neutral counterparts; their acidification potentially triggers oxidation instead. He devised a method which evolves fluorine at high yield and atmospheric pressure:


Christe later commented that the reactants "had been known for more than 100 years and even Moissan could have come up with this scheme." As late as 2008, some references still asserted that fluorine was too reactive for any chemical isolation.


Industrial applications

Fluorite mining, which supplies most global fluorine, peaked in 1989 when 5.6 million metric tons of ore were extracted. Chlorofluorocarbon restrictions lowered this to 3.6 million tons in 1994; production has since been increasing. Around 4.5 million tons of ore and revenue of US$550 million were generated in 2003; later reports estimated 2011 global fluorochemical sales at $15 billion and predicted 2016–18 production figures of 3.5 to 5.9 million tons, and revenue of at least $20 billion. Froth flotation separates mined fluorite into two main metallurgical grades of equal proportion: 60–85% pure metspar is almost all used in iron smelting whereas 97%+ pure acidspar is mainly converted to the key industrial intermediate hydrogen fluoride.


At least 17,000 metric tons of fluorine are produced industrially each year. It costs only $5–8 per kilogram as uranium or sulfur hexafluoride, but many times more as an element because of handling challenges. Most processes using free fluorine in large amounts employ in situ generation under vertical integration.


The largest application of fluorine gas, consuming up to 7,000 metric tons annually, is in the preparation of UF6 for the nuclear fuel cycle. Fluorine is used to fluorinate uranium tetrafluoride, itself formed from uranium dioxide and hydrofluoric acid. Fluorine is monoisotopic, so any mass differences between UF6 molecules are due to the presence of 235U or 238U, enabling uranium enrichment via gaseous diffusion or gas centrifuge. About 6,000 metric tons per year go into producing the inert dielectric SF6 for high-voltage transformers and circuit breakers, eliminating the need for hazardous polychlorinated biphenyls associated with oil-filled devices. Several fluorine compounds are used in electronics: rhenium and tungsten hexafluoride in chemical vapor deposition, tetrafluoromethane in plasma etching and nitrogen trifluoride in cleaning equipment. Fluorine is also used in the synthesis of organic fluorides, but its reactivity often necessitates conversion first to the gentler ClF3, BrF3, or IF5, which together allow calibrated fluorination. Fluorinated pharmaceuticals use sulfur tetrafluoride instead.


Inorganic fluorides

As with other iron alloys, around 3 kg (6.6 lb) metspar is added to each metric ton of steel; the fluoride ions lower its melting point and viscosity. Alongside its role as an additive in materials like enamels and welding rod coats, most acidspar is reacted with sulfuric acid to form hydrofluoric acid, which is used in steel pickling, glass etching and alkane cracking. One-third of HF goes into synthesizing cryolite and aluminium trifluoride, both fluxes in the Hall–Héroult process for aluminium extraction; replenishment is necessitated by their occasional reactions with the smelting apparatus. Each metric ton of aluminium requires about 23 kg (51 lb) of flux. Fluorosilicates consume the second largest portion, with sodium fluorosilicate used in water fluoridation and laundry effluent treatment, and as an intermediate en route to cryolite and silicon tetrafluoride. Other important inorganic fluorides include those of cobalt, nickel, and ammonium.


Organic fluorides

Organofluorides consume over 20% of mined fluorite and over 40% of hydrofluoric acid, with refrigerant gases dominating and fluoropolymers increasing their market share. Surfactants are a minor application but generate over $1 billion in annual revenue. Due to the danger from direct hydrocarbon–fluorine reactions above −150 °C (−238 °F), industrial fluorocarbon production is indirect, mostly through halogen exchange reactions such as Swarts fluorination, in which chlorocarbon chlorines are substituted for fluorines by hydrogen fluoride under catalysts. Electrochemical fluorination subjects hydrocarbons to electrolysis in hydrogen fluoride, and the Fowler process treats them with solid fluorine carriers like cobalt trifluoride.


Refrigerant gases

Halogenated refrigerants, termed Freons in informal contexts, are identified by R-numbers that denote the amount of fluorine, chlorine, carbon, and hydrogen present. Chlorofluorocarbons (CFCs) like R-11, R-12, and R-114 once dominated organofluorines, peaking in production in the 1980s. Used for air conditioning systems, propellants and solvents, their production was below one-tenth of this peak by the early 2000s, after widespread international prohibition. Hydrochlorofluorocarbons (HCFCs) and hydrofluorocarbons (HFCs) were designed as replacements; their synthesis consumes more than 90% of the fluorine in the organic industry. Important HCFCs include R-22, chlorodifluoromethane, and R-141b. The main HFC is R-134a with a new type of molecule HFO-1234yf, a Hydrofluoroolefin (HFO) coming to prominence owing to its global warming potential of less than 1% that of HFC-134a.


Polymers

About 180,000 metric tons of fluoropolymers were produced in 2006 and 2007, generating over $3.5 billion revenue per year. The global market was estimated at just under $6 billion in 2011. Fluoropolymers can only be formed by polymerizing free radicals.


Polytetrafluoroethylene (PTFE), sometimes called by its DuPont name Teflon, represents 60–80% by mass of the world's fluoropolymer production. The largest application is in electrical insulation since PTFE is an excellent dielectric. It is also used in the chemical industry where corrosion resistance is needed, in coating pipes, tubing, and gaskets. Another major use is in PFTE-coated fiberglass cloth for stadium roofs. The major consumer application is for non-stick cookware. Jerked PTFE film becomes expanded PTFE (ePTFE), a fine-pored membrane sometimes referred to by the brand name Gore-Tex and used for rainwear, protective apparel, and filters; ePTFE fibers may be made into seals and dust filters. Other fluoropolymers, including fluorinated ethylene propylene, mimic PTFE's properties and can substitute for it; they are more moldable, but also more costly and have lower thermal stability. Films from two different fluoropolymers replace glass in solar cells.


The chemically resistant (but expensive) fluorinated ionomers are used as electrochemical cell membranes, of which the first and most prominent example is Nafion. Developed in the 1960s, it was initially deployed as fuel cell material in spacecraft and then replaced mercury-based chloralkali process cells. Recently, the fuel cell application has reemerged with efforts to install proton exchange membrane fuel cells into automobiles. Fluoroelastomers such as Viton are crosslinked fluoropolymer mixtures mainly used in O-rings; perfluorobutane (C4F10) is used as a fire-extinguishing agent.


Surfactants

Fluorosurfactants are small organofluorine molecules used for repelling water and stains. Although expensive (comparable to pharmaceuticals at $200–2000 per kilogram), they yielded over $1 billion in annual revenues by 2006; Scotchgard alone generated over $300 million in 2000. Fluorosurfactants are a minority in the overall surfactant market, most of which is taken up by much cheaper hydrocarbon-based products. Applications in paints are burdened by compounding costs; this use was valued at only $100 million in 2006.


Agrichemicals

About 30% of agrichemicals contain fluorine, most of them herbicides and fungicides with a few crop regulators. Fluorine substitution, usually of a single atom or at most a trifluoromethyl group, is a robust modification with effects analogous to fluorinated pharmaceuticals: increased biological stay time, membrane crossing, and altering of molecular recognition. Trifluralin is a prominent example, with large-scale use in the U.S. as a weedkiller, but it is a suspected carcinogen and has been banned in many European countries. Sodium monofluoroacetate (1080) is a mammalian poison in which one sodium acetate hydrogen is replaced with fluorine; it disrupts cell metabolism by replacing acetate in the citric acid cycle. First synthesized in the late 19th century, it was recognized as an insecticide in the early 20th century, and was later deployed in its current use. New Zealand, the largest consumer of 1080, uses it to protect kiwis from the invasive Australian common brushtail possum. Europe and the U.S. have banned 1080.


Medicinal applications

Dental care

Population studies from the mid-20th century onwards show topical fluoride reduces dental caries. This was first attributed to the conversion of tooth enamel hydroxyapatite into the more durable fluorapatite, but studies on pre-fluoridated teeth refuted this hypothesis, and current theories involve fluoride aiding enamel growth in small caries. After studies of children in areas where fluoride was naturally present in drinking water, controlled public water supply fluoridation to fight tooth decay began in the 1940s and is now applied to water supplying 6 percent of the global population, including two-thirds of Americans. Reviews of the scholarly literature in 2000 and 2007 associated water fluoridation with a significant reduction of tooth decay in children. Despite such endorsements and evidence of no adverse effects other than mostly benign dental fluorosis, opposition still exists on ethical and safety grounds. The benefits of fluoridation have lessened, possibly due to other fluoride sources, but are still measurable in low-income groups. Sodium monofluorophosphate and sometimes sodium or tin(II) fluoride are often found in fluoride toothpastes, first introduced in the U.S. in 1955 and now ubiquitous in developed countries, alongside fluoridated mouthwashes, gels, foams, and varnishes.


Pharmaceuticals

Twenty percent of modern pharmaceuticals contain fluorine. One of these, the cholesterol-reducer atorvastatin (Lipitor), made more revenue than any other drug until it became generic in 2011. The combination asthma prescription Seretide, a top-ten revenue drug in the mid-2000s, contains two active ingredients, one of which – fluticasone – is fluorinated. Many drugs are fluorinated to delay inactivation and lengthen dosage periods because the carbon–fluorine bond is very stable. Fluorination also increases lipophilicity because the bond is more hydrophobic than the carbon–hydrogen bond, and this often helps in cell membrane penetration and hence bioavailability.


Tricyclics and other pre-1980s antidepressants had several side effects due to their non-selective interference with neurotransmitters other than the serotonin target; the fluorinated fluoxetine was selective and one of the first to avoid this problem. Many current antidepressants receive this same treatment, including the selective serotonin reuptake inhibitors: citalopram, its enantiomer escitalopram, and fluvoxamine and paroxetine. Quinolones are artificial broad-spectrum antibiotics that are often fluorinated to enhance their effects. These include ciprofloxacin and levofloxacin. Fluorine also finds use in steroids: fludrocortisone is a blood pressure-raising mineralocorticoid, and triamcinolone and dexamethasone are strong glucocorticoids. The majority of inhaled anesthetics are heavily fluorinated; the prototype halothane is much more inert and potent than its contemporaries. Later compounds such as the fluorinated ethers sevoflurane and desflurane are better than halothane and are almost insoluble in blood, allowing faster waking times.


PET scanning

Fluorine-18 is often found in radioactive tracers for positron emission tomography, as its half-life of almost two hours is long enough to allow for its transport from production facilities to imaging centers. The most common tracer is fluorodeoxyglucose which, after intravenous injection, is taken up by glucose-requiring tissues such as the brain and most malignant tumors; computer-assisted tomography can then be used for detailed imaging.


Oxygen carriers

Liquid fluorocarbons can hold large volumes of oxygen or carbon dioxide, more so than blood, and have attracted attention for their possible uses in artificial blood and in liquid breathing. Because fluorocarbons do not normally mix with water, they must be mixed into emulsions (small droplets of perfluorocarbon suspended in water) to be used as blood. One such product, Oxycyte, has been through initial clinical trials. These substances can aid endurance athletes and are banned from sports; one cyclist's near death in 1998 prompted an investigation into their abuse. Applications of pure perfluorocarbon liquid breathing (which uses pure perfluorocarbon liquid, not a water emulsion) include assisting burn victims and premature babies with deficient lungs. Partial and complete lung filling have been considered, though only the former has had any significant tests in humans. An Alliance Pharmaceuticals effort reached clinical trials but was abandoned because the results were not better than normal therapies.


Biological role

Fluorine is not essential for humans and other mammals, but small amounts are known to be beneficial for the strengthening of dental enamel (where the formation of fluorapatite makes the enamel more resistant to attack, from acids produced by bacterial fermentation of sugars). Small amounts of fluorine may be beneficial for bone strength, but the latter has not been definitively established. Both the WHO and the Institute of Medicine of the US National Academies publish recommended daily allowance (RDA) and upper tolerated intake of fluorine, which varies with age and gender.


Natural organofluorines have been found in microorganisms, plants and, recently, animals. The most common is fluoroacetate, which is used as a defense against herbivores by at least 40 plants in Africa, Australia and Brazil. Other examples include terminally fluorinated fatty acids, fluoroacetone, and 2-fluorocitrate. An enzyme that binds fluorine to carbon – adenosyl-fluoride synthase – was discovered in bacteria in 2002.


Toxicity

Elemental fluorine is highly toxic to living organisms. Its effects in humans start at concentrations lower than hydrogen cyanide's 50 ppm and are similar to those of chlorine: significant irritation of the eyes and respiratory system as well as liver and kidney damage occur above 25 ppm, which is the immediately dangerous to life and health value for fluorine. The eyes and nose are seriously damaged at 100 ppm, and inhalation of 1,000 ppm fluorine will cause death in minutes, compared to 270 ppm for hydrogen cyanide.


Hydrofluoric acid

Hydrofluoric acid is the weakest of the hydrohalic acids, having a pKa of 3.2 at 25 °C. Pure hydrogen fluoride is a volatile liquid due to the presence of hydrogen bonding, while the other hydrogen halides are gases. It is able to attack glass, concrete, metals, and organic matter.


Hydrofluoric acid is a contact poison with greater hazards than many strong acids like sulfuric acid even though it is weak: it remains neutral in aqueous solution and thus penetrates tissue faster, whether through inhalation, ingestion or the skin, and at least nine U.S. workers died in such accidents from 1984 to 1994. It reacts with calcium and magnesium in the blood leading to hypocalcemia and possible death through cardiac arrhythmia. Insoluble calcium fluoride formation triggers strong pain and burns larger than 160 cm2 (25 in2) can cause serious systemic toxicity.


Exposure may not be evident for eight hours for 50% HF, rising to 24 hours for lower concentrations, and a burn may initially be painless as hydrogen fluoride affects nerve function. If skin has been exposed to HF, damage can be reduced by rinsing it under a jet of water for 10–15 minutes and removing contaminated clothing. Calcium gluconate is often applied next, providing calcium ions to bind with fluoride; skin burns can be treated with 2.5% calcium gluconate gel or special rinsing solutions. Hydrofluoric acid absorption requires further medical treatment; calcium gluconate may be injected or administered intravenously. Using calcium chloride – a common laboratory reagent – in lieu of calcium gluconate is contraindicated, and may lead to severe complications. Excision or amputation of affected parts may be required.


Fluoride ion

Soluble fluorides are moderately toxic: 5–10 g sodium fluoride, or 32–64 mg fluoride ions per kilogram of body mass, represents a lethal dose for adults. One-fifth of the lethal dose can cause adverse health effects, and chronic excess consumption may lead to skeletal fluorosis, which affects millions in Asia and Africa, and, in children, to reduced intelligence. Ingested fluoride forms hydrofluoric acid in the stomach which is easily absorbed by the intestines, where it crosses cell membranes, binds with calcium and interferes with various enzymes, before urinary excretion. Exposure limits are determined by urine testing of the body's ability to clear fluoride ions.


Historically, most cases of fluoride poisoning have been caused by accidental ingestion of insecticides containing inorganic fluorides. Most current calls to poison control centers for possible fluoride poisoning come from the ingestion of fluoride-containing toothpaste. Malfunctioning water fluoridation equipment is another cause: one incident in Alaska affected almost 300 people and killed one person. Dangers from toothpaste are aggravated for small children, and the Centers for Disease Control and Prevention recommends supervising children below six brushing their teeth so that they do not swallow toothpaste. One regional study examined a year of pre-teen fluoride poisoning reports totaling 87 cases, including one death from ingesting insecticide. Most had no symptoms, but about 30% had stomach pains. A larger study across the U.S. had similar findings: 80% of cases involved children under six, and there were few serious cases.


Environmental concerns

Atmosphere

The Montreal Protocol, signed in 1987, set strict regulations on chlorofluorocarbons (CFCs) and bromofluorocarbons due to their ozone damaging potential (ODP). The high stability which suited them to their original applications also meant that they were not decomposing until they reached higher altitudes, where liberated chlorine and bromine atoms attacked ozone molecules. Even with the ban, and early indications of its efficacy, predictions warned that several generations would pass before full recovery. With one-tenth the ODP of CFCs, hydrochlorofluorocarbons (HCFCs) are the current replacements, and are themselves scheduled for substitution by 2030–2040 by hydrofluorocarbons (HFCs) with no chlorine and zero ODP. In 2007 this date was brought forward to 2020 for developed countries; the Environmental Protection Agency had already prohibited one HCFC's production and capped those of two others in 2003. Fluorocarbon gases are generally greenhouse gases with global-warming potentials (GWPs) of about 100 to 10,000; sulfur hexafluoride has a value of around 20,000. An outlier is HFO-1234yf which is a new type of refrigerant called a Hydrofluoroolefin (HFO) and has attracted global demand due to its GWP of less than 1 compared to 1,430 for the current refrigerant standard HFC-134a.


Biopersistence

Organofluorines exhibit biopersistence due to the strength of the carbon–fluorine bond. Perfluoroalkyl acids (PFAAs), which are sparingly water-soluble owing to their acidic functional groups, are noted persistent organic pollutants; perfluorooctanesulfonic acid (PFOS) and perfluorooctanoic acid (PFOA) are most often researched. PFAAs have been found in trace quantities worldwide from polar bears to humans, with PFOS and PFOA known to reside in breast milk and the blood of newborn babies. A 2013 review showed a slight correlation between groundwater and soil PFAA levels and human activity; there was no clear pattern of one chemical dominating, and higher amounts of PFOS were correlated to higher amounts of PFOA. In the body, PFAAs bind to proteins such as serum albumin; they tend to concentrate within humans in the liver and blood before excretion through the kidneys. Dwell time in the body varies greatly by species, with half-lives of days in rodents, and years in humans. High doses of PFOS and PFOA cause cancer and death in newborn rodents but human studies have not established an effect at current exposure levels.


See also

Notes

Sources

Citations

Indexed references

External links




Jehovah's Witnesses is a nontrinitarian, millenarian, and restorationist Christian denomination, stemming from the Bible Student movement founded by Charles Taze Russell in the nineteenth century. Russell co-founded Zion's Watch Tower Tract Society in 1881 to organize and print the movement's publications. A leadership dispute after Russell's death resulted in several groups breaking away, with Joseph Franklin Rutherford retaining control of the Watch Tower Society and its properties. Rutherford made significant organizational and doctrinal changes, including adoption of the name Jehovah's witnesses in 1931 to distinguish the group from other Bible Student groups and symbolize a break with the legacy of Russell's traditions. In 2025, Jehovah's Witnesses reported a peak membership of approximately 9.2 million worldwide.


Jehovah's Witnesses are known for their evangelism, distributing literature such as The Watchtower and Awake!, and for refusing military service and blood transfusions. They consider the use of God's name vital for proper worship. They reject Trinitarianism, inherent immortality of the soul, and hell, which they consider unscriptural doctrines. Jehovah's Witnesses believe that the destruction of the present world system at Armageddon is imminent, and the establishment of God's kingdom over earth is the only solution to all of humanity's problems. They do not observe Christmas, Easter, birthdays, or other holidays and customs they consider to have pagan origins incompatible with Christianity. They prefer to use their own Bible translation, the New World Translation of the Holy Scriptures.


Jehovah's Witnesses consider human society morally corrupt and under the influence of Satan, and most limit their social interaction with non-Witnesses. The denomination is directed by a group known as the Governing Body of Jehovah's Witnesses, which establishes all doctrines. Congregational disciplinary actions include formal expulsion and shunning, for what they consider serious offenses. Members who formally leave are considered to be disassociated and are also shunned. Some members who leave voluntarily successfully "fade" without being shunned. Former members may experience significant mental distress as a result of being shunned, and some seek reinstatement to maintain contact with their friends and family.


The group's position on conscientious objection to military service and refusal to salute state symbols—for example, national anthems and flags—has brought it into conflict with several governments. Jehovah's Witnesses have been persecuted, with their activities banned or restricted in some countries. Persistent legal challenges by Jehovah's Witnesses have influenced legislation related to civil rights in several countries. The organization has been criticized regarding biblical translation, doctrines, and alleged coercion of its members. The Watch Tower Society has made unfulfilled predictions about major biblical events, such as Jesus' Second Coming, the advent of God's kingdom, and Armageddon. Their policies for handling cases of child sexual abuse have been the subject of various formal inquiries.


Demographics

Jehovah's Witnesses have an active presence in most countries. In 2025, Jehovah's Witnesses reported approximately 9 million publishers—the term they use for members actively involved in preaching—in about 119,500 congregations. In the same year, they conducted Bible studies with 7,603,182 individuals (including those conducted by Witness parents with their children). 4,091 members served as missionaries in 2021. In 2025, Jehovah's Witnesses reported a worldwide annual increase of 2.5%. 20,635,015 people attended the annual memorial of Christ's death. The official published membership statistics, such as those above, include only those who submit reports for their personal ministry. As a result, only about half of those who self-identify as Jehovah's Witnesses in independent demographic studies are considered active by the faith itself. Research regarding the demographics of Jehovah's Witnesses is incredibly limited; sample sizes tend to be small and focused to a specific region. Cross-cultural studies are "virtually non-existent".


The 2008 US Pew Forum on Religion & Public Life survey found a low retention rate among members of the denomination: about 37% of people raised in the group continued to identify as Jehovah's Witnesses. The next lowest retention rates were for Buddhism at 50% and Catholicism at 68%. The study also found that 65% of adult American Jehovah's Witnesses are converts. In 2016, Jehovah's Witnesses had the lowest average household income among surveyed religious groups, with approximately half of Witness households in the United States earning less than $30,000 per year. As of 2016, Jehovah's Witnesses were considered to be the most racially diverse Christian denomination in the United States. A sociological comparative study by the Pew Research Center found that American Jehovah's Witnesses ranked highest in belief in God, importance of religion in one's life, frequency of religious attendance, frequency of prayers, frequency of Bible reading outside of religious services, belief that their prayers are answered, belief that their religion can only be interpreted one way, belief that theirs is the only one true faith leading to eternal life, opposition to abortion, and opposition to homosexuality. Jehovah's Witnesses also ranked lowest in interest in politics.


History

Scholarly analysis of Jehovah's Witnesses is limited in academia, with most works focusing on legal challenges faced by the group. The denomination does not cooperate with scholars beyond limited communication from anonymous individuals. Consequently, academics often rely on literature written by former members such as James Penton and Raymond Franz to understand its inner workings. The denomination has been variously described as a church, sect, new religious movement, or cult. Usage of the various terms has been debated among sociologists. When the term sect is used by sociologists, it is within the framework of church-sect typology for their activities within a specific country. Sociologists from the 1940s to the 1960s frequently compared the group's structure with totalitarianism. Throughout the 1970s and 80s, sociologists determined that cult was a reductionist label when applied to Jehovah's Witnesses, noting that new members did not undergo "sudden transformations" and made a rational choice to join the group. Academics generally stopped using the term cult in the 1980s due to its pejorative association and its usage by the Christian countercult movement, with new religious movement largely replacing it. Scholars George Chryssides and Zoe Knox avoid using the term new religious movement because it also has negative connotations. Chryssides refers to the denomination as an "old new religion".


Background

In 1870, Charles Taze Russell and others formed a group in Pittsburgh, Pennsylvania, to study the Bible. During his ministry, Russell disputed many of mainstream Christianity's tenets, including immortality of the soul, hellfire, predestination, Christ's return, the Trinity, and the burning up of the world. In 1876, he met Nelson H. Barbour. Later that year they jointly produced the book Three Worlds, which combined restitutionist views with end time prophecy.


The book taught that God's dealings with humanity were divided dispensationally, with each period ending with a "harvest", and that Jesus inaugurated the "harvest of the Gospel age" by means of his invisible return in 1874. The book asserted that 1914 would mark the end of a 2,520-year period called "the Gentile Times", at which time world society would be replaced by the full establishment of God's kingdom on earth. Beginning in 1878, Russell and Barbour jointly edited a religious magazine, Herald of the Morning. In June 1879, the two split over doctrinal differences, and in July, Russell began publishing the magazine Zion's Watch Tower and Herald of Christ's Presence, saying its purpose was to demonstrate that the world was in "the last days" and that a new age of earthly and human restitution under Jesus' reign was imminent.


From 1879, Watch Tower supporters gathered as autonomous congregations to study the Bible topically. Thirty congregations were founded, and during 1879 and 1880, Russell visited each to provide the format he recommended for conducting meetings. In 1881, Zion's Watch Tower Tract Society was presided over by William Henry Conley, and in 1884, Russell incorporated the society as a nonprofit business to distribute tracts and Bibles. He also published a six book series entitled Studies in the Scriptures. By about 1900, Russell had organized thousands of part- and full-time colporteurs, and was appointing foreign missionaries and establishing branch offices. By the 1910s, Russell's organization maintained nearly a hundred "pilgrims", or traveling preachers. Russell engaged in significant global publishing efforts during his ministry, and by 1912, he was the most distributed Christian author in the United States. He also directed The Photo-Drama of Creation, an eight-hour audiovisual presentation featuring biblical accounts.


Russell moved the Watch Tower Society's headquarters to Brooklyn, New York, in 1909, combining printing and corporate offices with a house of worship; volunteers were housed in a nearby residence he named Bethel. He identified the religious movement as "Bible Students", and more formally as the International Bible Students Association. By 1910, about 50,000 people worldwide were associated with the movement and congregations reelected him annually as their pastor. Russell died on October 31, 1916, at the age of 64 while returning from a ministerial speaking tour.


Joseph Rutherford

In January 1917, the Watch Tower Society's legal representative, Joseph Franklin Rutherford, was elected as its next president. His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner. The divisions between his supporters and opponents triggered a major turnover of members over the next decade. Because of disappointment over the changes and unfulfilled predictions, tens of thousands of defections occurred during the first half of Rutherford's tenure, leading to the formation of several Bible Student organizations independent of the Watch Tower Society, the largest of which was the Dawn Bible Students Association. There are varying estimates of how many Bible Students left during Rutherford's tenure, with Alan Rogerson believing the total number to be unclear. By mid-1919, an estimated one in seven of Russell-era Bible Students had ceased their association with the Society. Between 1921 and 1931 three-quarters were estimated to have left.


Rutherford enacted several changes under his leadership, many of which are considered "distinctive" to modern Jehovah's Witness beliefs and practices. Some of these changes include advocating for door-to-door preaching, prohibiting celebrations believed to be pagan such as Christmas, the belief that Jesus died on a stake instead of a cross, and a more uniform organizational hierarchy. In 1919, Rutherford instituted the appointment of a director in each congregation, and a year later all members were instructed to report their weekly preaching activity to the Brooklyn headquarters. In 1920, he announced that the Hebrew patriarchs (such as Abraham and Isaac) would be resurrected in 1925, marking the beginning of Christ's thousand-year earthly kingdom. In July 1917, he released The Finished Mystery as a seventh volume to the Studies in the Scriptures series. Rutherford claimed it to be Russell's posthumous work, but it was actually written by Clayton Woodworth, George Fisher, and Gertrude Seibert. It strongly criticized Catholic and Protestant clergy and Christian involvement in the Great War. As a result, Watch Tower Society directors were jailed for sedition under the Espionage Act in 1918 and members were subjected to mob violence; the directors were released in March 1919 and charges against them were dropped in 1920.


On July 26, 1931, at a convention in Columbus, Ohio, Rutherford introduced the new name Jehovah's witnesses, based on Isaiah 43:10: "Ye are my witnesses, saith the Lord, and my servant whom I have chosen: that ye may know and believe me, and understand that I am he: before me there was no God formed, neither shall there be after me" (King James Version). It was adopted by resolution. The name was chosen to distinguish his group of Bible Students from other independent groups that had severed ties with the Society, as well as to symbolize the instigation of new outlooks and the promotion of fresh evangelizing methods.


In 1932, Rutherford eliminated the system of locally elected elders. In 1938, he introduced what he called a theocratic organizational system, under which appointments in congregations worldwide were made from the Brooklyn headquarters. Doctrine regarding life after death also evolved under his tenure. In addition to the preexisting belief that there would be 144,000 people to survive Armageddon and live in heaven to rule over earth with Jesus, a separate class of members, the "great multitude", was introduced. This group would live in a paradise restored on earth; from 1935, new converts to the movement were considered part of that class. By the mid-1930s, the timing of the beginning of Jesus' presence, his enthronement as king, and the start of the last days were each moved to 1914. As their interpretations of the Bible evolved, Witness publications decreed that saluting national flags was a form of idolatry, which led to a new outbreak of mob violence and government opposition in various countries.


Nathan Knorr

Nathan Knorr was appointed as third president of the Watch Tower Bible and Tract Society in 1942. Knorr organized large international assemblies, instituted new training programs for members, and expanded missionary activity and branch offices worldwide. He also increased the use of explicit instructions guiding Jehovah's Witnesses' lifestyle and conduct as well as a greater use of congregational judicial procedures to enforce a strict moral code. Watch Tower Society literature stopped crediting individual contributors during his tenure, as he believed that recognition should only be given to God.


Knorr commissioned a new translation of the Bible, the New World Translation of the Holy Scriptures, the full version of which was released in 1961. Various Bible scholars, including Bruce M. Metzger and MacLean Gilmour, have said that while scholarship is evident in New World Translation, its rendering of certain texts is inaccurate and biased in favor of Witness practices and doctrines. Critics of the group such as Edmund C. Gruss and Christian writers such as Ray C. Stedman, Walter Martin, Norman Klann, and Anthony Hoekema say the New World Translation is scholastically dishonest. Most criticism of the New World Translation relates to its rendering of the New Testament, particularly regarding the introduction of the name Jehovah and in passages related to the Trinity doctrine.


The offices of elder and ministerial servant were restored to Witness congregations in 1972. In a major organizational overhaul in 1976, the power of the Watch Tower Society president was diminished, with authority for doctrinal and organizational decisions being passed to the Governing Body. Knorr introduced these changes as he believed that people making spiritual decisions should be "called by Christ" rather than elected. The presidency's role transitioned into heading the denomination's legal entity. The distinction between these roles grew further when all Governing Body members resigned as directors and the Christian Congregation of Jehovah's Witnesses, Inc. was formed in 2000. Since Knorr's death in 1977, the presidency has been held by Frederick Franz, Milton Henschel, Don Alden Adams and Robert Ciranko.


Further development

From 1966, Witness publications and convention talks built anticipation of the possibility that Jesus' thousand-year reign might begin in 1975 or shortly thereafter. The number of baptisms increased significantly, from about 59,000 in 1966 to more than 297,000 in 1974. By 1975, the number of active members exceeded two million. From 1971 to 1981, there was a net increase of 737,241 publishers worldwide, while baptisms totaled 1.71 million for the same period. Watch Tower Society literature did not say that 1975 would definitely mark the end, though it was strongly implied. Frederick Franz, then–president of the Watchtower Bible and Tract Society, stated at a 1975 convention that the great tribulation could be expected to start by the end of that year. Many Jehovah's Witnesses acted upon this information by quitting their jobs and preaching more fervently. After that prediction failed, ordinary Jehovah's Witness members were blamed for believing in the date rather than the Governing Body acknowledging responsibility. Membership declined significantly for a few years after the failed prediction.


Jehovah's Witnesses have not set any specific dates for the end since 1975. Their publications emphasize that "one cannot know the day or the hour", but they still believe Armageddon to be imminent. Verse 34 of Matthew 24, where Jesus tells his disciples that "this generation will by no means pass away until all these things happen", was interpreted to refer to the generation of people alive in 1914. The initial teaching was that Armageddon would begin before the last person alive during that timeframe had died. The time limit was removed in 1995. This doctrine changed further in 2008, where generation was interpreted to refer to both the original anointed class and their remnant, the latter of which would be alive when Armageddon began. In 2010, the meaning of generation was re-interpreted to include individuals whose lives overlapped with anointed individuals alive during 1914.


Organization

Jehovah's Witnesses are organized hierarchically, in what the leadership calls a theocratic organization, reflecting their belief that it is God's visible organization on earth. Jehovah's Witnesses establish branch offices to manage their activities in various countries or regions. Each branch office is also referred to as Bethel. Supporting staff live on these properties where they operate as a religious community and administrative unit. Their living expenses and those of other full-time volunteers are covered along with a basic monthly stipend. These volunteers are called Bethelites and are assigned specific tasks such as printing literature or doing laundry. They are allowed to marry but must leave Bethel if they have children. Bethelites are expected to read the Bible cover-to-cover during their first year of service. Consultants are sometimes hired for specialized tasks such as legal advice. Regular Jehovah's Witness members are encouraged to visit Bethel as a recreational activity.


Traveling overseers appoint local elders and ministerial servants, while branch offices may appoint regional committees for matters such as Kingdom Hall construction or disaster relief. Each congregation has a body of appointed unpaid male elders and ministerial servants. Elders maintain general responsibility for congregational governance, setting meeting times, selecting speakers and conducting meetings, directing the public preaching work, and creating judicial committees to investigate and decide disciplinary action for cases involving sexual misconduct or doctrinal breaches. New elders are appointed by a traveling overseer after recommendation by the existing body of elders. Ministerial servants—appointed in a similar manner as elders—fulfill clerical and attendant duties, but may also teach and conduct meetings. Jehovah's Witnesses do not use elder as a title to signify a formal clergy-laity division, though elders may employ ecclesiastical privilege regarding confession of sins.


Much of the denomination's funding is donated, primarily by members. There is no tithing or collection. In 2001 Newsday listed the Watch Tower Society as one of New York's 40 richest corporations, with revenues exceeding $950 million. In 2016, it ranked eighteenth for donations received by registered charities in Canada at $80 million. From 1969 until 2015, the denomination's headquarters were housed in Brooklyn, with plans to completely move its operations to Warwick in 2017. The property was sold to Kushner Companies for $340 million in 2016.


Governing Body

The denomination is led by the Governing Body—an all-male group that varies in size. The Governing Body directs several committees that are responsible for administrative functions, including publishing, assembly programs and evangelizing activities. Doctrines of Jehovah's Witnesses are established by the Governing Body, which assumes responsibility for interpreting and applying scripture. The Governing Body does not issue a single, comprehensive statement of faith, but expresses its doctrinal positions through publications published by the Watch Tower Society. The publications teach that doctrinal changes and refinements result from a process of progressive revelation, in which God gradually reveals his will and purpose, and that such enlightenment or "new light" results from the application of reason and study.


Sociologist Andrew Holden's ethnographic study of the group concluded that pronouncements of the Governing Body, through Watch Tower Society publications, carry almost as much weight as the Bible. The organization makes no provision for members to criticize or contribute to its teachings. Witness publications strongly discourage followers from questioning doctrine and counsel received from the Governing Body, reasoning that it is to be trusted as part of "God's organization". The denomination does not tolerate dissent over doctrines and practices; members who openly disagree with the group's teachings are expelled and shunned.


Gender roles

Jehovah's Witnesses have a complementarian view of women. Only men may hold positions of authority, such as ministerial servant or elder. Women may actively participate in the public preaching work, serve at Bethel, and profess to be members of the 144,000. They are not typically allowed to address the congregation directly. In rare circumstances, women can substitute in certain capacities if there are no eligible men. In these situations, women must wear a head covering if they are performing a teaching role. Jehovah's Witnesses believe that transgender people should live as the gender they were assigned at birth and view gender-affirming surgery as mutilation. Modesty in dress and grooming is frequently emphasized for both men and women.


Beliefs

Jehovah's Witnesses believe their denomination is a restoration of first-century Christianity. They believe that mainstream Christianity departed from true worship over time, that groups such as Cathars attempted to restore some aspects of it, and that the Protestant Reformation "did not go far enough". Older books published by the Watch Tower Society such as those by Charles Russell and Joseph Rutherford are usually unfamiliar to a modern Jehovah's Witness, although some congregations have these publications in their libraries. Jehovah's Witnesses consider the Bible scientifically and historically accurate and reliable and interpret much of it literally, but accept parts of it as symbolic. Jehovah's Witnesses are old Earth creationists. The entire Protestant canon of scripture is considered the inspired, inerrant word of God. Regular personal Bible reading is frequently recommended. Members are discouraged from formulating doctrines and "private ideas" reached through Bible research independent of Watch Tower Society publications and are cautioned against reading other religious literature. Adherents commonly call their body of beliefs "The Truth".


Jehovah

Jehovah's Witnesses emphasize the use of God's name, and they prefer the form Jehovah—a vocalization of God's name based on the Tetragrammaton. They believe that Jehovah is the only true god, the creator of all things, and the "Universal Sovereign". They believe that all worship should be directed toward him, and that he is not part of a Trinity; consequently, the group places more emphasis on God than on Christ. They believe that the Holy Spirit is God's applied power or "active force". Jehovah's Witnesses believe that they can have a personal relationship with God.


Jesus

Jehovah's Witnesses believe that Jesus is God's only direct creation, that everything else was created through him by means of God's power, and that the initial unassisted act of creation uniquely identifies Jesus as God's "only-begotten Son". As part of their nontrinitarian beliefs, they do not believe that Jesus is God the Son. They do believe that he was the first angel, and is the only archangel. Jehovah's Witnesses believe that Mary conceived Jesus as a virgin but do not believe that she was born free from sin or that she remained a virgin after his birth. Jehovah's Witnesses believe that Jesus served as a redeemer and a ransom sacrifice to atone for original sin. They believe that he died on a single upright post rather than a cross, which they regard as a pagan symbol. Accordingly, they do not use the word "crucifixion" when referring to Jesus' death. Jehovah's Witnesses believe that Jesus was resurrected with a "spirit body", and that he assumed human form only temporarily after his resurrection. Biblical references to the Michael, Abaddon (Apollyon), and the Word are interpreted as names for Jesus in various roles. Jesus is considered the only intercessor and high priest between God and humanity, appointed by God as the king and judge of his kingdom.


Life after death

Jehovah's Witnesses believe death is a state of nonexistence with no consciousness. There is no Hell of fiery torment; Hades and Sheol are understood to refer to the condition of death, termed the common grave. They consider the soul a life or a living body that can die. They believe that humanity is in a sinful state, from which release is possible only by means of Jesus' shed blood as a ransom, or atonement, for humankind's sins. Jehovah's Witnesses believe that a "little flock" of 144,000 selected humans go to heaven, but that God will resurrect the majority (the "other sheep") to a cleansed earth after Armageddon. They interpret Revelation 14:1–5 to mean that the number of Christians going to heaven is limited to exactly 144,000, who will rule with Jesus as kings and priests over earth. They believe that baptism as a Jehovah's Witness is vital for salvation, and do not recognize baptism from other denominations as valid. Jehovah's Witnesses believe that some people who died before Armageddon will be resurrected, taught the proper way to worship God, and then face a final test at the end of the millennial reign. This judgment will be based on their actions after resurrection rather than past deeds. At the end of the thousand years, Jesus will hand all authority back to God. Then a final test will take place when Satan is released to mislead humankind. Those who fail will die, along with Satan and his demons. They also believe that those who rejected their beliefs while still alive will not be resurrected.


Eschatology

Jehovah's Witnesses believe that Satan was originally a perfect angel who developed feelings of self-importance and craved worship. Satan influenced Adam and Eve to disobey God, and humanity subsequently became participants in a challenge involving the competing claims of Jehovah and Satan to universal sovereignty. Jehovah's Witnesses believe that Jesus began to rule invisibly in heaven as king of God's kingdom in October 1914 and that Satan was subsequently ousted from heaven to the earth. They base this belief on a rendering of the Greek word parousia—usually translated as "coming" when referring to Jesus—as "presence". Jehovah's Witnesses believe that they are the kingdom's representatives on earth. They also believe that they must remain separate from human governments, which they consider to be controlled by Satan. The kingdom is viewed as the means by which God will accomplish his original purpose for the earth, transforming it into a paradise without sickness or death. Jehovah's Witnesses do not currently suggest any specific date for the end of the world, but Watch Tower Society literature has previously made such statements about 1914, 1925 and 1975. These failed predictions were presented as "beyond doubt" and "approved by God". Some Watch Tower Society publications state that God has used Jehovah's Witnesses and the International Bible Students as a modern-day prophet.


A central teaching of Jehovah's Witnesses is that the world faces imminent destruction through intervention by God and Jesus Christ. This belief has been present since the group's founding. They believe that Jesus' inauguration as king in 1914 is a sign that the great tribulation is about to take place. Jehovah's Witnesses believe that all other present-day religions are false, identifying them with Babylon the Great, the "harlot" of Revelation 17. They believe that Nebuchadnezzar II had a dream where he saw a statue with a gold head, silver chest and arms, copper abdomen, iron legs, and feet that were a mixture of clay and iron. This dream is interpreted as a prophecy representing the rise and fall of empires: gold represents Babylon, silver represents Persia, copper represents Greece, iron represents Rome, and clay represents an Anglo-American empire. Jehovah's Witnesses believe that humanity is currently living in the last empire that will eventually be destroyed by the United Nations, which is also interpreted as the scarlet-colored wild beast. Satan will subsequently use world governments to attack Jehovah's Witnesses, which will prompt God to begin the war of Armageddon, during which all forms of human government will be destroyed and all people not counted as Jesus' sheep will be killed. After Armageddon, God will extend his heavenly kingdom to include earth, which will be transformed into a paradise like the Garden of Eden. They thus depart from the mainstream Christian belief that the "second coming" of Matthew 24 refers to a single moment of arrival on earth to judge humans.


Family life

Jehovah's Witnesses believe that dating should only occur if the couple is seriously considering marriage. Dating outside the denomination is strongly discouraged and can lead to religious sanctions. Dating Jehovah's Witnesses are encouraged to have a chaperone when they are together to avoid acting on sexual desires. All sexual relations outside marriage are grounds for expulsion if the person is not deemed repentant; homosexual activity is considered a serious sin, and same-sex marriage is forbidden. Masturbation is also prohibited.


Jehovah's Witnesses may get married at a Kingdom Hall in a simple ceremony and practices considered pagan such as wishing good luck or throwing rice are prohibited. An elder will give a talk to the congregation. Once married, a husband is considered to have spiritual headship over his wife, unless he is not one of Jehovah's Witnesses. Contraception is allowed. Divorce is forbidden if not sought on the grounds of adultery, which is called a "scriptural divorce". If a divorce is obtained for any other reason, remarriage is considered adulterous unless the former spouse has died or is considered to have committed sexual immorality. Spouses may separate in cases of domestic violence. Jehovah's Witness households are expected to have a family worship session each week.


Practices

Baptism

Baptism is considered a requirement for salvation. Baptisms performed by other denominations are not considered valid. Before being baptized, a member will become an unbaptized publisher. Jehovah's Witnesses do not practice infant baptism but allow children to be baptized as long as they meet the same requirements as other candidates. To qualify for baptism, an individual must correctly answer more than a hundred questions about their own lifestyle and the denomination's beliefs. Individuals undergoing baptism are directed to affirm publicly that their dedication and baptism identifies each "as one of Jehovah's Witnesses in association with God's spirit-directed organization," though Witness publications say baptism symbolizes personal dedication to God and not "to a man, work or organization."


Worship

Meetings for worship and study are held at Kingdom Halls, which are typically functional in character, and do not contain religious symbols. Witnesses are assigned to a congregation in whose "territory" they usually reside and attend weekly services they call "meetings", scheduled by congregation elders. The meetings are largely devoted to study of Watch Tower Society literature and the Bible. Jehovah's Witnesses have "considerable worldwide uniformity", as all congregations study the same materials on a schedule. Outsiders are encouraged to attend.


Historically, congregations met three times each week, but since 2009 meet for two sessions each week: one on a weekday and one on a weekend. Jehovah's Witnesses are expected to study the assigned material before attending. Children also attend meetings and do not have separate arrangements such as Sunday School. Gatherings are opened and closed with hymns called Kingdom songs and brief prayers. A Kingdom Hall may have multiple congregations that share the building. In 2014, decisions about which congregations would share a Kingdom Hall or whether additional Kingdom Halls should be built was transferred from individual congregations to the nearest branch office. After this change, many Kingdom Halls were sold.


Twice each year, Jehovah's Witnesses from a number of congregations that form a "circuit" gather for a one-day assembly. Larger groups of congregations meet annually for a three-day "regional convention", usually at an Assembly Hall built for this purpose. Rented stadiums or auditoriums are sometimes used instead. New members are baptized at these conventions. Jehovah's Witnesses consider their most important annual event to be the Memorial, which is observed on the fourteenth day of the Jewish month Nisan during Passover, and members advertise the event to outsiders. Unleavened bread and red wine is passed between attendees, but only those who consider themselves to be anointed partake (often with no one in attendance partaking), and a talk is given about the event's significance.


Evangelism

Jehovah's Witnesses are known for their efforts to spread their beliefs, distributing Watch Tower Society literature. The objective is to start a regular "Bible study" with anyone who is not already a member, with the intention that the student be baptized as a member of the group; members are advised to consider discontinuing Bible study with students who show no interest in becoming members. While Jehovah's Witnesses are well known for visiting people's homes, they have a variety of preaching methods. Literature carts were introduced in 2012, where Jehovah's Witnesses stay in a public place and wait for other people to approach them. Methods usually undertaken by those physically unable to engage in the door-to-door ministry include calling people by phone and writing letters. Jehovah's Witnesses are sometimes confused with Mormon missionaries. Converts as a result of their door-to-door evangelism are rare and happen at a rate comparable with other denominations that practice similar preaching methods.


Jehovah's Witnesses are taught that they are under a biblical command to engage in public preaching and often do so by working in pairs. They are instructed to devote as much time as possible to their ministry, and to submit a monthly "Field Service Report". Those who do not submit reports for six consecutive months are termed "inactive". Children also preach. Until 2023, every active Jehovah's Witness was expected to submit the amount of hours they spent preaching in their monthly field service report. In November 2023, this requirement was modified to only apply to members who have agreed to a specific hour requirement. As of 2025, auxiliary pioneers preach for 30 hours, regular pioneers preach for 50 hours per month; special pioneers preach for 100 hours each month, often in remote or under-represented areas, and receive a stipend to help pay for their living expenses. Other members are only required to indicate they engaged in some form of ministry during the month, along with any Bible studies they conducted. In 2025, Jehovah's Witnesses conducted about 7.6 million Bible studies (including studies conducted with their own children), and approximately 304,500 new members were baptized.


The denomination produces a significant amount of literature as part of its evangelism activities. In 2010, The Watchtower and Awake! were the world's most widely distributed magazines. Jehovah's Witnesses consider their literature to be "spiritual food" and provide it to interested parties for free. The group launched its first website in 1997: watchtower.org. In 2008, it was replaced with jw.org. Their website is often referenced in their evangelism, with its logo appearing in literature displays and outside Kingdom Halls. An increased reliance on electronic media has reduced their printing costs. The denomination archives most of its literature online, although certain entries have been changed after publication. It also offers a streaming service called JW Broadcasting. An animated series aimed at children has been produced called "Become Jehovah's Friend". An application, JW Language, has been designed to facilitate preaching with people who speak different languages. A specialized device for use in areas with limited internet access offers downloaded materials relevant to Jehovah's Witnesses.


Disciplinary action

Jehovah's Witnesses require individuals to be baptized by the denomination in order to be subject to their disciplinary procedures. The denomination does not tolerate dissent over doctrines and practices; members who openly disagree with the group's teachings are expelled, shunned, and condemned as apostates who are "mentally diseased". Some adherents "fade" and stop attending meetings without being formally subjected to the group's disciplinary procedures, although some former members have still experienced shunning through this method.


Members accused of persistent wrongdoing are brought to the attention of the elders who will then evaluate possible consequences. Members that have violated the group's standards—for example, dating a non-member—but not otherwise committed a serious sin may be "marked". Congregation members who are aware of another member's errant behaviour are advised to limit social contact with the marked individual. Elders may decide to form a committee in cases involving serious sin, which may result in the member being reproved or shunned. This process requires three elders to meet with the accused. These cases usually involve sexual misconduct or apostasy. Other serious sins involve accepting blood transfusions (which does not require a judicial committee), smoking, using recreational drugs, divorce (unless a spouse committed adultery), celebration of holidays or birthdays, abortion (which is considered murder), and political activities such as voting in elections. Procedures related to congregational discipline are primarily described in the book, Shepherd the Flock of God, provided only to elders. People who formally leave Jehovah's Witnesses are considered to be disassociated and are also shunned. Jehovah's Witnesses can also be disassociated for accepting a blood transfusion.


The practice of shunning may serve to deter other members from dissident behavior. Shunning also helps maintain a "uniformity of belief". Shunned individuals may experience suicidal ideation and often struggle with feelings of low self esteem, shame, and guilt. Former members may experience significant mental distress as a result of being shunned and some seek reinstatement to keep contact with their friends and family. Former members may also experience ambiguous loss or panic attacks. Expelled individuals may eventually be reinstated to the congregation if deemed repentant by congregation elders, and some seek reinstatement to maintain contact with their friends and family. Reinstatement can be a long process, which may be mentally and emotionally draining. Funerals for expelled members may not be performed at Kingdom Halls.


Baptized children are also subject to the same moral standards and consequences for failing to comply. They are allowed to stay with their families until reaching the age of majority. Jehovah's Witnesses lost state funding as a religious community in Norway because of its shunning policy, with the country concluding that shunning, particularly of children, constitutes psychological violence. Subsequently, the group made some changes to its shunning policy in 2024; individuals may offer "simple greetings" to shunned members instead of completely avoiding them if the individual is not deemed to be an apostate. As of 2024, two elders may have a more informal meeting with a minor who is considered to have committed a "serious sin" along with his or her parents before deciding whether a formal committee meeting is required. Parents are also no longer prohibited from attending judicial committees with minors.


Separateness

Jehovah's Witnesses believe that the Bible condemns mixing religions, on the basis that there can only be one truth from God, and therefore reject interfaith and ecumenical movements. They believe that only Jehovah's Witnesses represent true Christianity and that other denominations do not meet God's requirements; all other Christian denominations (collectively referred to as "Christendom") along with all other religions are considered "false religion". Jehovah's Witnesses are taught that it is vital to remain "separate from the world." Their literature defines the "world" as "the mass of mankind apart from Jehovah's approved servants" and teach that it is morally contaminated and ruled by Satan. Jehovah's Witnesses are taught that association with "worldly" people presents a danger to their faith.


For many years, the Watch Tower Society has stated that secondary education is "spiritually dangerous" as it requires extended association with those outside the group. The denomination has also presented further education as unnecessary in view of their belief that Armageddon is imminent. Trade schools are suggested as an alternative. Adolescent Jehovah's Witnesses are also encouraged to become Bethelites. In 2025, the view of higher education was adjusted, with a statement that, "While there are dangers involved in pursuing certain forms of education, basically, whether to obtain additional education or not is a matter for personal decision," and that, "no Christian—including the elders—should judge a fellow Christian's personal decision on this matter."


Jehovah's Witnesses do not celebrate religious holidays such as Christmas and Easter, nor do they observe birthdays, national holidays, or other celebrations they consider to honor people other than Jesus. They believe that these and many other customs have pagan origins or reflect nationalistic spirit. Members are told that spontaneous giving at other times can help their children to not feel deprived of birthdays or other celebrations. Wedding anniversaries are allowed. Jehovah's Witnesses are not permitted to work in industries associated with the military and refuse national military service, which in some countries may result in their arrest and imprisonment. They also refuse to salute flags or participate in patriotic activities. Adherents see themselves as a worldwide brotherhood that transcends national boundaries and ethnic loyalties.


Jehovah's Witnesses are often viewed as being without agency or brainwashed by the anti-cult movement. Andrew Holden believes that most members who join millenarian movements such as Jehovah's Witnesses have made an informed choice, but that defectors "are seldom allowed a dignified exit", and describes the administration as autocratic. Rodney Stark believes that Jehovah's Witness leaders are "not always very democratic" and that members "are expected to conform to rather strict standards," but adds that "enforcement tends to be very informal, sustained by the close bonds of friendship within the group", and that members see themselves as "part of the power structure rather than subject to it."


After the publication of The Elementary Forms of New Religious Life, academics began to describe various new religious movements as either world-affirming, world-accommodating, or world-rejecting. Jehovah's Witnesses were labelled as world-rejecting. Bryan R. Wilson believed that Jehovah's Witnesses conflict with society at large, impose "tests of merit on would-be members", have strict disciplinary procedures, and expect absolute commitment. Sociologist Ronald Lawson has suggested that the group's intellectual and organizational isolation, coupled with the intense indoctrination of adherents, rigid internal discipline, and considerable persecution, has contributed to the consistency of its sense of urgency in its apocalyptic message.


Alan Rogerson describes the group's leadership as totalitarian, while historian James Irvin Lichti  rejects this interpretation. James A. Beckford classified the group's organizational structure as totalizing, with assertive leadership, specific and narrow objectives, control over competing demands on members' time and energy, and control over the quality of new members. Other characteristics of the classification include likelihood of friction with secular authorities, reluctance to cooperate with other religious organizations, a high rate of membership turnover, a low rate of doctrinal change, and strict uniformity of beliefs among members. Beckford also identified the group's chief characteristics as historicism (identifying historical events as relating to the outworking of God's purpose), absolutism (conviction that Jehovah's Witness leaders dispense absolute truth), activism (capacity to motivate members to perform missionary tasks), rationalism (conviction that Witness doctrines have a rational basis devoid of mystery), authoritarianism (rigid presentation of regulations without the opportunity for criticism) and world indifference (rejection of certain secular requirements and medical treatments).


Former members Heather Botting and Gary Botting compare the cultural paradigms of the denomination to George Orwell's Nineteen Eighty-Four. Critics believe that by disparaging individual decision-making, the group's leaders cultivate a system of unquestioning obedience in which members abrogate responsibility and rights over their personal lives. Critics also accuse the group's leaders of exercising "intellectual dominance" over adherents, controlling information, and creating "mental isolation", which former Governing Body member Raymond Franz argued were all elements of mind control. Some Jehovah's Witnesses describe themselves to academics as "Physically In, Mentally Out" (PIMO); these individuals privately question certain doctrine but remain inside the organization to maintain contact with their friends and family.


Rejection of blood transfusions

Jehovah's Witnesses typically refuse blood transfusions, which they consider a violation of God's law based on their interpretation of Acts 15:28, 29 and other scriptures. This prohibition has existed since 1945. They also do not eat blood-based foods, such as blood sausage. Since 1961, acceptance of a blood transfusion without subsequent repentance has been grounds for expulsion from the group. Members are directed to refuse blood transfusions, even in "a life-or-death situation". Their literature implies that there is a blood alternative for every medical situation and "emphasizes the danger of blood transfusions". Jehovah's Witnesses do not accept the transfusion of "whole blood, packed red cells, platelets, white cells or plasma". Autologous blood donation, where one's blood is stored for later use, is also considered unacceptable. Members may accept some blood plasma fractions at their own discretion. Some Jehovah's Witnesses may accept prohibited blood products if medical confidentiality is upheld, although Jehovah's Witnesses who work in a hospital may break such confidentiality. Jehovah's Witness patients are generally open to non-blood alternative treatments, even if they are less effective.


Courts have intervened in life-threatening situations involving children that require blood transfusions to allow the treatment to take place. Courts may allow mature minors to reject blood transfusions based on their beliefs. The May 22, 1994 issue of Awake! entitled Youths Who Put God First featured children who refused blood transfusions and subsequently died.


The Watch Tower Society provides pre-formatted durable power of attorney documents prohibiting major blood components, in which members can specify which allowable fractions and treatments they will accept. The denomination has established Hospital Liaison Committees as a cooperative arrangement between individual Jehovah's Witness members and medical professionals and hospitals to provide information about bloodless treatment options. Patients who accept certain blood products in the committee's presence are deemed to have disassociated and are shunned. The National Secular Society advocates against hospitals partnering with hospital liaison committees due to medical coercion.


Handling of sexual abuse cases

Jehovah's Witnesses have been accused of having policies and culture that help to conceal cases of sexual abuse within the organization. When investigating cases of child abuse, elders are instructed to immediately call the organization's headquarters or branch office. The group states that this requirement is to ensure compliance with the law. An investigation by the Canadian Broadcasting Corporation determined that elders were asked questions such as, "How many elders believe the victim is to blame or willingly participated in the act?" Jehovah's Witnesses have been criticized for the "two witness rule" for congregational discipline, based on an application of scriptures in Deuteronomy 19:15 and Matthew 18:15–17, which requires sexual abuse to be substantiated by secondary evidence if the accused person denies wrongdoing. In cases where corroboration is lacking, the Watch Tower Society's instruction is that "the elders will leave the matter in Jehovah's hands". A former member has said that the policy effectively requires that there be third-party witness to an act of molestation, "which is an impossibility". Jehovah's Witnesses maintain a database of confidential files in regards to child abuse, which are marked as "Do Not Destroy". An elder in New Zealand was tasked with destroying "personal notes" in their database when the organization was under investigation for child abuse. In the United States, the group was fined four-thousand dollars daily (accruing a total of two million dollars) for delaying an order to provide its documentation.


The group's failure to report abuse allegations to authorities has also been criticized. The Watch Tower Society's policy is that elders inform authorities when required by law to do so, but otherwise leave that up to the victim and their family. In jurisdictions with priest–penitent privilege, confessions of abuse may be considered confidential. William Bowen, a former Jehovah's Witness elder who established the Silentlambs organization to assist sex abuse victims in the denomination, has claimed that Witness leaders discourage followers from reporting incidents of sexual misconduct to authorities. Other critics have alleged that the organization is reluctant to alert authorities to protect its "crime-free" reputation. However, in response to the charge that their policies "protect pedophiles rather than protect the children", the organization has maintained that the best way to protect children is to educate parents; they also say they do not sponsor activities that separate children from parents. In court cases in the United Kingdom and the United States, the Watch Tower Society has been found negligent in its protection of children from known sex offenders within the congregation. The Society has settled other child abuse lawsuits out of court, paying $780,000 in one case. In 2015, the Australian Royal Commission into Institutional Responses to Child Sexual Abuse found that "there was no evidence before the Royal Commission of the Jehovah's Witness organisation having or not having reported to police any of the 1,006 alleged perpetrators of child sexual abuse identified by the organisation since 1950." The Royal Commission also found that the Watch Tower Society legal department routinely provided incorrect information to elders based on an incorrect understanding of what constitutes a legal obligation to report crimes in Australia. In 2017, the Charity Commission for England and Wales began an inquiry into Jehovah's Witnesses' handling of allegations of child sexual abuse in the United Kingdom. In 2021, Jehovah's Witnesses in Australia agreed to join the nation's redress scheme for sexual assault survivors to maintain its charity status there.


Government interactions

Controversy about various beliefs, doctrines and practices of Jehovah's Witnesses has led to opposition from governments, communities, and other religious groups. Religious commentator Ken Jubber wrote, "Viewed globally, this persecution has been so persistent and of such intensity that it would not be inaccurate to regard Jehovah's Witnesses as the most persecuted group of Christians of the twentieth century." Several cases involving Jehovah's Witnesses have been heard by Supreme Courts worldwide. They generally relate to the right to practice their religion, displays of patriotism and military service, and blood transfusions. Cases in their favor have been heard in the United States, Canada and many European countries.


Political and religious animosity toward Jehovah's Witnesses has at times led to mob action and government oppression in various countries. Their political neutrality and refusal to serve in the military has led to imprisonment of members who refused conscription during World War II and other periods of compulsory national service, especially in countries that do not provide religious exemptions. Their religious activities are banned or restricted in some countries, including China, Russia, Vietnam, and many Muslim-majority countries.


Australia

In 1931, the Australian government monitored radio broadcasts of Rutherford's sermons as they had received complaints about anti-Catholic rhetoric. The religious group became especially unpopular after 1940 due to their political neutrality in the second world war, prompting people to write to government officials about the names and addresses of known members. In 1941, Jehovah's Witnesses became an illegal organization. Various groups supported the ban, which caused political pressure to enforce it; Member of Parliament Maurice Blackburn opposed a ban, believing it to be caused by religious intolerance. Once the ban was enacted, the assets of the Watchtower Bible and Tract Society were seized by the government. Witness homes were raided to confiscate their religious literature. Despite these measures, Jehovah's Witnesses continued their activities. The ban was overturned in 1943 when the High Court concluded that these restrictions violated the constitution.


Canada

In the early 1900s, radio stations were operated by congregations in Saskatoon, Edmonton, Vancouver, and Toronto. In 1927, the federal government minister responsible for radio licensing, Arthur Cardin, revoked the licenses for these radio stations because they shared airspace with the Ku Klux Klan in Canada. According to Gary Botting, this "strange alliance" was formed due to a mutual opposition against the Roman Catholic church. In response, Rutherford bought airtime from other radio stations. When Hector Charlesworth banned this activity as well, he was "indirectly attacked" in an issue of the Golden Age and Jehovah's Witnesses launched a petition to regain their licenses that resulted in 406,270 signatures. Charlesworth's actions were debated by the House of Commons in 1933. While multiple members expressed concern that this prohibition was censorship of free speech, the ban was not lifted.


In 1940, a year after Canada entered World War II, the denomination itself was banned under the War Measures Act as a subversive organization. This ban continued until 1943. A separate ban on the Watchtower Bible and Tract Society was not lifted until 1945. More than 100,000 dollars in assets were seized by the Canadian government and tonnes of literature produced by the group were confiscated. Hundreds of adherents were prosecuted as members of an illegal organization. Jehovah's Witnesses were interned in camps along with political dissidents and people of Chinese and Japanese descent. During this period, many Jehovah's Witness children were expelled from school, while others were placed in foster homes or juvenile detention. After the ban was lifted, men who had been jailed tried to apply for an ordained minister exemption without success. This led to a legal case being filed, R. v. Stewart, which ruled that Jehovah's Witnesses were participants in a "commercial undertaking" and did not qualify as ministers. A similar outcome was reached in Greenlees v. A.G. Canada, where the judge decided that Jehovah's Witnesses could not be ministers because they considered every member to be one and that they did not have an organizational structure independent of the Watchtower Bible and Tract Society.


Jehovah's Witnesses faced discrimination in Quebec until the Quiet Revolution, including bans on distributing literature or holding meetings. Roncarelli v Duplessis was a 1959 legal case heard by the Supreme Court of Canada. The court held that in 1946 Maurice Duplessis, Premier and Attorney General of Quebec, had overstepped his authority by ordering the manager of the Liquor Commission to revoke the liquor licence of Frank Roncarelli, a Montreal restaurant owner and Jehovah's Witness who was an outspoken critic of the Roman Catholic Church in Quebec. Roncarelli provided bail for Jehovah's Witnesses arrested for distributing pamphlets attacking the Roman Catholic Church. The Supreme Court found Duplessis liable for $33,000 in damages plus Roncarelli's court costs. Another legal case heard that year was Lamb v Benoit, where a Jehovah's Witness woman was arrested for distributing religious pamphlets.


China

Jehovah's Witnesses are banned in China. Missionaries like Amber Scorah were sent there to preach clandestinely.


Cuba

During the Cuban Revolution and its aftermath, Jehovah's Witnesses became one of the largest religious groups in Cuba. Despite initially cordial relations with the government, led by Fidel Castro, attempts made to suppress religious groups became apparent. In 1962, the Cuban Ministry of Communications  imposed restrictions on the distribution of Jehovah's Witnesses' literature. According to the 1963 Witness Yearbook, arrests for preaching increased in 1962. The following year, foreign missionaries were exiled and hundreds of Jehovah's Witnesses were arrested for holding public and private gatherings without permission from local authorities. In 1965, the Cuban government banned the Cuban Watch Tower Society. This marked the beginning of heightened hostility towards Jehovah's Witnesses, who began to face legal repercussions for refusing to salute the flag or serve in the military when conscripted. Military Units to Aid Production were established by the government to isolate groups deemed to be "deviant" or "undesirable", including Jehovah's Witnesses. Those who refused military conscription were typically sentenced to one to six years in these camps. A 1966 report documented instances of torture and murder of Witnesses by the government agents who worked at these camps. Despite the repercussions, Jehovah's Witnesses remained persistent in their activism. This ultimately prompted the Cuban government to officially ban the sect on July 1, 1974. The 1976 Constitution reaffirmed the ruling.


Eritrea

Religious groups must be registered in order to legally worship in Eritrea. Jehovah's Witnesses, as well as other Christian and Muslim groups, have been refused this legal recognition. Jehovah's Witnesses have been imprisoned for their refusal to perform military service and for attending religious services.


France

Jehovah's Witnesses were officially registered as a religious group in France in 1947. In 1995, they were designated as a "dangerous sect" by French law. In 1999, the country demanded back taxes on donations to the religious group's organization from 1993 and 1996, which would have been €57.5 million. This tax ruling was overturned by the European Court of Human Rights on June 30, 2011.


Germany

In 1933, there were approximately 20,000 Jehovah's Witnesses in Nazi Germany, of whom about 10,000 were imprisoned. Jehovah's Witnesses suffered religious persecution by the Nazis because they refused military service and allegiance to Hitler's National Socialist Party. Of those, 2,000 were sent to Nazi concentration camps, where they were identified by purple triangles; as many as 1,200 died, including 250 who were executed. They were hanged, beheaded, beaten to death, or shot dead. Conditions for Jehovah's Witnesses improved in 1942, when they were increasingly given work details that required little supervision, such as farming, gardening, transportation and unloading goods, while others worked in civilian clothing in a health resort, as housekeepers for Nazi officials, or were given construction and craft tasks at military buildings.


Unlike Jews and Romani, who were persecuted on the basis of their ethnicity, Jehovah's Witnesses could escape persecution and personal harm by signing a document indicating renunciation of their faith, submission to state authority, and support of the German military. Historian Sybil Milton writes, "their courage and defiance in the face of torture and death punctures the myth of a monolithic Nazi state ruling over docile and submissive subjects." Jehovah's Witnesses would preach inside the concentration camps, hold meetings, and smuggle in their religious literature.


Approximately 800 children of Jehovah's Witnesses were taken away from their families. Witness children typically expressed defiance to the Nazi regime's attempts to make them act against their beliefs. They were often expelled from public schools due to their refusal to say "Heil Hitler". Some children were sent to reeducation centers, while others were adopted by families in good standing with the Nazi regime.


In East Germany, from the 1950s to the 1980s, Jehovah's Witnesses were persecuted extensively by the Stasi, which frequently used decomposition methods against them. Jehovah's Witnesses were considered a threat because their beliefs did not conform to socialist standards and their members sometimes had contact with the West.


Greece

Greece had a ban on public evangelism in the 1930s. Approximately 60 Jehovah's Witnesses were imprisoned for violating this law. The case was eventually appealed to the European Court of Human Rights, who ruled in favour of Jehovah's Witnesses in 1993. This decision also benefited other religious groups in the country.


Japan

In Japan, following the publication of Shūkyō nisei-related guidelines, a survey was conducted about child abuse within Jehovah's Witnesses, the results of which were forwarded to the government. Ninety-two percent of 583 respondents reported that they had experienced physical abuse as children. The lawyer's group conducting the survey believed this to be evidence of systemic religious abuse.


Norway

Norway provides state subsidies to religious communities, with some restrictions. Although Jehovah's Witnesses qualified for more than thirty years, they did not receive this funding in Oslo and Viken in 2022. The decision was appealed and upheld by the Ministry of Children and Families. In 2023, Jehovah's Witnesses were fully deregistered as a religious community in Norway as a result of their shunning practice. The Supreme Court ruled that religious communities can determine who can be members but that restrictions on additional funding are acceptable. Therefore, the organization no longer receives 1.3 million euros each year in state subsidies. The denomination's deregistration also means that they lost the right to perform civil marriages. The director of Human Rights Without Frontiers believes that by deregistering Jehovah's Witnesses, Norway is interfering with the group's religious freedom.


Russia

In April 1951, about 9,300 Jehovah's Witnesses in the Soviet Union were deported to Siberia as part of Operation North.


In April 2017, the Supreme Court of Russia labeled Jehovah's Witnesses an extremist organization, banned its activities in Russia, and issued an order to confiscate its assets.


Singapore

During British colonial rule in 1941, Watch Tower Society literature was banned in Singapore due to Jehovah's Witnesses' continued refusal to enlist in the Allied Forces during World War II. The group was officially registered as a society in 1960 under the Societies Ordinance Act of 1890. While freedom of religion is constitutionally protected in post-independence Singapore, Jehovah's Witnesses continue to face restrictions, especially where conscription is concerned. In 1972, the group was deregistered for being "prejudicial to public welfare and order", with their refusal of mandatory military service cited as a key concern. Since then, male adherents who reject enlistment, typically about six individuals each year, are imprisoned under the Enlistment Act 1970 as conscientious objection is not recognized. However, they do not receive permanent criminal records, and are usually assigned tasks such as cooking, gardening and laundry in lieu of military activities for about two years.


South Korea

South Korea did not have a religious exemption for military service until 2018, which led to more than 19,000 Jehovah's Witnesses being imprisoned there.


United States

In the United States, legal challenges by Jehovah's Witnesses prompted a series of state and federal court rulings that reinforced judicial protections for civil liberties. Among the rights strengthened by Witness court victories in the US are the protection of religious conduct from federal and state interference, the right to abstain from patriotic rituals and military service, the right of patients to refuse medical treatment, and the right to engage in public discourse. Authors including William Whalen, Shawn Francis Peters and former members Barbara Grizzuti Harrison, Alan Rogerson, and William Schnell have claimed the arrests and mob violence in the 1930s and 1940s were the consequence of what appeared to be a deliberate course of provocation of authorities and other religious groups by Jehovah's Witnesses. Harrison, Schnell, and Whalen have suggested Rutherford invited and cultivated opposition for publicity purposes in a bid to attract dispossessed members of society, and to convince members that persecution by the outside world was evidence of the truth of their struggle to serve God.


In 1943, the Supreme Court ruled in West Virginia State Board of Education v. Barnette that requiring students to salute the flag was a violation of their First Amendment rights.


See also

Explanatory notes

References

Sources

External links




Psychedelic scenes


Psychedelic film


Salvia divinorum  (Latin: sage of the diviners; also called ska maría pastora, seer's sage, yerba de la pastora, magic mint or simply salvia) is a species of plant in the sage genus Salvia, known for its transient psychoactive properties when its leaves, or extracts made from the leaves, are administered by smoking, chewing, or drinking (as a tea). The leaves contain the potent compound salvinorin A and can induce a dissociative state and hallucinations.


Mazatec shamans have a long and continuous tradition of religious use of S. divinorum to facilitate visionary states of consciousness during spiritual healing sessions. A media panic in the Western world, especially in the United States c. 2007, centered on reports of video sharing of drug use on the internet, legal teenage use of the drug, as well as a teenage suicide in Delaware, despite it being "unclear" what role the drug played in the incident. S. divinorum is legal in some countries, including the U.S. at the federal level; however over half of U.S. states have passed laws criminalizing it.


Its native habitat is cloud forest in the isolated Sierra Mazateca of Oaxaca, Mexico, where it grows in shady, moist locations. The plant grows to over a meter high, has hollow square stems like others in the mint family Lamiaceae, large leaves, and occasional white flowers with violet calyxes. Botanists have not determined whether S. divinorum is a cultigen or a hybrid because native plants reproduce vegetatively and rarely produce viable seed.


Because the plant has not been well-studied in high-quality clinical research, little is known about its toxicology, adverse effects, or safety over long-term consumption. Its chief active psychoactive constituent is a structurally unique diterpenoid called salvinorin A, a potent κ-opioid agonist. Although not thoroughly assessed, preliminary research indicates S. divinorum may have low toxicity (high LD50). Its effects are rapid but short-lived.


Etymology

The genus name, Salvia, was first used by Pliny for a plant that was likely Salvia officinalis (common sage) and is derived from the Latin salvere. The specific epithet, divinorum, was given because of the plant's traditional use in divination. It is often loosely translated as "diviner's sage" or "seer's sage". Albert Hofmann, who collected the first plants with Gordon Wasson, objected to the new plant being given the name divinorum: "I was not very happy with the name because Salvia divinorum means 'Salvia of the ghosts,' whereas Salvia divinatorum, the correct name, means 'Salvia of the priests'." It is now in the botanical literature under the name Salvia divinorum due to priority rules.


Common names

There are many common names for S. divinorum, including sage of the diviners, ska maría pastora, seer's sage, yerba de la pastora, simply salvia, and colloquially sally-d and magic mint.


History

Salvia divinorum is native to the Sierra Mazateca in Oaxaca, Mexico, where it is still used by the Mazatec, primarily to facilitate shamanic visions in the context of curing or divination. S. divinorum is one of several plant species with hallucinogenic properties that are ritually used by Mazatec shamans. In their rituals, the shamans use only freshly harvested S. divinorum leaves. They see the plant as an incarnation of the Virgin Mary, and begin the ritual with an invocation to Mary, Saint Peter, the Holy Trinity, and other saints. Ritual use traditionally involves being in a quiet place after ingestion of the leaf—the Maztec shamans say that "La Maria (S. divinorum) speaks with a quiet voice."


It is also used in smaller amounts, as a diuretic, and to treat ailments including diarrhea, anemia, headaches, rheumatism, and a semi-magical disease known as panzón de borrego, or a swollen belly (literally, "lamb belly").


The history of the plant is not well known, and there has been no definitive answer to the question of its origin. Speculation includes Salvia divinorum being a wild plant native to the area; a cultigen of the Mazatecs; or a cultigen introduced by another Indigenous group. Botanists have also not been able to determine whether it is a hybrid or a cultigen.


Academic discovery

Salvia divinorum was first recorded in print by Jean Basset Johnson in 1939 while he was studying Mazatec shamanism. He later documented its use and reported its effects through personal testimonials of users. It was not until 2002 that Bryan Roth and his team identified the psychoactive mechanism.


Gordon Wasson tentatively postulated that the plant could be the mythological pipiltzintzintli, the "Noble Prince" of the Aztec codices. Wasson's speculation has been the subject of further debate amongst ethnobotanists, with some scepticism coming from Leander J. Valdés, and counterpoints more supportive of Wasson's theory from Jonathan Ott.


The identity of another mysterious Aztec entheogen, namely that of poyomatli, has also been suggested as being Salvia divinorum.
Here too there are other candidate plants, notably cacahuaxochitl (Quararibea funebris),


Botany

Salvia divinorum has large green ovate (often also dentate) leaves, with a yellow undertone that reach 10 to 30 cm (4 to 12 in) long. The leaves have no hairs on either surface, and little or no petiole. The plant grows to well over 1 metre (3 ft) in height, on hollow square stems which tend to break or trail on the ground, with the plant rooting quite readily at the nodes and internodes.


The flowers, which bloom only rarely, grow in whorls on a 30-centimetre (12 in) inflorescence, with about six flowers to each whorl. The .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}3-centimetre (1+1⁄4 in) flowers are white, curved and covered with hairs, and held in a small violet calyx that is covered in hairs and glands. When it does bloom in its native habitat, it does so from September to May.


Early authors erred in describing the flowers as having blue corollas, based on Epling and Játiva's description. The first plant material they received was dried, so they based the flower color on an erroneous description by Hofmann and Wasson, who didn't realize that their "blue flowers, crowned with a white dome" were in fact violet calyces with unopened white corollas.


Distribution and habitat

Salvia divinorum is endemic to the Sierra Mazateca in the state of Oaxaca in Mexico, growing in the primary or secondary cloud forest and tropical evergreen forest at elevations from 300 to 1,830 metres (980 to 6,000 ft). Its most common habitat is black soil along stream banks where small trees and bushes provide an environment of low light and high humidity.


Reproduction

Salvia divinorum produces few viable seeds even when it does flower—no seeds have ever been observed on plants in the wild. For an unknown reason, pollen fertility is also comparatively reduced. There is no active pollen tube inhibition within the style, but some event or process after the pollen tube reaches the ovary is aberrant. The likeliest explanations are inbreeding depression or hybridity, although the latter was rejected in 2010. All of the Mazatec populations appear to be clonal. The plant's square stems break easily and tend to trail on the ground, rooting easily at the nodes and internodes.


Taxonomy

Salvia divinorum was first documented in 1939; yet, it was many years before botanists could identify the plant due to Mazatec secrecy about the growing sites. Flowers were needed for a definitive identification of the species. In 1962, the Swiss chemist Albert Hofmann and ethnomycologist R. Gordon Wasson traveled throughout the Sierra Mazateca researching Mazatec rituals while looking for specimens of the plant. They were unable to locate live plants. Eventually, the Mazatec provided them some flowering specimens. These specimens were sent to botanists Carl Epling and Carlos D. Játiva, who described and named the plant as Salvia divinorum, in reference to its use in divination and healing by the Mazatec. By 1985, up to fifteen populations of the plant had been located.


Until 2010, there were differing opinions as to whether Salvia divinorum is an interspecific hybrid. The plant's partial sterility was suggestive of a hybrid origin, though no two parent species have been found with an obvious affinity to Salvia divinorum. One other possibility for the plant's partial sterility is that long-term cultivation and selection have produced an inbred cultigen.


In 2010, a molecular phylogenetic approach by DNA sequencing of Salvia divinorum and a number of related species suggested that the species is not a hybrid. One earlier proposed parent was Salvia cyanea (a synonym for Salvia concolor), which Epling and Játiva believed to be closely allied to Salvia divinorum. The 2010 study demonstrated Salvia divinorum's closest relative to be Salvia venulosa—a rare and endemic Salvia that is native to Colombia, growing in shaded, wooded gullies at 1,500 to 2,000 m (4,900 to 6,600 ft) elevation. It also showed that Salvia divinorum does not belong to the Salvia section Dusenostachys, as believed earlier. The genetic study also indicated that Salvia venulosa was likely misplaced into Salvia section Tubiflorae, and that it may not be related to other Colombian Salvia species, though further tests are needed. A 2013 follow-up analysis of more Salvia species reported the same result.


The origin of Salvia divinorum was still a mystery as of 1993, one of only three plants in the extensive genus Salvia (approximately 900 species) with unknown origins—the other two being Salvia tingitana and Salvia buchananii.


Strains

There are two commonly cultivated strains which are known to be distinct. One is the strain that was collected in 1962 by ecologist and psychologist Sterling Bunnell (the Bunnell strain), colloquially mis-attributed as the Wasson-Hofmann strain. The other was collected from Huautla de Jiménez in 1991 by anthropologist Bret Blosser (the Blosser or Palatable strain). There are other strains that are not as well documented, such as the Luna strain (possibly Bunnell) isolated from a Hawaiian patch of Salvia divinorum clones, featuring unusually serrated and rounded rather than ovate leaves.


Cultivation

Propagation by cuttings

Salvia divinorum is usually propagated through vegetative reproduction. Small cuttings, between 5 and 20 cm (2 and 8 in) long, cut off of the mother plant just below a node, will usually root in plain tap water within two or three weeks.


Flowering

Blooms occur when the day length becomes shorter than 12 hours (beginning in mid-October in some places), necessitating the use of shade cloth in urban environments with exposure to light pollution caused by HPS lighting. Both Siebert and hobbyist  reports indicate that viable seeds can, on occasion, be produced under cultivation, following hand-pollination or hummingbird pollination.


Chemistry

The known active constituent of Salvia divinorum is a trans-neoclerodane diterpenoid known as salvinorin A (chemical formula C23H28O8). This compound is present in the dried plant at about 0.18%.


Salvinorin A is not an alkaloid (meaning it does not contain a basic nitrogen), unlike most known opioid receptor ligands. Salvinorin A is the first documented diterpene hallucinogen.


Similar to many psychoactive herbs, Salvia divinorum synthesizes and excretes its active constituent (salvinorin A) via trichomes, of the peltate-glandular morphology, located just beneath the cuticle (subcuticular) layer.


Potency

By mass, salvinorin A "is the most potent naturally occurring hallucinogen." It is active at doses as low as 200 μg. Synthetic chemicals, such as LSD (active at 20–30 μg doses), can be more potent. Research has shown that salvinorin A is a potent and selective κ-opioid (kappa-opioid) receptor agonist. It has been reported that the effects of salvinorin A in mice are blocked by κ-opioid receptor antagonists. However, it is an even more potent D2 receptor partial agonist, and it is likely this action plays a significant role in its effects as well. Salvinorin A has no actions at the 5-HT2A serotonin receptor, the principal molecular target responsible for the actions of 'classic' hallucinogens, such as mescaline and LSD, nor is it known to have affinity for any other sites to date.


In experiments, salvinorin A has shown little toxicity. Rodents chronically exposed to levels many times greater than those to which humans expose themselves show no signs of organ damage.


Other terpenoids

Other terpenoids have been isolated from Salvia divinorum, including other salvinorins and related compounds named divinatorins and salvinicins. None of these compounds has shown significant (sub-micromolar) affinity at the κ-opioid receptor, and there is no evidence that they contribute to the plant's psychoactivity.


Other pharmaceutical action

Salvinorin A is capable of inhibiting excess intestinal motility (e.g. diarrhea), through a combination of κ-opioid and cannabinoid (mainly CB1 receptor) receptors in inflamed but not normal gut in vivo. The mechanism of action for Salvinorin A on ileal tissue has been described as 'prejunctional', as it was able to modify electrically induced contractions, but not those of exogenous acetylcholine. Results from a study at the University of Iowa indicate that it may have potential as an analgesic and as a therapeutic tool for treating drug addictions.


A pharmacologically important aspect of the contraction-reducing (antispasmodic) properties of ingested Salvinorin A on gut tissue is that it is only pharmacologically active on inflamed and not normal tissue, thus reducing possible side-effects.


Ingestion

There are a few ways to consume Salvia divinorum. In traditional Mazatec ritual, shamans use only fresh Salvia divinorum leaves. Modern methods have been developed to more effectively absorb the active principle, salvinorin A. If enough salvinorin A is absorbed, an altered state of consciousness can occur. The duration of experience varies with the method of ingestion and the amount of salvinorin A absorbed.


Traditional methods

Mazatec shamans crush the leaves to extract leaf juices from about 20 to 80 or more fresh leaves (i.e. about 50–200 g or 2–7 oz). They usually mix these juices with water to create an infusion or 'tea' which they drink to induce visions in ritual healing ceremonies.


Chewing and swallowing a large number of fresh leaves is the other Mazatec method. Oral consumption of the leaf makes the effects come on more slowly, over a period of 10 to 20 minutes. The experience, from the onset of effects, lasts from about 30 minutes up to one and a half hours.


Doses for chewing vastly exceed doses used for smoking. By calculating the concentrations per leaf ("an average concentration of 2.45 mg per gram" of leaf), the average weight per leaf ("about 50 g" per 20 leaves, or 2.5 g/leaf), and the standard dose for chewing (about 8–28 leaves), the doses can range from about 50 mg to 172 mg.


Modern methods

Modern methods of ingestion include smoking or chewing the leaf, or using a tincture, as described in the following sections.


Salvia divinorum is becoming more widely known and used in modern culture. The National Survey on Drug Use and Health, an annual US based survey sponsored by the Substance Abuse and Mental Health Services Administration (SAMHSA), for 2006 estimated that about 1.8 million persons aged 12 or older had used Salvia divinorum in their lifetime, of which approximately 750,000 had done so in that year. The following year, 2007, saw the annual figure rise from 750,000 to 1 million US users.


Smoking

Dry leaves can be smoked in a pipe, or through the use of a water pipe to cool the smoke. The temperature required to release salvinorin from the plant material is quite high (about 240 °C). A cooler flame will work, but the direct application of a more intense flame, such as that of a torch lighter, is often preferred.


Some find that untreated dry leaf produces unnoticeable or only light effects. Concentrated preparations or extracts which may be smoked in place of untreated leaves, have become widely available. This enhanced (or "fortified") leaf is described by a number followed by an x (e.g. 5x, 10x), the multiplicative factors being generally indicative of the relative amounts of leaf concentrate, though there is no accepted standard for these claims. Other sources may use a system of color codes to form their own standards of potency; for example, "green", "yellow", and "red."


These grades of potency may be roughly indicative of the relative concentration of the active principle, (salvinorin A), but the measure should not be taken as absolute. Overall extract potency will depend on the (naturally varying) strength of the untreated leaf used in preparing the extract, as well as the efficiency of the extraction process itself. Extracts reduce the overall amount of inhalations needed to ingest a given amount of active principle, thus facilitating more powerful experiences.


If salvia is smoked, then the main effects are experienced quickly. The most intense 'peak' is reached within a minute or so and lasts for 1–5 minutes, followed by a gradual tapering off. At 5–10 minutes, less intense yet still noticeable effects typically persist, giving way to a returning sense of the everyday and familiar until back to baseline after about 15 to 20 minutes.


Quid chewing

The traditional method of chewing the leaves has continued in modern use. However, salvinorin A is generally considered to be inactive when orally ingested, as salvinorin A is effectively deactivated by the gastrointestinal system. Therefore, in what's understood to be a modern innovation, the 'quid' of leaves is held in the mouth as long as possible in order to facilitate absorption of the active constituents through the oral mucosa. 'Quid' refers to the fact that at the end of this method the user spits out the leaves rather than swallowing them because ingesting the leaves has no known effect. Chewing consumes more of the plant than smoking, and produces a longer-lasting experience.


Using a tincture

Less commonly, some may ingest salvia in the form of a tincture. This is administered sublingually, usually with the aid of a glass dropper. It may be taken diluted with water just before use, which may slightly reduce the intensity of its effects, but can also serve to lessen or avoid a stinging sensation in the mouth caused by the presence of alcohol. Tinctures vary in potency, and the effects can range from inducing a mild meditative state to bringing about a more intense hallucinatory one.


When taken as a tincture the effects and duration are similar to other methods of oral ingestion, though they may be significantly more intense, depending on extract potency.


Effects

Aside from individual reported experiences, there has been a limited amount of published work summarizing salvia divinorum's effects. A survey of salvia users found that 38% described the effects as unique in comparison to other methods of altering consciousness. 23% said the effects were like yoga, meditation, or trance. Users have written prose about their experiences; some describing their visions pictorially, and there exist examples of 'salvia-inspired' visionary art. Others claim musical inspiration from the plant.


Near-death experience

A 2019 large-scale study found that ketamine, Salvia divinorum, and DMT (and other classical psychedelic substances) are linked to near-death experiences.


Vaporization

Ethnobotanist Daniel Siebert cautions that inhaling hot air can be irritating and potentially damaging to the lungs. Vapor produced by a heat gun needs to be cooled by running it through a water pipe or cooling chamber before inhalation.


Research (US)

In 2007 an ABC news report said excitement over research into salvia "could vanish overnight if the federal government criminalized the sale or possession of salvia, as the Drug Enforcement Agency  is considering doing right now." Scientists worry that such legislation would restrict further work.


Controversy

The relatively recent emergence of Salvia divinorum in modern Western culture, in comparison to its long continuing traditions of Indigenous use, contrasts widely differing attitudes on the subject.


Opinions range from veneration of the plant as a spiritual sacrament or "a gift from the gods" to a "threat to society," to be banned as quickly as possible in order to "spare countless families the horror of losing a loved one to the relentless tentacles of drug abuse".


Media coverage

Interest in Salvia divinorum escalated in the news media in the late 2000s, particularly in the United States, where an increasing number of newspaper reports have been published and television news stories broadcast.


These stories generally raise alarms over salvia's legal status, for example comparing it to LSD, or describing it as "the new pot", with parental concerns being raised by particular focus on salvia's use by younger teens.


Story headlines may also include 'danger' keywords, such as "Dangerous Herb is Legal..." or "Deadly Dangers Of A Street Legal High".


Mainstream news coverage and journalistic opinion has widely been negative on the subject. In a local news report aired on ABC affiliate WJLA in Washington, DC on July 11, 2007, the anchors are seen to exchange expressions of incredulity when referring to a salvia story with the following introduction "Now, an exclusive I-Team investigation of a hallucinogenic drug that has begun to sweep the nation. What might amaze you is that right now the federal government is doing nothing to stop it."


In March 2008, a Texas news report aired with the story "A legal drug that teenagers are now using to get high could soon be banned here in San Antonio - all because of a Fox News 4 investigation," going on to say, "The drug is legal in Texas, at least for now. But a News 4 investigation could lead to a new ordinance to protect your kids."


Many salvia media stories headline with comparisons to LSD. However, while LSD and salvinorin A may have comparative potencies, in the sense that both can produce their effects at low dosages, they are otherwise quite different. The two substances are not chemically similar or related, as salvinorin A is found naturally in a single plant while LSD is chemically semisynthesized from lysergamides like ergotamine. They are ingested in different ways and produce different effects, which manifest themselves over different timescales. For example, the effects of salvia when smoked typically last for only a few minutes as compared to LSD, whose effects can persist for 8 to 12 hours.


Brett's law

A particular focus of many US media stories is the long-running coverage of the case of Brett Chidester, a 17-year-old Delaware student who committed suicide in January 2006 by carbon monoxide poisoning.


Reportedly, some months before this, Brett's mother Kathleen Chidester had learned about his salvia use and questioned him about it. Brett said that he had ceased his experimentation, but his parents did not believe that he was telling the truth. They have instead argued that salvia-induced depression was largely to blame for his death. Some of Brett's earlier writings about his salvia experiences have been used to suggest that it made him think "existence in general is pointless." Some media stories have referred to these earlier written experience reports as if they were part of Brett's suicide note. A law was soon passed in Delaware classifying the herb as a Schedule I controlled substance in that state. This legislation was nicknamed Brett's law (formally referred to as Senate bill 259).


Although the Chidester story has been given continued exposure by US media, there has not been anywhere else, either before or since this controversial incident, any other reported cases involving or alleging Salvia divinorum as a serious factor in suicide, overdose, accidental, or any other kind of death. Regarding this, San Francisco attorney Alex Coolman has commented, "It's remarkable that Chidester's parents, and only Chidester's parents, continue to be cited over and over again by the mainstream media in their coverage of the supposed 'controversy' over the risks of Salvia divinorum."


Kathleen Chidester has continued campaigning for "Schedule I"-like legislation beyond their home state of Delaware. For example, three years after Brett's death, in written testimony in support of Senator Richard Colburn's proposed Senate Bill to the Maryland State Legislature, saying, "My hope and goal is to have salvia regulated across the US. It's my son's legacy and I will not end my fight until this happens."


Usage shown on YouTube

In 2007, videos were shared on YouTube of alleged salvia users laughing uncontrollably, apparently unable to perform simple tasks or to communicate. In an interview published in the San Francisco Chronicle in June 2007, Daniel Siebert said that the videos "make salvia look like some horrible drug that makes people nuts and dangerous ..." and that it stops people from realizing "there are sensible ways to use something like this."


Waco Representative Charles Anderson (R), who is sponsoring one of several bills to ban salvia in Texas, told colleagues at a legislative hearing about a video that depicts a salvia user behind the wheel of a car. "What we really worry about, is youngsters doing this and then getting in a vehicle or getting on a motorcycle or jumping in a pool somewhere."


Michigan Representative Michael Sak (D) submitted a bill which proposed Schedule I classification of Salvia divinorum and salvinorin A. He said that if people had questions about the deleterious effects of salvia, they should go on YouTube to watch the videos. A reporter questioned Sak as to whether he had ever seen a "Girls Gone Wild" video, and whether that would incite him to make alcohol illegal (Sak replied that he hadn't yet had a chance to review the material).


Nebraska Senator Vickie McDonald responded with "Anytime anything's on YouTube it's an issue," and "Legislators, parents, grandparents, we need to be on top of these things,"  McDonald proposed Schedule I listing Salvia divinorum as part of their Controlled Substances Act, under which possession of salvia would have been considered a Class IV felony with a penalty of up to five years and trafficking would have fallen under a Class III felony with up to a 20 year penalty.


In Massachusetts, YouTube videos were shown by a retired police officer to public health and judiciary committees as evidence in favor of outlawing it there.


The issue has been raised of whether the salvia videos are in breach of YouTube's own community guidelines, which ask users not to "cross the line" and post videos showing "bad stuff" like "drug abuse". The question is considered as particularly problematical as the videos may be something of an enforcement grey area.


Legal status

Many countries control Salvia divinorum in some manner. As of 2015, it is illegal in Australia, Belgium, parts of Canada, Croatia, Czech Republic, Denmark, Germany, Hong Kong, Italy, Japan, Latvia, Lithuania, Poland, Portugal, Republic of Ireland, Romania, South Korea, Sweden, and Switzerland. It is legal to possess and grow in Chile, France and Spain, but not to sell. In Russia, it is legal to possess, but not grow or sell. Estonia, Finland, Iceland, and Norway treat it as a medicinal herb that requires a prescription.


The prohibitive degree of Salvia divinorum legislation varies widely from country to country. Australia has imposed its strictest 'schedule 9' (US Schedule I equivalent), and Italy has also placed salvia in its 'Table I' of controlled substances (also US Schedule I equivalent). In Spain, there are controls focusing only on the commercial trade of Salvia divinorum, personal cultivation (i.e. for non-commercial use) is not targeted.


Legislation may prove difficult to enforce. The plant has a nondescript appearance; unlike many other drug plants, the leaves are non-descript, and without a detectable odour. Salvia divinorum can be grown as an ordinary houseplant without the need of special equipment such as hydroponics or high-power lights.


United Kingdom

In the United Kingdom, following a local newspaper story in October 2005, a parliamentary Early Day Motion was raised calling for Salvia divinorum to be banned there. However, it only received 11 signatures. A second motion raised in October 2008 attracted 18 signatures, and it was reported that Mann had also written to Jacqui Smith, then Home Secretary. The Advisory Council on the Misuse of Drugs, the independent body that advises UK government on drugs, was asked to investigate further.


On the 28 January 2016, the Psychoactive Substances Act 2016 was passed. The act came into force on 26 May 2016, across the entire United Kingdom, making S. divinorum illegal to possess with intent to supply, possess on custodial premises, import for human consumption, or produce for human consumption. The two sponsors for the bill were Conservative House of Lords member Michael Bates and Conservative MP Theresa May.


Australia

Salvia divinorum is considered a Schedule 9 prohibited substance in Australia under the Poisons Standard. Under the Standard, schedule 9 prohibited substances are defined as "Substances which may be abused or misused, the manufacture, possession, supply or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of Commonwealth and/or State or Territory Health Authorities."


United States

National legislation for amendment of the Controlled Substances Act to place salvinorin A and Salvia divinorum in Schedule I at the federal level in the United States was proposed in 2002 by Representative Joe Baca (D- California). Those opposed to bill HR 5607 include Daniel Siebert, who sent a letter to Congress arguing against the proposed legislation, and the Center for Cognitive Liberty and Ethics (CCLE), who sent key members of the US Congress a report on Salvia divinorum and its active principle, along with letters from an array of scientists who expressed concern that scheduling Salvia divinorum would negatively impact important research on the plant. The bill did not pass.


Although salvia is not regulated under the Controlled Substances Act, as of 2009, it had been made illegal in 13 states.  Delaware banned it after salvia use was reported to have played a role in the suicide of a teenager.  Alabama, Delaware, Illinois, Louisiana, Michigan, Missouri, Ohio, Texas, and other states have passed their own laws. Several other states have proposed legislation against salvia, including Alaska, California, Florida, Iowa, Maryland, New Jersey, New York, Oregon, and Pennsylvania. Many of these proposals have not made it into law, with motions having failed, stalled or otherwise died, for example at committee review stages.


Where individual state legislation does exist, it varies from state to state in its prohibitive degree.


Legal consequences may also exist even in states without bans on salvia in particular. Christian Firoz, a Nebraska store owner, was charged for selling salvia, but not under the auspices of any specific law against Salvia divinorum. Firoz was instead charged under a general Nebraskan statute which makes it illegal to sell a product to induce an intoxicated condition. Firoz was found not guilty.. See also the legal status of salvia in North Dakota and Nebraska.


Salvia divinorum has been banned by various branches of the U.S. military and some military bases.


Internet sale

Some internet vendors will not sell live salvia cuttings, leaf, or leaf products to states where its use is restricted or prohibited. Per their drugs and drug paraphernelia policy, eBay does not permit sale of Salvia divinorum or derived products (despite legality in most areas).


Opinions and arguments

Concerns expressed by some politicians on the subject of salvia reflect those of the media, with comparisons to LSD and particular focus on "protecting our children" being echoed; and with legislative proposals following soon after news stories breaking.


Some arguments against salvia have been of a preventative nature, "We need to stop this before it gets to be a huge problem not after it gets to be a huge problem," or of an imitative nature, "The Australians have clearly found a problem with it. There's obviously a risk in people taking it." Concerns about driving while under the influence of salvia have also been expressed.


Opponents of more prohibitive measures against salvia argue that such reactions are largely due to an inherent prejudice and a particular cultural bias rather than any actual balance of evidence, pointing out inconsistencies in attitudes toward other more toxic and addictive drugs such as alcohol and nicotine. While not objecting to some form of legal control, in particular with regard to the sale to minors or sale of enhanced high-strength extracts, most salvia proponents otherwise argue against stricter legislation.


Those advocating consideration of Salvia divinorum's potential for beneficial use in a modern context argue that more could be learned from Mazatec culture, where salvia is not really associated with notions of drug taking at all and it is rather considered as a spiritual sacrament. In light of this it is argued that Salvia divinorum could be better understood more positively as an entheogen rather than pejoratively as a hallucinogen.


Public opinion

Despite its growing notoriety in some circles, media stories generally suggest that the public at large are still mostly unaware of salvia, with the majority perhaps having never even heard of it.


Although published responses may not necessarily be representative of public opinion as a whole, some news agencies generally support reader and viewer feedback in connection with their stories.


References

Citations

Notes

News references

UK

U.S.

External links




A wildfire, forest fire, or a bushfire is an unplanned and uncontrolled fire in an area of combustible vegetation. Some natural forest ecosystems depend on wildfire. Modern forest management often engages in prescribed burns to mitigate fire risk and promote natural forest cycles. However, controlled burns can turn into wildfires by mistake.


Wildfires can be classified by cause of ignition, physical properties, combustible material present, and the effect of weather on the fire. Wildfire severity results from a combination of factors such as available fuels, physical setting, and weather. Climatic cycles with wet periods that create substantial fuels, followed by drought and heat, often precede severe wildfires. These cycles have been intensified by climate change,: 247  and can be exacerbated by curtailment of mitigation measures (such as budget or equipment funding), or sheer enormity of the event.


Wildfires are a common type of disaster in some regions, including Siberia (Russia); California, Washington, Oregon, Texas, Florida (United States); British Columbia (Canada); and Australia. Areas with Mediterranean climates or in the taiga biome are particularly susceptible. Wildfires can severely impact humans and their settlements. Effects include for example the direct health impacts of smoke and fire, as well as destruction of property (especially in wildland–urban interfaces), and economic losses. There is also the potential for contamination of water and soil.


At a global level, human practices have made the impacts of wildfire worse, with a doubling in land area burned by wildfires compared to natural levels.: 247  Humans have impacted wildfire through climate change (e.g. more intense heat waves and droughts), land-use change, and wildfire suppression.: 247  The carbon released from wildfires can add to carbon dioxide concentrations in the atmosphere and thus contribute to the greenhouse effect. This creates a climate change feedback.: 20 


Naturally occurring wildfires can have beneficial effects on those ecosystems that have evolved with fire. In fact, many plant species depend on the effects of fire for growth and reproduction.


Ignition

The ignition of a fire takes place through either natural causes or human activity (deliberate or not).

Natural causes

Natural occurrences that can ignite wildfires without the involvement of humans include lightning, volcanic eruptions, sparks from rock falls, and spontaneous combustions.


Human activity

Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming. In the tropics, farmers often practice the slash-and-burn method of clearing fields during the dry season.


In middle latitudes, the most common human causes of wildfires are equipment generating sparks (chainsaws, grinders, mowers, etc.), overhead power lines, and arson.


Arson may account for over 20% of human caused fires, although human activities, including campfires, power line failures, and equipment use, are responsible for approximately 85% of wildfires. The combination of these ignition sources with dry conditions leads to more frequent and severe fires.  However, in the 2019–20 Australian bushfire season "an independent study found online bots and trolls exaggerating the role of arson in the fires." In the 2023 Canadian wildfires false claims of arson gained traction on social media; however, arson is generally not the main cause of wildfires in Canada. In California, generally 6–10% of wildfires annually are arson.


Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.


Spread

The spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:


Physical properties

Wildfires occur when all the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation that is subjected to enough heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are needed to evaporate any water in the material and heat the material to its fire point.


Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, often as a consequence of droughts, plants dry out and are therefore more flammable.


A wildfire front is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of 100 °C (212 °F). Next, the pyrolysis of wood at 230 °C (450 °F) releases flammable gases. Finally, wood can smolder at 380 °C (720 °F) or, when heated sufficiently, ignite at 590 °C (1,000 °F). Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to 800 °C (1,500 °F), which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or torching: the drying of tree canopies and their subsequent ignition from below.


 Wildfires have a rapid forward rate of spread (FROS) when burning through dense uninterrupted fuels. They can move as fast as 10.8 kilometres per hour (6.7 mph) in forests and 22 kilometres per hour (14 mph) in grasslands. Wildfires can advance tangentially to the main front to form a flanking front, or burn in the opposite direction of the main front by backing. They may also spread by jumping or spotting as winds and vertical convection columns carry firebrands (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels around a wildfire are especially vulnerable to ignition from firebrands. Spotting can create spot fires as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as 20 kilometres (12 mi) from the fire front.


Especially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than 80 kilometres per hour (50 mph). Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.


Intensity variations during day and night

Intensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour fire day that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.


Climate change effects

Increasing risks due to climate change

Climate change promotes the type of weather that makes wildfires more likely. In some areas, an increase of wildfires has been attributed directly to climate change.: 247  Evidence from Earth's past also shows more fire in warmer periods. Climate change increases potential evapotranspiration. This can cause vegetation and soils to dry out when potential evaporation exceeds precipitation and available moisture from the given ecosystem. The vapor pressure deficit also contributes to increasing wildfire risk and has been worsening in the warming climate. When a fire starts in an area with very dry vegetation, it can spread rapidly. Higher temperatures can also lengthen the fire season. This is the time of year in which severe wildfires are most likely, particularly in regions where snow is disappearing.


Weather conditions are raising the risks of wildfires. But the total area burnt by wildfires has decreased. This is mostly because savanna has been converted to cropland, so there are fewer trees to burn.


Climate variability including heat waves, droughts, and El Niño, and regional weather patterns, such as high-pressure ridges, can increase the risk and alter the behavior of wildfires dramatically. Years of high precipitation can produce rapid vegetation growth, which when followed by warmer periods can encourage more widespread fires and longer fire seasons. High temperatures dry out the fuel loads and make them more flammable, increasing tree mortality and posing significant risks to global forest health. Since the mid-1980s, in the Western US, earlier snowmelt and associated warming have also been associated with an increase in length and severity of the wildfire season, or the most fire-prone time of the year. A 2019 study indicates that the increase in fire risk in California may be partially attributable to human-induced climate change.


In the summer of 1974–1975 (southern hemisphere), Australia suffered its worst recorded wildfire, when 15% of Australia's land mass suffered "extensive fire damage". Fires that summer burned up an estimated 117 million hectares (290 million acres; 1,170,000 square kilometres; 450,000 square miles). In Australia, the annual number of hot days (above 35 °C or 95 °F) and very hot days (above 40 °C or 104 °F) has increased significantly in many areas of the country since 1950. The country has always had bushfires but in 2019, the extent and ferocity of these fires increased dramatically. For the first time catastrophic bushfire conditions were declared for Greater Sydney. New South Wales and Queensland declared a state of emergency but fires were also burning in South Australia and Western Australia.


In 2019, extreme heat and dryness caused massive wildfires in Siberia, Alaska, Canary Islands, Australia, and in the Amazon rainforest. The fires in the latter were caused mainly by illegal logging. The smoke from the fires expanded over a huge territory including major cities, dramatically reducing air quality.



As of August 2020, the wildfires in that year were 13% worse than in 2019 due primarily to climate change, deforestation and agricultural burning. The Amazon rainforest's existence is threatened by fires. Record-breaking wildfires in 2021 occurred in Turkey, Greece and Russia, thought to be linked to climate change.

Carbon dioxide and other emissions from fires

The carbon released from wildfires can add to greenhouse gas concentrations. Climate models do not yet fully reflect this feedback.: 20 



Wildfires release large amounts of carbon dioxide, black and brown carbon particles, and ozone precursors such as volatile organic compounds and nitrogen oxides (NOx) into the atmosphere. These emissions affect radiation, clouds, and climate on regional and even global scales. Wildfires also emit substantial amounts of semi-volatile organic species that can partition from the gas phase to form secondary organic aerosol (SOA) over hours to days after emission. In addition, the formation of the other pollutants as the air is transported can lead to harmful exposures for populations in regions far away from the wildfires. While direct emissions of harmful pollutants can affect first responders and residents, wildfire smoke can also be transported over long distances and impact air quality across local, regional, and global scales.

The health effects of wildfire smoke, such as worsening cardiovascular and respiratory conditions, extend beyond immediate exposure, contributing to nearly 16,000 annual deaths, a number expected to rise to 30,000 by 2050. The economic impact is also significant, with projected costs reaching $240 billion annually by 2050, surpassing other climate-related damages.


Over the past century, wildfires have accounted for 20–25% of global carbon emissions, the remainder from human activities. Global carbon emissions from wildfires through August 2020 equaled the average annual emissions of the European Union. In 2020, the carbon released by California's wildfires was significantly larger than the state's other carbon emissions.


Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO2 into the atmosphere, which is between 13–40% of the annual global carbon dioxide emissions from burning fossil fuels.


In June and July 2019, fires in the Arctic emitted more than 140 megatons of carbon dioxide, according to an analysis by CAMS. To put that into perspective this amounts to the same amount of carbon emitted by 36 million cars in a year. The recent wildfires and their massive CO2 emissions mean that it will be important to take them into consideration when implementing measures for reaching greenhouse gas reduction targets accorded with the Paris climate agreement. Due to the complex oxidative chemistry occurring during the transport of wildfire smoke in the atmosphere, the toxicity of emissions was indicated to increase over time.


Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%. The Amazon is estimated to hold around 90 billion tons of carbon. As of 2019, the earth's atmosphere has 415 parts per million of carbon, and the destruction of the Amazon would add about 38 parts per million.


Some research has shown wildfire smoke can have a cooling effect.


Research in 2007 stated that black carbon in snow changed temperature three times more than atmospheric carbon dioxide. As much as 94 percent of Arctic warming may be caused by dark carbon on snow that initiates melting. The dark carbon comes from fossil fuels burning, wood and other biofuels, and forest fires. Melting can occur even at low concentrations of dark carbon (below five parts per billion).


Prevention and mitigation

Wildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. Prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement.


Wildfire prevention programs around the world may employ techniques such as wildland fire use (WFU) and prescribed or controlled burns. Wildland fire use refers to any fire of natural causes that is monitored but allowed to burn. Controlled burns are fires ignited by government agencies under less dangerous weather conditions. Other objectives can include maintenance of healthy forests, rangelands, and wetlands, and support of ecosystem diversity.


Strategies for wildfire prevention, detection, control and suppression have varied over the years. One common and inexpensive technique to reduce the risk of uncontrolled wildfires is controlled burning: intentionally igniting smaller less-intense fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to limit the accumulation of plants and other debris that may serve as fuel, while also maintaining high species diversity. While other people claim that controlled burns and a policy of allowing some wildfires to burn is the cheapest method and an ecologically appropriate policy for many forests, they tend not to take into account the economic value of resources that are consumed by the fire, especially merchantable timber. Some studies conclude that while fuels may also be removed by logging, such thinning treatments may not be effective at reducing fire severity under extreme weather conditions.


Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines 5 to 10 meters (16 to 33 ft) wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.


Goat grazing programs

As climate change drives more frequent and more intense wildfires, more effort is being given to mitigation of fire potential by active measures such as managing fire fuels (ground cover, weeds, small shrubs, coyote brush, etc). In Northern California, for example, goat herds have been used in many communities to reduce the amount of fire fuels on the outskirts of some communities. It is estimated that 60 to 80,000 goats were thus employed by 2024.


Detection

The demand for timely, high-quality fire information has increased in recent years. Fast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.


Public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.


Local sensor networks

A small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or tree-rechargeable: able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.


The Department of Natural Resources signed a contract with PanoAI for the installation of 360 degree 'rapid detection' cameras around the Pacific northwest, which are mounted on cell towers and are capable of continuous monitoring of a 24-kilometre (15 mi) radius. Additionally, Sensaio Tech, based in Brazil and Toronto, has released a sensor device that continuously monitors 14 different variables common in forests, ranging from soil temperature to salinity. This information is connected live back to clients through dashboard visualizations, while mobile notifications are provided regarding dangerous levels.


Satellite and aerial monitoring

Satellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than 39 °C (102 °F). The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from 2 to 3 kilometers (1 to 2 mi) for MODIS and AVHRR data and up to 12 kilometers (7.5 mi) for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution may also limit the effectiveness of satellite imagery. Global Forest Watch provides detailed daily updates on fire alerts.


In 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions.


In 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375 m fire product, put it to use during several large wildfires in Kruger.


Since 2021 NASA has provided active fire locations in near real-time via the Fire Information for Resource Management System (FIRMS).


The increased prevalence of wildfires has led to proposals deploy technologies based on artificial intelligence for early detection, prevention, and prediction of wildfires.


Suppression

Wildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of 54,500 square kilometers (13,000,000 acres) per year.


Above all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, United States, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.


Costs of wildfire suppression

The suppression of wild fires takes up a large amount of a country's gross domestic product which directly affects the country's economy. While costs vary wildly from year to year, depending on the severity of each fire season, in the United States, local, state, federal and tribal agencies collectively spend tens of billions of dollars annually to suppress wildfires. In the United States, it was reported that approximately $6 billion was spent between 2004–2008 to suppress wildfires in the country. In California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.


Wildland firefighting safety

Wildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000 and 2016, more than 350 wildland firefighters died on-duty.


Especially in hot weather conditions, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.


Smoke, ash, and debris can also pose serious respiratory hazards for wildland firefighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.


Firefighters are also at risk of cardiac events including strokes and heart attacks. Firefighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland firefighters face include slips, trips, falls, burns, scrapes, and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.


Fire retardants

Fire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.


Typical fire retardants contain the same agents as fertilizers. Fire retardants may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.


Modeling

Wildfire modeling is concerned with numerical simulation of wildfires to comprehend and predict fire behavior. Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.


Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including wind speed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by large wildfires than by small fires.


Impacts on the natural environment

On the atmosphere

Most of Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about 10 kilometers (6 mi). The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot (black carbon), and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach 6,100 meters (20,000 ft) over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding 1,600 kilometers (1,000 mi). Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.


Wildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentrations beyond safe levels.


On ecosystems

Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.


Some ecosystems are adapted to low-severity fires, where trees can survive but underbrush is cleared. Human suppression of lightning-caused fires in areas like Canada and the United States has created a buildup of fuel compared to more frequent fires before the 20th century. This has resulted in fewer but higher-severity fires which can kill mature trees.


High-severity wildfire creates complex early seral forest habitat (also called "snag forest habitat"), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to the soil. The heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding. The exclusion of wildfires can contribute to vegetation regime shifts, such as woody plant encroachment.


Although some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as Lygodium microphyllum and Bromus tectorum, can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.


In the Amazon rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off 2.5 square kilometers (600 acres) of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants. Burn severity can be measured by satellites to calculate the Normalized Burn Ratio.


On waterways

Debris and chemical runoff into waterways after wildfires can make drinking water sources unsafe. Though it is challenging to quantify the impacts of wildfires on surface water quality, research suggests that the concentration of many pollutants increases post-fire. The impacts occur during active burning and up to years later. Increases in nutrients and total suspended sediments can happen within a year while heavy metal concentrations may peak 1–2 years after a wildfire.


Benzene is one of many chemicals that have been found in drinking water systems after wildfires. Benzene can permeate certain plastic pipes and thus require long times to be removed from the water distribution infrastructure. Researchers estimated that, in worst case scenarios, more than 286 days of constant flushing of a contaminated HDPE service line were needed to reduce benzene below safe drinking water limits. Temperature increases caused by fires, including wildfires, can cause plastic water pipes to generate toxic chemicals such as benzene.


On plant and animals

Fire adaptations are traits of plants and animals that help them survive wildfire or to use resources created by wildfire. These traits can help plants and animals increase their survival rates during a fire and/or reproduce offspring after a fire. Both plants and animals have multiple strategies for surviving and reproducing after fire. Plants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition.


For example, plants of the genus Eucalyptus contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called serotiny. Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.


Fires have also allowed for surviving species to create adaptations post-fire. This allows for better survival post-fire and furthers establishment . Adaptations as an example for boreal forests can include thick bark and post-fire flowering that is directly linked to fires. Other examples of species outside of plants and trees allow for further adaptation post-fire to survive and regenerate their population. This allows for better dispersal and new dispersal patterns to create a more stable environment. Vegetation, as an example, showed that it had greater gene flow than when unburnt and undisturbed.


Impacts on humans

Wildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human-induced geographic and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.


Airborne hazards

The most noticeable adverse effect of wildfires is the destruction of property. However, hazardous chemicals released also significantly impact human health.


Wildfire smoke is composed primarily of carbon dioxide and water vapor. Other common components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small airborne particulates (in solid form or liquid droplets) are also present in smoke and ash debris. 80–90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.


Carbon dioxide in smoke poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 μm in diameter and smaller, have been identified as the major health threats. High levels of heavy metals, including lead, arsenic, cadmium, and copper were found in the ash debris following the 2007 Californian wildfires. A national clean-up campaign was organised in fear of the health effects from exposure. In the devastating California Camp Fire (2018) that killed 85 people, lead levels increased by around 50 times in the hours following the fire at a site nearby (Chico). Zinc concentration also increased significantly in Modesto, 240 kilometres (150 mi) away. Heavy metals such as manganese and calcium were found in numerous California fires as well. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.


The degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract through inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.


The U.S. Environmental Protection Agency (EPA) developed the air quality index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use it to determine their exposure to hazardous air pollutants based on visibility range.


Health effects

Wildfire smoke contains particulates that may have adverse effects upon the human respiratory system. Evidence of the health effects should be relayed to the public so that exposure may be limited. The evidence can also be used to influence policy to promote positive health outcomes.


Inhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.


Particulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on particle diameter: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer. lmpact on the body upon inhalation varies by size. Coarse PM is filtered by the upper airways and can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of heavier and more toxic materials that lead to short-term effects with stronger impact.


Smaller PM moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM2.5 causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes damage the cells and impact cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised.  Particulates less than 0.1 micrometer are called ultrafine particle (UFP). It is a major component of wildfire smoke. UFP can enter the bloodstream like PM2.5–0.1 however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM2.5 is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly associated with exposure to fine PM from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.


Asthma exacerbation

Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma.


An observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. PM triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.


Although some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma.


There is consistent evidence between wildfire smoke and the exacerbation of asthma.


Asthma is one of the most common chronic disease among children in the United States, affecting an estimated 6.2 million children. Research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved in this. Considerable airway development occurs during the 2nd and 3rd trimesters and continues until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects, as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Studies have found significant association between PM2.5, NO2 and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressors is most likely present in distressed communities, and as this can be correlated with childhood asthma, it may further explain links between early childhood exposure to air pollution, neighborhood poverty, and childhood risk.


Carbon monoxide danger

Carbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. Thus, it is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headaches, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma, and even death. Even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990 to 2006 found that 21.9% of the deaths occurred from heart attacks.


Another important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from various countries who were directly and indirectly affected by wildfires were found to demonstrate different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.


Epidemiology

The Western US has seen an increase in both the frequency and intensity of wildfires over the last several decades. This has been attributed to the arid climate of there and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western US. Evidence has demonstrated that wildfire smoke can increase levels of airborne particulate.


The EPA has defined acceptable concentrations of PM in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.


An increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in 2003, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.


Children participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide, it is estimated that 339,000 people die due to the effects of wildfire smoke each year.



Besides the size of PM, their chemical composition should also be considered. Antecedent studies have demonstrated that the chemical composition of PM2.5 from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke such as solid fuels.

Post-fire risks

After a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits. The Intergovernmental Panel on Climate Change (IPCC) also reports that wildfires cause significant damage to electric systems, especially in dry regions.


Chemically contaminated drinking water, at levels of hazardous waste concern, is a growing problem. In particular, hazardous waste scale chemical contamination of buried water systems was first discovered in the U.S. in 2017, and has since been increasingly documented in Hawaii, Colorado, and Oregon after wildfires. In 2021, Canadian authorities adapted their post-fire public safety investigation approaches in British Columbia to screen for this risk, but have not found it as of 2023. Another challenge is that private drinking wells and the plumbing within a building can also become chemically contaminated and unsafe. Households experience a wide-variety of significant economic and health impacts related to this contaminated water. Evidence-based guidance on how to inspect and test wildfire impacted wells  and building water systems was developed for the first time in 2020. In Paradise, California, for example, the 2018 Camp Fire caused more than $150 million dollars' worth of damage. This required almost a year of time to decontaminate and repair the municipal drinking water system from wildfire damage.


The source of this contamination was first proposed after the 2018 Camp Fire in California as originating from thermally degraded plastics in water systems, smoke and vapors entering depressurized plumbing, and contaminated water in buildings being sucked into the municipal water system. In 2020, it was first shown that thermal degradation of plastic drinking water materials was one potential contamination source. In 2023, the second theory was confirmed where contamination could be sucked into pipes that lost water pressure.


Other post-fire risks, can increase if other extreme weather follows. For example, wildfires make soil less able to absorb precipitation, so heavy rainfall can result in more severe flooding and damages like mud slides.


At-risk groups

Firefighters

Firefighters are at greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Some of the most common health conditions that firefighters acquire from prolonged smoke inhalation include cardiovascular and respiratory diseases. For example, wildland firefighters can become hypoxic as a result of oxygen deprivation. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed.


Between 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis. Wildfires that reach urban environments create additional toxic fumes and carcinogenic particles from burning metals, plastics, electronics, paints, and other common materials.


Residents

Residents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods. They are also at risk for future wildfires and may move away to areas they consider less risky.


Wildfires affect large numbers of people in Western Canada and the United States. In California alone, more than 350,000 people live in towns and cities in "very high fire hazard severity zones".


Direct risks to building residents in fire-prone areas can be moderated through design choices such as choosing fire-resistant vegetation, maintaining landscaping to avoid debris accumulation and to create firebreaks, and by selecting fire-retardant roofing materials. Potential compounding issues with poor air quality and heat during warmer months may be addressed with MERV 11 or higher outdoor air filtration in building ventilation systems, mechanical cooling, and a provision of a refuge area with additional air cleaning and cooling, if needed.


History

The first evidence of wildfires is fossils of the giant fungi Prototaxites preserved as charcoal, discovered in South Wales and Poland, dating to the Silurian period (about 430 million years ago). Smoldering surface fires started to occur sometime before the Early Devonian period 405 million years ago. Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30–31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.


Wildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera Eucalyptus, Pinus and Sequoia, which have thick bark to withstand fires and employ pyriscence.


Human involvement

The human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered pre-existing landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle Ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from HMS Beagle observed Aboriginal Australians using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in lands protected by Kakadu National Park to encourage biodiversity.


Wildfires typically occur during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).


According to a paper published in the journal Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this as a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.


Increases of certain tree species (i.e. conifers) over others (i.e. deciduous trees) can increase wildfire risk, especially if these trees are also planted in monocultures.
Some invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.


Society and culture

Wildfires have a place in many cultures. "To spread like wildfire" is a common idiom in English, meaning something that "quickly affects or becomes known by more and more people".


Wildfire activity has been attributed as a major factor in the development of Ancient Greece. In modern Greece, as in many other regions, it is the most common disaster caused by a natural hazard and figures prominently in the social and economic lives of its people.


In 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie Bambi, and the official mascot of the U.S. Forest Service, Smokey Bear. The Smokey Bear fire prevention campaign has yielded one of the most popular characters in the United States; for many years there was a living Smokey Bear mascot, and it has been commemorated on postage stamps.


There are also significant indirect or second-order societal impacts from wildfire, such as demands on utilities to prevent power transmission equipment from becoming ignition sources, and the cancelation or nonrenewal of homeowners insurance for residents living in wildfire-prone areas.


See also

References

Sources

Attribution




Christian Science is a set of beliefs and practices which are associated with members of the Church of Christ, Scientist. Adherents are commonly known as Christian Scientists or students of Christian Science, and the church is sometimes informally known as the Christian Science church. It was founded in 1879 in New England by Mary Baker Eddy, who wrote the 1875 book Science and Health with Key to the Scriptures, which outlined the theology of Christian Science. The book was originally called Science and Health; the subtitle with a Key to the Scriptures was added in 1883 and later amended to with Key to the Scriptures.


The book became Christian Science's central text, along with the Bible, and by 2001 had sold over nine million copies.


Eddy and 26 followers were granted a charter by the Commonwealth of Massachusetts in 1879 to found the "Church of Christ (Scientist)"; the church would be reorganized under the name "Church of Christ, Scientist" in 1892. The Mother Church, The First Church of Christ, Scientist, was built in Boston, Massachusetts, in 1894. Known as the "thinker's religion", Christian Science became the fastest growing religion in the United States, with nearly 270,000 members by 1936 — a figure which had declined to just over 100,000 by 1990 and reportedly to under 50,000 by 2009. The church is known for its newspaper, The Christian Science Monitor, which won seven Pulitzer Prizes between 1950 and 2002, and for its public Reading Rooms around the world.


Christian Science's religious tenets differ considerably from many other Christian denominations, including key concepts such as the Trinity, the divinity of Jesus, atonement, the resurrection, and the Eucharist. Eddy, for her part, described Christian Science as a return to "primitive Christianity and its lost element of healing". Adherents subscribe to a radical form of philosophical idealism, believing that reality is purely spiritual and the material world an illusion. This includes the view that disease is a mental error rather than physical disorder, and that the sick should be treated not by medicine but by a form of prayer that seeks to correct the beliefs responsible for the illusion of ill health.


The church does not require that Christian Scientists avoid medical care—many adherents use dentists, optometrists, obstetricians, physicians for broken bones, and vaccination when required by law—but maintains that Christian Science prayer is most effective when not combined with medicine. The reliance on prayer and avoidance of medical treatment has been blamed for the deaths of adherents and their children. Between the 1880s and 1990s, several parents and others were prosecuted for, and in a few cases convicted of, manslaughter or neglect.


Overview

Metaphysical family

Several periods of Protestant Christian revival nurtured a proliferation of new religious movements in the United States. In the latter half of the 19th century, these included what came to be known as the metaphysical family: groups such as Christian Science, Divine Science, the Unity School of Christianity, and (later) the United Church of Religious Science. From the 1890s, the liberal section of the movement became known as New Thought, in part to distinguish it from the more authoritarian Christian Science.


The term metaphysical referred to the movement's philosophical idealism, a belief in the primacy of the mental world. Adherents believed that material phenomena were the result of mental states, a view expressed as "life is consciousness" and "God is mind." The supreme cause was referred to as Divine Mind, Truth, God, Love, Life, Spirit, Principle or Father–Mother, reflecting elements of Plato, Hinduism, Berkeley, Hegel, Swedenborg, and transcendentalism.


The metaphysical groups became known as the mind-cure movement because of their strong focus on healing. Medical practice was in its infancy, and patients regularly fared better without it. This provided fertile soil for the mind-cure groups, who argued that sickness was an absence of "right thinking" or failure to connect to Divine Mind. The movement traced its roots in the United States to Phineas Parkhurst Quimby (1802–1866), a New England clockmaker turned mental healer. His advertising flyer, "To the Sick" included this explanation of his clairvoyant methodology: "he gives no medicines and makes no outward applications, but simply sits down by the patients, tells them their feelings and what they think is their disease. If the patients admit that he tells them their feelings, &c., then his explanation is the cure; and, if he succeeds in correcting their error, he changes the fluids of the system and establishes the truth, or health. The Truth is the Cure. This mode of practise applies to all cases. If no explanation is given, no charge is made, for no effect is produced." Mary Baker Eddy had been a patient of his (1862–1865), leading to debate about how much of Christian Science was based on his ideas.


New Thought and Christian Science differed in that Eddy saw her views as a unique and final revelation. Eddy's idea of malicious animal magnetism (that people can be harmed by the bad thoughts of others) marked another distinction, introducing an element of fear that was absent from the New Thought literature. Most significantly, she dismissed the material world as an illusion, rather than as merely subordinate to Mind, leading her to reject the use of medicine, or materia medica, and making Christian Science the most controversial of the metaphysical groups. Reality for Eddy was purely spiritual.


Christian Science theology

Christian Science leaders place their religion within mainstream Christian teaching, according to J. Gordon Melton, and reject any identification with the New Thought movement. Eddy was strongly influenced by her Congregationalist upbringing. According to the church's tenets, adherents accept "the inspired Word of the Bible as  sufficient guide to eternal Life ... acknowledge and adore one supreme and infinite God ...  acknowledge His Son, one Christ; the Holy Ghost or divine Comforter; and man in God's image and likeness." When founding the Church of Christ, Scientist, in April 1879, Eddy wrote that she wanted to "reinstate primitive Christianity and its lost element of healing". Later she suggested that Christian Science was a kind of second coming and that Science and Health was an inspired text. In 1895, in the Manual of the Mother Church, she ordained the Bible and Science and Health as "Pastor over the Mother Church".


Christian Science theology differs in several respects from that of traditional Christianity. Eddy's Science and Health reinterprets key Christian concepts, including the Trinity, divinity of Jesus, atonement, and resurrection; beginning with the 1883 edition, she added "with a Key to the Scriptures" to the title and included a glossary that redefined the Christian vocabulary. At the core of Eddy's theology is the view that the spiritual world is the only reality and is entirely good, and that the material world, with its evil, sickness and death, is an illusion. Eddy saw humanity as an "idea of Mind" that is "perfect, eternal, unlimited, and reflects the divine", according to Bryan Wilson; what she called "mortal man" is simply humanity's distorted view of itself. Despite her view of the non-existence of evil, an important element of Christian Science theology is that evil thought, in the form of malicious animal magnetism, can cause harm, even if the harm is only apparent.


Eddy viewed God not as a person but as "All-in-all". Although she often described God in the language of personhood—she used the term "Father–Mother God" (as did Ann Lee, the founder of Shakerism), and, in the third edition of Science and Health, she referred to God as "she"—God is mostly represented in Christian Science by the synonyms "Mind, Spirit, Soul, Principle, Life, Truth, Love". The Holy Ghost is Christian Science, and heaven and hell are states of mind. There is no supplication in Christian Science prayer. The process involves the Scientist engaging in a silent argument to affirm to herself the unreality of matter, something Christian Science practitioners will do for a fee, including in absentia, to address ill health or other problems. Wilson writes that Christian Science healing is "not curative ... on its own premises, but rather preventative of ill health, accident and misfortune, since it claims to lead to a state of consciousness where these things do not exist. What heals is the realization that there is nothing really to heal." It is a closed system of thought, viewed as infallible if performed correctly; healing confirms the power of Truth, but its absence derives from the failure, specifically the bad thoughts, of individuals.


Eddy accepted as true the creation narrative in the Book of Genesis up to chapter 2, verse 6—that God created man in his image and likeness—but she rejected the rest "as the story of the false and the material", according to Wilson. Her theology is nontrinitarian: she viewed the Trinity as suggestive of polytheism. She saw Jesus as a Christian Scientist, a "Way-shower" between humanity and God, and she distinguished between Jesus the man and the concept of Christ, the latter a synonym for Truth and Jesus the first person fully to manifest it. The crucifixion was not a divine sacrifice for the sins of humanity, the atonement (the forgiveness of sin through Jesus's suffering) "not the bribing of God by offerings", writes Wilson, but an "at-one-ment" with God. Her views on life after death were vague and, according to Wilson, "there is no doctrine of the soul" in Christian Science: "fter death, the individual continues his probationary state until he has worked out his own salvation by proving the truths of Christian Science." Eddy did not believe that the dead and living could communicate.


To the more conservative of the Protestant clergy, Eddy's view of Science and Health as divinely inspired was a challenge to the Bible's authority. "Eddyism" was viewed as a cult; one of the first uses of the modern sense of the word was in A. H. Barrington's Anti-Christian Cults (1898), a book about Spiritualism, Theosophy and Christian Science. In a few cases Christian Scientists were expelled from Christian congregations, but ministers also worried that their parishioners were choosing to leave. In May 1885 the London Times' Boston correspondent wrote about the "Boston mind-cure craze": "Scores of the most valued Church members are joining the Christian Scientist branch of the metaphysical organization, and it has thus far been impossible to check the defection." In 1907 Mark Twain described the appeal of the new religion to its adherents:


 has delivered to them a religion which has revolutionized their lives, banished the glooms that shadowed them, and filled them and flooded them with sunshine and gladness and peace; a religion which has no hell; a religion whose heaven is not put off to another time, with a break and a gulf between, but begins here and now, and melts into eternity as fancies of the waking day melt into the dreams of sleep.


They believe it is a Christianity that is in the New Testament; that it has always been there, that in the drift of ages it was lost through disuse and neglect, and that this benefactor has found it and given it back to men, turning the night of life into day, its terrors into myths, its lamentations into songs of emancipation and rejoicing.


There we have Mrs. Eddy as her followers see her. ...  They sincerely believe that Mrs. Eddy's character is pure and perfect and beautiful, and her history without stain or blot or blemish. But that does not settle it.

History

Mary Baker Eddy and the early Christian Science movement

Mary Baker Eddy was born Mary Morse Baker on a farm in Bow, New Hampshire, the youngest of six children in a religious family of Protestant Congregationalists. In common with most women at the time, Eddy was given little formal education, but read widely at home and was privately tutored. From childhood, she lived with protracted ill health. Eddy's first husband died six months after their marriage and three months before their son was born, leaving her penniless; and as a result of her poor health she lost custody of the boy when he was four. She married again, and her new husband promised to become the child's legal guardian, but after their marriage he refused to sign the needed papers and the boy was taken to Minnesota and told his mother had died. Eddy, then known as Mary Patterson, and her husband moved to rural New Hampshire, where Eddy continued to suffer from health problems which often kept her bedridden. Eddy tried various cures for her health problems, including conventional medicine as well as many forms of alternative medicine such as Grahamism, electrotherapy, homeopathy, hydropathy, and finally mesmerism under Phineas Quimby. She was later accused by critics, beginning with Julius Dresser, of borrowing ideas from Quimby in what biographer Gillian Gill would call the "single most controversial issue" of her life.


In February 1866, Eddy fell on the ice in Lynn, Massachusetts. Evidence suggests she had severe injuries, but a few days later she apparently asked for her Bible, opened it to an account of one of Jesus' miracles, and left her bed telling her friends that she was healed through prayer alone. The moment has since been controversial, but she considered this moment one of the "falling apples" that helped her to understand Christian Science, although she said she did not fully understand it at the time.


In 1866, after her fall on the ice, Eddy began teaching her first student and began writing her ideas which she eventually published in Science and Health with Key to the Scriptures, considered her most important work. Her students voted to form a church called the Church of Christ (Scientist) in 1879, later reorganized as The First Church of Christ, Scientist, also known as The Mother Church, in 1892. She founded the Massachusetts Metaphysical College in 1881 to continue teaching students, Eddy started a number of periodicals: The Christian Science Journal in 1883, the Christian Science Sentinel in 1898, The Herald of Christian Science in 1903, and The Christian Science Monitor in 1908, the latter being a secular newspaper. The Monitor has gone on to win seven Pulitzer prizes as of 2011. She also wrote numerous books and articles in addition to Science and Health, including the Manual of The Mother Church which contained by-laws for church government and member activity, and founded the Christian Science Publishing Society in 1898 in order to distribute Christian Science literature. Although the movement started in Boston, the first purpose-built Christian Science church building was erected in 1886 in Oconto, Wisconsin. During Eddy's lifetime, Christian Science spread throughout the United States and to other parts of the world including Canada, Great Britain, Germany, South Africa, Hong Kong, the Philippines, Australia, and elsewhere.


Eddy encountered significant opposition after she began teaching and writing on Christian Science, which only increased towards the end of her life. One of the most prominent examples was Mark Twain, who wrote a number of articles on Eddy and Christian Science which were first published in Cosmopolitan magazine in 1899 and were later published as a book. Another extended criticism, which again was first serialized in a magazine and then published in book form, was Georgine Milmine and Willa Cather's The Life of Mary Baker G. Eddy and the History of Christian Science which first appeared in McClure's magazine in January 1907. Also in 1907, several of Eddy's relatives filed an unsuccessful lawsuit instigated by the New York World, known in the press as the "Next Friends Suit", against members of Eddy's household, alleging that she was mentally unable to manage her own affairs. The suit fell apart after Eddy was interviewed in her home in August 1907 by the judge and two court-appointed masters (one a psychiatrist) who concluded that she was mentally competent. Separately, she was seen by two psychiatrists, including Allan McLane Hamilton, who came to the same conclusion. The McClure's and New York World stories are considered to at least partially be the reason Eddy asked the church in July 1908 to found the Christian Science Monitor as a platform for responsible journalism.


Eddy died two years later, on the evening of Saturday, December 3, 1910, aged 89. The Mother Church announced at the end of the Sunday morning service that Eddy had "passed from our sight". The church stated that "the time will come when there will be no more death," but that Christian Scientists "do not look for  return in this world." Her estate was valued at $1.5 million, most of which she left to the church.


The Christian Science movement after 1910

In the aftermath of Eddy's death, some newspapers speculated that the church would fall apart, while others expected it to continue just as it had before. As it was, the movement continued to grow in the first few decades after 1910. The Manual of the Mother Church prohibits the church from publishing membership figures, and it is not clear exactly when the height of the movement was. A 1936 census counted c. 268,915 Christian Scientists in the United States (2,098 per million), and Rodney Stark believes this to be close to the height. However, the number of Christian Science churches continued to increase until around 1960, at which point there was a reversal and, since then, many churches have closed their doors. The number of Christian Science practitioners in the United States began to decline in the 1940s according to Stark. According to J. Gordon Melton, in 1972 there were 3,237 congregations worldwide, of which roughly 2,400 were in the United States; and, in the following ten years, about 200 congregations were closed.


During the years after Eddy's death, the church has gone through a number of hardships and controversies. This included attempts to make practicing Christian Science illegal in the United States and elsewhere; a period known as the Great Litigation which involved two intertwined lawsuits regarding church governance; persecution under the Nazi and Communist regimes in Germany and the Imperial regime in Japan; a series of lawsuits involving the deaths of members of the church, most notably some children; and a controversial decision to publish a book by Bliss Knapp. In conjunction with the Knapp book controversy, there was controversy within the church involving The Monitor Channel, part of The Christian Science Monitor which had been losing money, and which eventually led to the channel shutting down. Acknowledging their earlier mistake, of accepting a multi-million dollar publishing incentive to offset broadcasting losses, The Christian Science Board Of Directors, with the concurrence of the Trustees Of The Christian Science Publishing Society, withdrew Destiny Of The Mother Church from publication in September 2023. In addition, it has since its beginning been branded as a cult by more fundamentalist strains of Christianity, and attracted significant opposition as a result. A number of independent teachers and alternative movements of Christian Science have emerged since its founding, but none of these individuals or groups have achieved the prominence of the Christian Science church.


Despite the hardships and controversies, many Christian Science churches and Reading Rooms remain in existence around the world, and, in recent years, there have been reports of the religion growing in Africa, though it remains significantly behind other evangelical groups. The Christian Science Monitor also remains a well-respected non-religious paper which is especially noted for its international reporting and lack of partisanship.


Healing practices

Christian Science prayer

ll healing is a metaphysical process. That means that there is no person to be healed, no material body, no patient, no matter, no illness, no one to heal, no substance, no person, no thing and no place that needs to be influenced. This is what the practitioner must first be clear about.

— Practitioner Frank Prinz-Wondollek, 2011.

Christian Scientists avoid almost all medical treatment, relying instead on Christian Science prayer. This consists of silently arguing with oneself; there are no appeals to a personal god, and no set words. Caroline Fraser wrote in 1999 that the practitioner might repeat: "the allness of God using Eddy's seven synonyms—Life, Truth, Love, Spirit, Soul, Principle and Mind," then that "Spirit, Substance, is the only Mind, and man is its image and likeness; that Mind is intelligence; that Spirit is substance; that Love is wholeness; that Life, Truth, and Love are the only reality." She might deny other religions, the existence of evil, mesmerism, astrology, numerology, and the symptoms of whatever the illness is. She concludes, Fraser writes, by asserting that disease is a lie, that this is the word of God, and that it has the power to heal.


Christian Science practitioners are certified by the Church of Christ, Scientist, to charge a fee for Christian Science prayer. There were 1,249 practitioners worldwide in 2015; in the United States in 2010 they charged $25–$50 for an e-mail, telephone or face-to-face consultation. Their training is a two-week, 12-lesson course called "primary class", based on the Recapitulation chapter of Science and Health. Practitioners wanting to teach primary class take a six-day "normal class", held in Boston once every three years, and become Christian Science teachers. There are also Christian Science nursing homes. They offer no medical services; the nurses are Christian Scientists who have completed a course of religious study and training in basic skills, such as feeding and bathing.


The Christian Science Journal and Christian Science Sentinel publish anecdotal healing testimonials (they published 53,900 between 1900 and April 1989), which must be accompanied by statements from three verifiers: "people who know  well and have either witnessed the healing or can vouch for  integrity in sharing it". Philosopher Margaret P. Battin wrote in 1999 that the seriousness with which these testimonials are treated by Christian Scientists ignores factors such as false positives caused by self-limiting conditions. Because no negative accounts are published, the testimonials strengthen people's tendency to rely on anecdotes. A church study published in 1989 examined 10,000 published testimonials, 2,337 of which the church said involved conditions that had been medically diagnosed, and 623 of which were "medically confirmed by follow-up examinations". The report offered no evidence of the medical follow-up. The Massachusetts Committee for Children and Youth listed among the report's flaws that it had failed to compare the rates of successful and unsuccessful Christian Science treatment.


Nathan Talbot, a church spokesperson, told the New England Journal of Medicine in 1983 that church members were free to choose medical care, but according to former Christian Scientists those who do may be ostracized. In 2010 the New York Times reported church leaders as saying that, for over a year, they had been "encouraging members to see a physician if they feel it is necessary", and that they were repositioning Christian Science prayer as a supplement to medical care, rather than a substitute. The church has lobbied to have the work of Christian Science practitioners covered by insurance.


As of 2015, it was reported that Christian Scientists in Australia were not advising anyone against vaccines, and the religious exception was deemed "no longer current or necessary". In 2021, a church Committee on Publication reiterated that although vaccination was an individual choice, that the church did not dictate against it, and those who were not vaccinated did not do so because of any "church dogma".


Church of Christ, Scientist

Governance

In the hierarchy of the Church of Christ, Scientist, only the Mother Church in Boston, The First Church of Christ, Scientist, uses the definite article in its name. Otherwise the first Christian Science church in any city is called First Church of Christ, Scientist, then Second Church of Christ, Scientist, and so on, followed by the name of the city (for example, Third Church of Christ, Scientist, London). When a church closes, the others in that city are not renamed.


Founded in April 1879, the Church of Christ, Scientist is led by a president and five-person board of directors. There is a public-relations department, known as the Committee on Publication, with representatives around the world; this was set up by Eddy in 1898 to protect her own and the church's reputation. The church was accused in the 1990s of silencing internal criticism by firing staff, delisting practitioners and excommunicating members.


The church's administration is headquartered on Christian Science Center on the corner of Massachusetts Avenue and Huntington Avenue,  located on several acres in the Back Bay section of Boston. The 14.5-acre site includes the Mother Church (1894), Mother Church Extension (1906), the Christian Science Publishing Society building (1934)—which houses the Mary Baker Eddy Library and the church's administrative staff—the Sunday School building (1971), and the Church Colonnade building (1972). It also includes the 26-story Administration Building (1972), designed by Araldo Cossutta of I. M. Pei & Associates, which until 2008 housed the administrative staff from the church's 15 departments. There is also a children's fountain and a 690 ft × 100 ft (210 m × 30 m) reflecting pool.


Manual of The Mother Church

Eddy's Manual of The Mother Church (first published 1895) lists the church's by-laws. Requirements for members include daily prayer and daily study of the Bible and Science and Health. Members must subscribe to church periodicals if they can afford to, and pay an annual tax to the church of not less than one dollar.


Prohibitions include engaging in mental malpractice; visiting a store that sells "obnoxious" books; joining other churches; publishing articles that are uncharitable toward religion, medicine, the courts or the law; and publishing the number of church members. The manual also prohibits engaging in public debate about Christian Science without board approval, and learning hypnotism. It includes "The Golden Rule": "A member of The Mother Church shall not haunt Mrs. Eddy's drive when she goes out, continually stroll by her house, or make a summer resort near her for such a purpose."


Services

The Church of Christ, Scientist is a lay church which has no ordained clergy or rituals, and performs no baptisms; with clergy of other faiths often performing marriage or funeral services since they have no clergy of their own. Its main religious texts are the Bible and Science and Health. Each church has two Readers, who read aloud a "Bible lesson" or "lesson sermon" made up of selections from those texts during the Sunday service, and a shorter set of readings to open Wednesday evening testimony meetings. In addition to readings, members offer testimonials during the main portion of the Wednesday meetings, including recovery from ill health attributed to prayer. There are also hymns, time for silent prayer, and repeating together the Lord's Prayer at each service.


Notable members

Notable adherents of Christian Science have included Directors of Central Intelligence William H. Webster and Admiral Stansfield M. Turner; and Richard Nixon's chief of staff H. R. Haldeman and Chief Domestic Advisor John Ehrlichman. The viscounts Waldorf and Nancy Astor, the latter of whom was the first female member of British Parliament, were both Christian Scientists; as were two other early women in Parliament, Thelma Cazalet-Keir and Margaret Wintringham. Thelma's brother Victor Cazalet was also a member of the church. Another was naval officer Charles Lightoller, who survived the sinking of the Titanic in 1912. Other adherents in the United States government also include Senator Jocelyn Burdick, Governor Scott McCallum, and Treasury Secretary Henry Paulson. A number of suffragists were Christian Scientists including Vida Goldstein, Muriel Matters, and Nettie Rogers Shuler. Businesswomen Martha Matilda Harper and Bette Nesmith Graham were both Christian Scientists. As was the founder of the Braille Institute of America, J. Robert Atkinson.


In sports, Harry Porter, Harold Bradley Jr., and George Sisler were all adherents. Christian Scientists within the film industry, include Carol Channing and Jean Stapleton; Colleen Dewhurst; Joan Crawford, Doris Day, George Hamilton, Mary Pickford, Ginger Rogers, Mickey Rooney; Horton Foote; King Vidor; Robert Duvall, and Val Kilmer. Those raised by Christian Scientists include biographer John Matteson, jurist Helmuth James Graf von Moltke, military analyst Daniel Ellsberg; Ellen DeGeneres, Henry Fonda, Audrey Hepburn; James Hetfield, Marilyn Monroe, Robin Williams, Elizabeth Taylor, and Anne Archer. Four prominent African American entertainers who have been associated with Christian Science are Pearl Bailey, Lionel Hampton, Everett Lee, and Alfre Woodard.


Christian Science Publishing Society

The Christian Science Publishing Society publishes several periodicals, including the Christian Science Monitor, winner of seven Pulitzer Prizes between 1950 and 2002. This had a daily circulation in 1970 of 220,000, which by 2008 had contracted to 52,000. In 2009 it moved to a largely online presence with a weekly print run. In the 1980s the church produced its own television programs, and in 1991 it founded a 24-hour news channel, which closed with heavy losses after 13 months.


The church also publishes the weekly Christian Science Sentinel, the monthly Christian Science Journal, and the Herald of Christian Science, a non-English publication. In April 2012 JSH-Online made back issues of the Journal, Sentinel and Herald available online to subscribers.


Works by Mary Baker Eddy

See also

Citations

Notes

Roy M. Anker, 1999: "Mary Baker Eddy, the founder of Christian Science (denominationally known as the Church of Christ, Scientist), the most prominent, successful, controversial, and distinctive of all the groups whose inspiration scholars trace to the healing and intellectual influence of Quimby."


Rodney Stark, 1998: "But, of course, Christian Science was not just another Protestant sect. Like Joseph Smith, Mary Baker Eddy added too much new religious culture for her movement to qualify fully as a member of the Christian family—as all the leading clerics of the time repeatedly and vociferously pointed out. However, unlike Madame Blavatsky's Theosophical Society, and like the Mormons, Christian Science retained an immense amount of Christian culture. These continuities allowed converts from a Christian background to preserve a great deal of cultural capital."


Eddy, January 1901: "I should blush to write of Science and Health with Key to the Scriptures as I have, were it of human origin, and I, apart from God, its author. But, as I was only a scribe echoing the harmonies of heaven in divine metaphysics, I cannot be super-modest in my estimate of the Christian Science textbook."


References

Eddy, Manual of the Mother Church, p. 17.


Trammell, Mary M., chair, Christian Science board of directors (March 26, 2010). "Letter; What the Christian Science Church Teaches" Archived 2022-08-07 at the Wayback Machine. The New York Times.


Roy M. Anker, "Revivalism, Religious Experience and the Birth of Mental Healing", Self-help and Popular Religion in Early American Culture: An Interpretive Guide, Westport, Connecticut: Greenwood Publishing Company, 1999(a), (pp. 11–100), pp. 8, 176ff.


For early uses of New Thought, William Henry Holcombe, Condensed Thoughts about Christian Science (pamphlet), Chicago: Purdy Publishing Company, 1887; Horatio W. Dresser, "The Metaphysical Movement" (from a statement issued by the Metaphysical Club, Boston, 1901), The Spirit of the New Thought, New York: Thomas Y. Crowell Company, 1917, p. 215.


Also see "Religion: New Thought" Archived 2014-12-20 at the Wayback Machine, Time magazine, 7 November 1938; "Phineas Parkhurst Quimby" Archived 2014-11-11 at the Wayback Machine, Encyclopædia Britannica, September 9, 2013.


Simmons 1995, p. 62 Archived 2022-11-01 at the Wayback Machine; Whorton, James C. (2004). Nature Cures: The History of Alternative Medicine in America. New York: Oxford University Press. pp. 128–129 Archived 2022-11-01 at the Wayback Machine.


Claudia Stokes, The Altar at Home: Sentimental Literature and Nineteenth-Century American Religion, University of Pennsylvania Press, 2014, p. 181 Archived 2022-11-01 at the Wayback Machine.


"Sunday church services and Wednesday testimony meetings" Archived 2014-02-09 at the Wayback Machine, and "Online Wednesday meetings" Archived 2020-06-13 at the Wayback Machine, First Church of Christ, Scientist.


Sources

Further reading

Church histories


Books by former Christian Scientists


External links




The Affordable Care Act (ACA), formally known as the Patient Protection and Affordable Care Act (PPACA) and informally as Obamacare, is a landmark U.S. federal statute enacted by the 111th United States Congress and signed into law by President Barack Obama on March 23, 2010. Together with amendments made to it by the Health Care and Education Reconciliation Act of 2010, it represents the U.S. healthcare system's most significant regulatory overhaul and expansion of coverage since the enactment of Medicare and Medicaid in 1965. Most of the act remains in effect.


The ACA's major provisions came into force in 2014. By 2016, the uninsured share of the population had roughly halved, with estimates ranging from 20 to 24 million additional people covered. The law also enacted delivery system reforms intended to constrain healthcare costs and improve quality. After it came into effect, increases in overall healthcare spending slowed, including premiums for employer-based insurance plans. The increased coverage was due, roughly equally, to an expansion of Medicaid eligibility and changes to individual insurance markets. Both received new spending, funded by a combination of new taxes and cuts to Medicare provider rates and Medicare Advantage. Several Congressional Budget Office (CBO) reports stated that overall these provisions reduced the budget deficit, that repealing ACA would increase the deficit, and that the law reduced income inequality.


The act largely retained the existing structure of Medicare, Medicaid, and the employer market, but individual markets were radically overhauled. Insurers were made to accept all applicants without charging based on pre-existing conditions or demographic status (except age). To combat the resultant adverse selection, the act mandated that individuals buy insurance (or pay a monetary penalty) and that insurers cover a list of "essential health benefits". Young people were allowed to stay on their parents' insurance plans until they were 26 years old.


Before and after its enactment the ACA faced strong political opposition, calls for repeal, and legal challenges. In the Sebelius decision, the U.S. Supreme Court ruled that Congress cannot use the Commerce Clause to compel individuals to buy health insurance but upheld the individual mandate by recharacterizing the penalty as a tax. The Court also held that states could opt out of the ACA's Medicaid expansion without losing existing federal Medicaid funding, while otherwise upholding the law. This led Republican-controlled states not to participate in Medicaid expansion. Polls initially found that a plurality of Americans opposed the act, although its individual provisions were generally more popular. By 2017, the law had majority support. The Tax Cuts and Jobs Act of 2017 set the individual mandate penalty at $0 starting in 2019.


Provisions

ACA amended the Public Health Service Act of 1944 and inserted new provisions on affordable care into Title 42 of the United States Code. The individual insurance market was radically overhauled, and many of the law's regulations applied specifically to this market, while the structure of Medicare, Medicaid, and the employer market were largely retained. Some regulations applied to the employer market, and the law also made delivery system changes that affected most of the health care system.


Insurance regulations: individual policies

All new individual major medical health insurance policies sold to individuals and families faced new requirements. The requirements took effect on January 1, 2014. They include:


Individual mandate

The individual mandate required everyone to have insurance or pay a penalty. The mandate and limits on open enrollment were designed to avoid the insurance death spiral, minimize the free rider problem and prevent the healthcare system from succumbing to adverse selection.


The mandate was intended to increase the size and diversity of the insured population, including more young and healthy participants to broaden the risk pool, spreading costs.


Among the groups who were not subject to the individual mandate are:


The Tax Cuts and Jobs Act of 2017, set to $0 the penalty for not complying with the individual mandate, starting in 2019.


Exchanges

ACA mandated that health insurance exchanges be provided for each state. The exchanges are regulated, largely online marketplaces, administered by either federal or state governments, where individuals, families and small businesses can purchase private insurance plans. Exchanges first offered insurance for 2014. Some exchanges also provide access to Medicaid.


States that set up their own exchanges have some discretion on standards and prices. For example, states approve plans for sale, and thereby influence (through negotiations) prices. They can impose additional coverage requirements—such as abortion. Alternatively, states can make the federal government responsible for operating their exchanges.


Premium subsidies

Individuals whose household incomes are between 100% and 400% of the federal poverty level (FPL) are eligible to receive federal subsidies for premiums for policies purchased on an ACA exchange, provided they are not eligible for Medicare, Medicaid, the Children's Health Insurance Program, or other forms of public assistance health coverage, and do not have access to affordable coverage (no more than 9.86% of income for the employee's coverage) through their own or a family member's employer. Households below the federal poverty level are not eligible to receive these subsidies. Lawful Residents and some other legally present immigrants whose household income is below 100% FPL and are not otherwise eligible for Medicaid are eligible for subsidies if they meet all other eligibility requirements. Married people must file taxes jointly to receive subsidies. Enrollees must have U.S. citizenship or proof of legal residency to obtain a subsidy.


The subsidies for an ACA plan purchased on an exchange stop at 400% of the federal poverty level (FPL). According to the Kaiser Foundation, this results in a sharp "discontinuity of treatment" at 400% FPL, which is sometimes called the "subsidy cliff". After-subsidy premiums for the second lowest cost silver plan (SCLSP) just below the cliff are 9.86% of income in 2019.


Subsidies are provided as an advanceable, refundable tax credit.


The amount of subsidy is sufficient to reduce the premium for the second-lowest-cost silver plan (SCLSP) on an exchange to a sliding-scale percentage of income. The percentage is based on the percent of federal poverty level (FPL) for the household, and varies slightly from year to year. In 2019, it ranged from 2.08% of income (100%-133% FPL) to 9.86% of income (300%-400% FPL). The subsidy can be used for any plan available on the exchange, but not catastrophic plans. The subsidy may not exceed the premium for the purchased plan.


(In this section, the term "income" refers to modified adjusted gross income.)


Small businesses are eligible for a tax credit provided they enroll in the SHOP Marketplace.


a..mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}^ In 2019, the federal poverty level was $25,100 for family of four (outside of Alaska and Hawaii).


b.^ If the premium for the second lowest cost silver plan (SLCSP) is greater than the amount in this column, the amount of the premium subsidy will be such that it brings the net cost of the SCLSP down to the amount in this column. Otherwise, there will be no subsidy, and the SLCSP premium will (of course) be no more than (usually less than) the amount in this column.


Note: The numbers in the table do not apply for Alaska and Hawaii.


Cost-sharing reduction subsidies

As written, ACA mandated that insurers reduce copayments and deductibles for ACA exchange enrollees earning less than 250% of the FPL. Medicaid recipients were not eligible for the reductions.


So-called cost-sharing reduction (CSR) subsidies were to be paid to insurance companies to fund the reductions. During 2017, approximately $7 billion in CSR subsidies were to be paid, versus $34 billion for premium tax credits.


The latter was defined as mandatory spending that does not require an annual Congressional appropriation. CSR payments were not explicitly defined as mandatory. This led to litigation and disruption later.


Risk management

Risk corridors

The risk-corridor program was a temporary risk management device.: 1  It was intended to encourage reluctant insurers into ACA insurance market from 2014 to 2016. For those years the Department of Health and Human Services (DHHS) would cover some of the losses for insurers whose plans performed worse than they expected. Loss-making insurers would receive payments paid for in part by profit-making insurers. Similar risk corridors had been established for the Medicare prescription drug benefit.


While many insurers initially offered exchange plans, the program did not pay for itself as planned, losing up to $8.3 billion for 2014 and 2015. Authorization had to be given so DHHS could pay insurers from "general government revenues". However, the Consolidated Appropriations Act, 2014 stated that no funds "could be used for risk-corridor payments". leaving the government in a potential breach of contract with insurers who offered qualified health plans.


Several insurers sued the government at the United States Court of Federal Claims to recover the funds believed owed to them under the Risk Corridors program. While several were summarily closed, in the case of Moda Health v the United States, Moda Health won a $214-million judgment in February 2017. Federal Claims judge Thomas C. Wheeler stated, "the Government made a promise in the risk corridors program that it has yet to fulfill. Today, the court directs the Government to fulfill that promise." Moda Health's case was appealed by the government to the United States Court of Appeals for the Federal Circuit along with the appeals of the other insurers; here, the Federal Circuit reversed the Moda Health ruling and ruled across all the cases in favor of the government, that the appropriations riders ceded the government from paying out remain money due to the insurers. The Supreme Court reversed this ruling in the consolidated case, Maine Community Health Options v. United States, reaffirming as with Judge Wheeler that the government had a responsibility to pay those funds under the ACA and the use of riders to de-obligate its from those payments was illegal.


Reinsurance

The temporary reinsurance program is meant to stabilize premiums by reducing the incentive for insurers to raise premiums due to concerns about higher-risk enrollees. Reinsurance was based on retrospective costs rather than prospective risk evaluations. Reinsurance was available from 2014 through 2016.


Risk adjustment

Risk adjustment involves transferring funds from plans with lower-risk enrollees to plans with higher-risk enrollees. It was intended to encourage insurers to compete based on value and efficiency rather than by attracting healthier enrollees. Of the three risk management programs, only risk adjustment was permanent. Plans with low actuarial risk compensate plans with high actuarial risk.


Medicaid expansion

ACA revised and expanded Medicaid eligibility starting in 2014. All U.S. citizens and legal residents with income up to 133% of the poverty line would qualify for coverage in any state that participated in the Medicaid program. Previously, states could set various lower thresholds for certain groups and were not required to cover adults without dependent children. The federal government was to pay 100% of the increased cost in 2014, 2015 and 2016; 95% in 2017, 94% in 2018, 93% in 2019, and 90% in 2020 and all subsequent years. A 5% "income disregard" made the effective income eligibility limit for Medicaid 138% of the poverty level. However, the Supreme Court ruled in NFIB v. Sebelius that this provision of ACA was coercive, and that states could choose to continue at pre-ACA eligibility levels.


Medicare savings

Medicare reimbursements were reduced to insurers and drug companies for private Medicare Advantage policies that the Government Accountability Office and Medicare Payment Advisory Commission found to be excessively costly relative to standard Medicare; and to hospitals that failed standards of efficiency and care.


Taxes

Medicare taxes

Income from self-employment and wages of single individuals in excess of $200,000 annually are subjected to an additional tax of 0.9%. The threshold amount is $250,000 for a married couple filing jointly (threshold applies to their total compensation), or $125,000 for a married person filing separately.


In ACA's companion legislation, the Health Care and Education Reconciliation Act of 2010, an additional tax of 3.8% was applied to unearned income, specifically the lesser of net investment income and the amount by which adjusted gross income exceeds the above income limits.


Excise taxes

ACA included an excise tax of 40% ("Cadillac tax") on total employer premium spending in excess of specified dollar amounts (initially $10,200 for single coverage and $27,500 for family coverage) indexed to inflation. This tax was originally scheduled to take effect in 2018, but was delayed until 2020 by the Consolidated Appropriations Act, 2016 and again to 2022. The excise tax on high-cost health plans was completely repealed as part of H.R.1865 - Further Consolidated Appropriations Act, 2020.


Excise taxes totaling $3 billion were levied on importers and manufacturers of prescription drugs. An excise tax of 2.3% on medical devices and a 10% excise tax on indoor tanning services were applied as well. The tax was repealed in late 2019.


SCHIP

The State Children's Health Insurance Program (CHIP) enrollment process was simplified.


Dependents

Beginning September 23, 2010, dependents were permitted to remain on their parents' insurance plan until their 26th birthday, including dependents who no longer lived with their parents, are not a dependent on a parent's tax return, are no longer a student, or are married.


Employer mandate

Businesses that employ fifty or more people but do not offer health insurance to their full-time employees are assessed additional tax if the government has subsidized a full-time employee's healthcare through tax deductions or other means. This is commonly known as the employer mandate. This provision was included to encourage employers to continue providing insurance once the exchanges began operating.


Delivery system reforms

The act includes delivery system reforms intended to constrain costs and improve quality. These include Medicare payment changes to discourage hospital-acquired conditions and readmissions, bundled payment initiatives, the Center for Medicare and Medicaid Innovation, the Independent Payment Advisory Board, and accountable care organizations.


Hospital quality

Health care cost/quality initiatives included incentives to reduce hospital infections, adopt electronic medical records, and to coordinate care and prioritize quality over quantity.


Bundled payments

Medicare switched from fee-for-service to bundled payments. A single payment was to be paid to a hospital and a physician group for a defined episode of care (such as a hip replacement) rather than separate payments to individual service providers.


Accountable care organizations

The Medicare Shared Savings Program (MSSP) was established by section 3022 of the Affordable Care Act. It is the program by which an accountable care organization interacts with the federal government, and by which accountable care organizations can be created. It is a fee-for-service model.


The Act allowed the creation of accountable care organizations (ACOs), which are groups of doctors, hospitals and other providers that commit to give coordinated care to Medicare patients. ACOs were allowed to continue using fee-for-service billing. They receive bonus payments from the government for minimizing costs while achieving quality benchmarks that emphasize prevention and mitigation of chronic disease. Missing cost or quality benchmarks subjected them to penalties.


Unlike health maintenance organizations, ACO patients are not required to obtain all care from the ACO. Also, unlike HMOs, ACOs must achieve quality-of-care goals.


Medicare drug benefit (Part D)

Medicare Part D participants received a 50% discount on brand name drugs purchased after exhausting their initial coverage and before reaching the catastrophic-coverage threshold. By 2020, the "doughnut hole" would be completely filled.


State waivers

From 2017 onwards, states can apply for a "waiver for state innovation" which allows them to conduct experiments that meet certain criteria. To obtain a waiver, a state must pass legislation setting up an alternative health system that provides insurance at least as comprehensive and as affordable as ACA, covers at least as many residents and does not increase the federal deficit. These states can escape some of ACA's central requirements, including the individual and employer mandates and the provision of an insurance exchange. The state would receive compensation equal to the aggregate amount of any federal subsidies and tax credits for which its residents and employers would have been eligible under ACA, if they cannot be paid under the state plan.


Other insurance provisions

The Community Living Assistance Services and Supports Act (or CLASS Act) established a voluntary and public long-term care insurance option for employees, The program was abolished as impractical without ever having taken effect.


Consumer Operated and Oriented Plans (CO-OP), member-governed non-profit insurers, could start providing health care coverage, based on a 5-year federal loan. As of 2017, only four of the original 23 co-ops were still in operation, including Mountain Health CO-OP.


Nutrition labeling requirements

Nutrition labeling requirements officially took effect in 2010, but implementation was delayed, and they actually took effect on May 7, 2018.


Legislative history

ACA followed a long series of unsuccessful attempts by one party or the other to pass major insurance reforms. Innovations were limited to health savings accounts (2003), medical savings accounts (1996) or flexible spending accounts, which increased insurance options, but did not materially expand coverage. Health care was a major factor in multiple elections, but until 2009, neither party had the votes to overcome the other's opposition.


Individual mandate

The concept of an individual mandate goes back to at least 1989, when The Heritage Foundation, a conservative think-tank, proposed an individual mandate as an alternative to single-payer health care. It was championed for a time by conservative economists and Republican senators as a market-based approach to healthcare reform on the basis of individual responsibility and avoidance of free rider problems. Specifically, because the 1986 Emergency Medical Treatment and Active Labor Act (EMTALA) requires any hospital participating in Medicare (nearly all do) to provide emergency care to anyone who needs it, the government often indirectly bore the cost of those without the ability to pay.


President Bill Clinton proposed a major healthcare reform bill in 1993 that ultimately failed. Clinton negotiated a compromise with the 105th Congress to instead enact the State Children's Health Insurance Program (SCHIP) in 1997. The failed Clinton plan included a mandate for employers to provide health insurance to all employees through a regulated marketplace of health maintenance organizations. Republican senators proposed an alternative that would have required individuals, but not employers, to buy insurance.


The 1993 Republican Health Equity and Access Reform Today (HEART) Act, contained a "universal coverage" requirement with a penalty for noncompliance—an individual mandate—as well as subsidies to be used in state-based 'purchasing groups'. Advocates included prominent Republican senators such as John Chafee, Orrin Hatch, Chuck Grassley, Bob Bennett and Kit Bond. The 1994 Republican Consumer Choice Health Security Act, initially contained an individual mandate with a penalty provision; however, author Don Nickles subsequently removed the mandate, stating, "government should not compel people to buy health insurance". At the time of these proposals, Republicans did not raise constitutional issues; Mark Pauly, who helped develop a proposal that included an individual mandate for George H. W. Bush, remarked, "I don't remember that being raised at all. The way it was viewed by the Congressional Budget Office in 1994 was, effectively, as a tax."


In 2006, an insurance expansion bill was enacted at the state level in Massachusetts. The bill contained both an individual mandate and an insurance exchange. Republican Governor Mitt Romney used a line-item veto on some provisions, and the Democratic legislature overrode some of his changes (including the mandate). Romney's implementation of the 'Health Connector' exchange and individual mandate in Massachusetts was at first lauded by Republicans. During Romney's 2008 presidential campaign, Senator Jim DeMint praised Romney's ability to "take some good conservative ideas, like private health insurance, and apply them to the need to have everyone insured". Romney said of the individual mandate: "I'm proud of what we've done. If Massachusetts succeeds in implementing it, then that will be the model for the nation."


In 2007 Republican Senator Bob Bennett and Democratic Senator Ron Wyden introduced the Healthy Americans Act, which featured an individual mandate and state-based, regulated insurance markets called "State Health Help Agencies". The bill attracted bipartisan support, but died in committee. Many of its sponsors and co-sponsors remained in Congress during the 2008 healthcare debate.


By 2008 many Democrats were considering this approach as the basis for healthcare reform. Experts said the legislation that eventually emerged from Congress in 2009 and 2010 bore similarities to the 2007 bill and that it took ideas from the Massachusetts reforms.


Academic foundation

A driving force behind Obama's healthcare reform was Peter Orszag, Director of the Office of Management and Budget. Obama called Orszag his "healthcare czar" because of his knowledge of healthcare reform. Orszag had previously been director of the Congressional Budget Office, and under his leadership the agency had focused on using cost analysis to create an affordable and effective approach to health care reform. Orszag claimed that healthcare reform became Obama's top agenda item because he wanted it to be his legacy. According to an article by Ryan Lizza in The New Yorker, the core of "the Obama budget is Orszag's belief ...a government empowered with research on the most effective medical treatments". Obama bet "his presidency on Orszag's thesis of comparative effectiveness." Orszag's policies were influenced by an article in The Annals of Internal Medicine co-authored by Elliott S. Fisher, David Wennberg and others. The article presented strong evidence based on the co-authors' research that numerous procedures, therapies and tests were being delivered with scant evidence of their medical value. If those procedures and tests could be eliminated, this evidence suggested, medical costs might provide the savings to give healthcare to the uninsured population. After reading a New Yorker article that used the "Dartmouth findings" to compare two counties in Texas with enormous variations in Medicare costs using hard data, Obama directed that his entire staff read it. More than anything else, the Dartmouth data intrigued Obama since it gave him an academic rationale for reshaping medicine.


The concept of comparing the effectiveness of healthcare options based on hard data ("comparative effectiveness" and "evidence-based medicine") was pioneered by John E. Wennberg, founder of The Dartmouth Institute, co-founder of The Foundation for Informed Medical Decision Making and senior advisor to Health Dialog Inc., a venture that he and his researchers created to help insurers implement the Dartmouth findings.


Healthcare debate, 2008–10

Healthcare reform was a major topic during the 2008 Democratic presidential primaries. As the race narrowed, attention focused on the plans presented by the two leading candidates, Hillary Clinton and the eventual nominee, Barack Obama. Each candidate proposed a plan to cover the approximately 45 million Americans estimated to not have health insurance at some point each year. Clinton's proposal would have required all Americans to obtain coverage (in effect, an individual mandate), while Obama's proposal provided a subsidy without a mandate.


During the general election, Obama said fixing healthcare would be one of his top four priorities as president. Obama and his opponent, Senator John McCain, both proposed health insurance reforms, though their plans differed. McCain proposed tax credits for health insurance purchased in the individual market, which was estimated to reduce the number of uninsured people by about 2 million by 2018. Obama proposed private and public group insurance, income-based subsidies, consumer protections, and expansions of Medicaid and SCHIP, which was estimated at the time to reduce the number of uninsured people by 33.9 million by 2018 at a higher cost.


Obama announced to a joint session of Congress in February 2009 his intent to work with Congress to construct a plan for healthcare reform. By July, a series of bills were approved by committees within the House of Representatives. On the Senate side, from June to September, the Senate Finance Committee held a series of 31 meetings to develop a proposal. This group—in particular, Democrats Max Baucus, Jeff Bingaman and Kent Conrad, along with Republicans Mike Enzi, Chuck Grassley and Olympia Snowe—met for more than 60 hours, and the principles they discussed, in conjunction with the other committees, became the foundation of a Senate bill.


Congressional Democrats and health policy experts, such as MIT economics professor Jonathan Gruber and David Cutler, argued that guaranteed issue would require both community rating and an individual mandate to ensure that adverse selection or "free riding" would not result in an insurance "death spiral". They chose this approach after concluding that filibuster-proof support in the Senate was not present for more progressive plans such as single-payer. By deliberately drawing on bipartisan ideas—the same basic outline was supported by former Senate Majority Leaders Howard Baker, Bob Dole, Tom Daschle and George J. Mitchell—the bill's drafters hoped to garner the necessary votes.


However, following the incorporation of an individual mandate into the proposal, Republicans threatened to filibuster any bill that contained it. Senate Minority Leader Mitch McConnell, who led the Republican response, concluded Republicans should not support the bill.


Republican senators, including those who had supported earlier proposals with a similar mandate, began to describe the mandate as "unconstitutional". Journalist Ezra Klein wrote in The New Yorker, "a policy that once enjoyed broad support within the Republican Party suddenly faced unified opposition."


The reform debate also attracted focused attention from lobbyists, including pharmaceutical and healthcare insurance company advocacy, to assure continued opposition from all Republicans and even from among more conservative democrats, who had not fully supported the public option in earlier proposals.


During the August 2009 summer congressional recess, many members went back to their districts and held town hall meetings on the proposals. The nascent Tea Party movement organized protests and many conservative groups and individuals attended the meetings to oppose the proposed reforms. Threats were made against members of Congress over the course of the debate.


In September 2009 Obama delivered another speech to a joint session of Congress supporting the negotiations. On November 7, the House of Representatives passed the Affordable Health Care for America Act on a 220–215 vote and forwarded it to the Senate for passage.


Senate

The Senate began work on its own proposals while the House was still working. The United States Constitution requires all revenue-related bills to originate in the House. To formally comply with this requirement, the Senate repurposed H.R. 3590, a bill regarding housing tax changes for service members. It had been passed by the House as a revenue-related modification to the Internal Revenue Code. The bill became the Senate's vehicle for its healthcare reform proposal, discarding the bill's original content. The bill ultimately incorporated elements of proposals that were reported favorably by the Senate Health and Finance committees. With the Republican Senate minority vowing to filibuster, 60 votes would be necessary to pass the Senate. At the start of the 111th Congress, Democrats had 58 votes. The Minnesota Senate election was ultimately won by Democrat Al Franken, making 59. Arlen Specter switched to the Democratic party in April 2009, giving them 60 seats, enough to end a filibuster.


Negotiations were undertaken attempting to satisfy moderate Democrats and to bring Republican senators aboard; particular attention was given to Republicans Bennett, Enzi, Grassley and Snowe.


After the Finance Committee vote on October 15, negotiations turned to moderate Democrats. Senate Majority Leader Harry Reid focused on satisfying centrists. The holdouts came down to Joe Lieberman of Connecticut, an independent who caucused with Democrats, and conservative Nebraska Democrat Ben Nelson. Lieberman's demand that the bill not include a public option was met, although supporters won various concessions, including allowing state-based public options such as Vermont's failed Green Mountain Care.


The White House and Reid addressed Nelson's concerns during a 13-hour negotiation with two concessions: a compromise on abortion, modifying the language of the bill "to give states the right to prohibit coverage of abortion within their own insurance exchanges", which would require consumers to pay for the procedure out of pocket if the state so decided; and an amendment to offer a higher rate of Medicaid reimbursement for Nebraska. The latter half of the compromise was derisively termed the "Cornhusker Kickback" and was later removed.


On December 23, the Senate voted 60–39 to end debate on the bill: a cloture vote to end the filibuster. The bill then passed, also 60–39, on December 24, 2009, with all Democrats and two independents voting for it, and all Republicans against (except Jim Bunning, who did not vote). The bill was endorsed by the American Medical Association and AARP.


On January 19, 2010, Massachusetts Republican Scott Brown was elected to the Senate in a special election to replace the recently deceased Ted Kennedy, having campaigned on giving the Republican minority the 41st vote needed to sustain Republican filibusters. Additionally, the symbolic importance of losing Kennedy's traditionally Democratic Massachusetts seat made many Congressional Democrats concerned about the political cost of the bill.


House

With Democrats no longer able to get the 60 votes to break a filibuster in the Senate, White House Chief of Staff Rahm Emanuel argued that Democrats should scale back to a less ambitious bill, but House Speaker Nancy Pelosi pushed back, dismissing more moderate reform as "Kiddie Care".


Obama remained insistent on comprehensive reform. The news that Anthem in California intended to raise premium rates for its patients by as much as 39% gave him new evidence of the need for reform. On February 22, he laid out a "Senate-leaning" proposal to consolidate the bills. He held a meeting with both parties' leaders on February 25. The Democrats decided the House would pass the Senate's bill, to avoid another Senate vote.


House Democrats had expected to be able to negotiate changes in a House–Senate conference before passing a final bill. Since any bill that emerged from conference that differed from the Senate bill would have to pass the Senate over another Republican filibuster, most House Democrats agreed to pass the Senate bill on condition that it be amended by a subsequent bill. They drafted the Health Care and Education Reconciliation Act, which could be passed by the reconciliation process.


Per the Congressional Budget Act of 1974, reconciliation cannot be subject to a filibuster. But reconciliation is limited to budget changes, which is why the procedure was not used to pass ACA in the first place; the bill had inherently non-budgetary regulations. Although the already-passed Senate bill could not have been passed by reconciliation, most of House Democrats' demands were budgetary: "these changes—higher subsidy levels, different kinds of taxes to pay for them, nixing the Nebraska Medicaid deal—mainly involve taxes and spending. In other words, they're exactly the kinds of policies that are well-suited for reconciliation."


The remaining obstacle was a pivotal group of pro-life Democrats led by Bart Stupak who were initially reluctant to support the bill. The group found the possibility of federal funding for abortion significant enough to warrant opposition. The Senate bill had not included language that satisfied their concerns, but they could not address abortion in the reconciliation bill as it would be non-budgetary. Instead, Obama issued Executive Order 13535, reaffirming the principles in the Hyde Amendment to continue banning the use of federal funds for abortion. This won the support of Stupak and members of his group and assured the bill's passage. The House passed the Senate bill with a 219–212 vote on March 21, 2010, with 34 Democrats and all 178 Republicans voting against it. It passed the second bill, by 220–211, the same day (with the Senate passing this bill via reconciliation by 56-43 a few days later). The day after the passage of ACA, March 22, Republicans introduced legislation to repeal it. Obama signed ACA into law on March 23, 2010.


Post-enactment

Since passage, Republicans have voted to repeal all or parts of the Affordable Care Act more than sixty times.


The Tax Cuts and Jobs Act of 2017 eliminated the fine for violating the individual mandate, starting in 2019. (The requirement itself is still in effect.) In 2019 Congress repealed the so-called "Cadillac" tax on health insurance benefits, an excise tax on medical devices, and the Health Insurance Tax.


The American Rescue Plan Act of 2021, enacted during the COVID-19 pandemic in the United States, expanded subsidies for marketplace health plans. A continuation of these subsidies was introduced as part of the Inflation Reduction Act of 2022.


Impact

Coverage

The law caused a significant reduction in the number and percentage of people without health insurance. The CDC reported that the percentage of people without health insurance fell from 16.0% in 2010 to 8.9% from January to June 2016. The uninsured rate dropped in every congressional district in the U.S. from 2013 to 2015. The Congressional Budget Office reported in March 2016 that approximately 12 million people were covered by the exchanges (10 million of whom received subsidies) and 11 million added to Medicaid. Another million were covered by ACA's "Basic Health Program", for a total of 24 million. CBO estimated that ACA would reduce the net number of uninsured by 22 million in 2016, using a slightly different computation for the above figures totaling ACA coverage of 26 million, less 4 million for reductions in "employment-based coverage" and "non-group and other coverage".


The U.S. Department of Health and Human Services (HHS) estimated that 20.0 million adults (aged 18–64) gained healthcare coverage via ACA as of February 2016; similarly, the Urban Institute found in 2016 that 19.2 million non-elderly Americans gained health insurance coverage from 2010 to 2015. In 2016, CBO estimated the uninsured at approximately 27 million people, or around 10% of the population or 7–8% excluding unauthorized immigrants.


States that expanded Medicaid had a 7.3% uninsured rate on average in the first quarter of 2016, while those that did not had a 14.1% uninsured rate, among adults aged 18–64. As of December 2016 32 states (including Washington DC) had adopted the Medicaid extension.


A 2017 study found that the ACA reduced socioeconomic disparities in health care access.


The Affordable Care Act reduced the percent of Americans between 18 and 64 who were uninsured from 22.3 percent in 2010 to 12.4 percent in 2016. About 21 million more people have coverage ten years after the enactment of the ACA. Ten years after its enactment studies showed that the ACA also had a positive effect on health and caused a reduction in mortality.


Taxes

Excise taxes from the Affordable Care Act raised $16.3 billion in fiscal year 2015. $11.3 billion came from an excise tax placed directly on health insurers based on their market share. Annual excise taxes totaling $3 billion were levied on importers and manufacturers of prescription drugs.


The Individual mandate tax was $695 per individual or $2,085 per family at a minimum, reaching as high as 2.5% of household income (whichever was higher). The tax was set to $0 beginning in 2019.


In the fiscal year 2018, the individual and employer mandates yielded $4 billion each. Excise taxes on insurers and drug makers added $18 billion. Income tax surcharges produced 437 billion.


ACA reduced income inequality measured after taxes, due to the income tax surcharges and subsidies. CBO estimated that subsidies paid under the law in 2016 averaged $4,240 per person for 10 million individuals receiving them, roughly $42 billion. The tax subsidy for the employer market, was approximately $1,700 per person in 2016, or $266 billion total.


Insurance exchanges

As of August 2016, 15 states operated their own health insurance marketplace. Other states either used the federal exchange, or operated in partnership with or supported by the federal government. By 2019, 12 states and Washington DC operated their own exchanges.


Medicaid expansion in practice

As of December 2019, 37 states (including Washington DC) had adopted the Medicaid extension. Those states that expanded Medicaid had a 7.3% uninsured rate on average in the first quarter of 2016, while the others had a 14.1% uninsured rate, among adults aged 18 to 64. Following the Supreme Court ruling in 2012, which held that states would not lose Medicaid funding if they did not expand Medicaid under ACA, several states rejected the option. Over half the national uninsured population lived in those states.


The Centers for Medicare and Medicaid Services (CMS) estimated that the cost of expansion was $6,366 per person for 2015, about 49 percent above previous estimates. An estimated 9 to 10 million people had gained Medicaid coverage, mostly low-income adults. The Kaiser Family Foundation estimated in October 2015 that 3.1 million additional people were not covered because of states that rejected the Medicaid expansion.


In many states income thresholds were significantly below 133% of the poverty line. Many states did not make Medicaid available to childless adults at any income level. Because subsidies on exchange insurance plans were not available to those below the poverty line, such individuals had no new options. For example, in Kansas, where only non-disabled adults with children and with an income below 32% of the poverty line were eligible for Medicaid, those with incomes from 32% to 100% of the poverty level ($6,250 to $19,530 for a family of three) were ineligible for both Medicaid and federal subsidies to buy insurance. Absent children, non-disabled adults were not eligible for Medicaid there.


Studies of the impact of Medicaid expansion rejections calculated that up to 6.4 million people would have too much income for Medicaid but not qualify for exchange subsidies. Several states argued that they could not afford the 10% contribution in 2020. Some studies suggested rejecting the expansion would cost more due to increased spending on uncompensated emergency care that otherwise would have been partially paid for by Medicaid coverage,


A 2016 study found that residents of Kentucky and Arkansas, which both expanded Medicaid, were more likely to receive health care services and less likely to incur emergency room costs or have trouble paying their medical bills. Residents of Texas, which did not accept the Medicaid expansion, did not see a similar improvement during the same period. Kentucky opted for increased managed care, while Arkansas subsidized private insurance. Later Arkansas and Kentucky governors proposed reducing or modifying their programs. From 2013 to 2015, the uninsured rate dropped from 42% to 14% in Arkansas and from 40% to 9% in Kentucky, compared with 39% to 32% in Texas.


A 2016 DHHS study found that states that expanded Medicaid had lower premiums on exchange policies, because they had fewer low-income enrollees, whose health on average is worse than that of those with higher income.


In September 2019, the Census Bureau reported that states that expanded Medicaid under the ACA had considerably lower uninsured rates than states that did not. For example, for adults between 100% and 399% of poverty level, the uninsured rate in 2018 was 12.7% in expansion states and 21.2% in non-expansion states. Of the 14 states with uninsured rates of 10% or greater, 11 had not expanded Medicaid. The drop in uninsured rates due to expanded Medicaid has broadened access to care among low-income adults, with post-ACA studies indicating an improvement in affordability, access to doctors, and usual sources of care.


A study using national data from the Health Reform Monitoring Survey determined that unmet need due to cost and inability to pay medical bills significantly decreased among low-income (up to 138% FPL) and moderate-income (139-199% FPL) adults, with unmet need due to cost decreasing by approximately 11 percentage points among low-income adults by the second enrollment period. Importantly, issues with cost-related unmet medical needs, skipped medications, paying medical bills, and annual out-of-pocket spending have been significantly reduced among low-income adults in Medicaid expansion states compared to non-expansion states.


As well, expanded Medicaid has led to a 6.6% increase in physician visits by low-income adults, as well as increased usage of preventative care such as dental visits and cancer screenings among childless, low-income adults. Improved health care coverage due to Medicaid expansion has been found in a variety of patient populations, such as adults with mental and substance use disorders, trauma patients, cancer patients, and people living with HIV. Compared to 2011–13, in 2014 there was a 5.4 percentage point reduction in the uninsured rate of adults with mental disorders (from 21.3% to 15.9%) and a 5.1 percentage point reduction in the uninsured rate of adults with substance use disorders (from 25.9% to 20.8%); with increases in coverage occurring primarily through Medicaid. Use of mental health treatment increased by 2.1 percentage points, from 43% to 45.1%.


Among trauma patients nationwide, the uninsured rate has decreased by approximately 50%. Adult trauma patients in expansion states experienced a 13.7 percentage point reduction in uninsured rates compared to adult trauma patients in non-expansion states, and an accompanying 7.4 percentage point increase in discharge to rehabilitation. Following Medicaid expansion and dependent coverage expansion, young adults hospitalized for acute traumatic injury in Maryland experienced a 60% increase in rehabilitation, 25% reduction in mortality, and a 29.8% reduction in failure-to-rescue. Medicaid expansion's swift impact on cancer patients was demonstrated in a study using the National Cancer Institute's Surveillance, Epidemiology, and End Results (SEER) program that evaluated more than 850,000 patients diagnosed with breast, lung, colorectal, prostate cancer, or thyroid cancer from 2010 to 2014. The study found that a cancer diagnosis in 2014 was associated with a 1.9 percentage-point absolute and 33.5% relative decrease in uninsured rates compared to a diagnosis made between 2010 and 2013. Another study, using Surveillance, Epidemiology, and End Results (SEER) Program data from 2010 to 2014, found that Medicaid expansion was associated with a 6.4% net increase in early stage (in situ, local, or regional) diagnoses of all cancers combined.


Data from the Centers for Disease and Prevention's (CDC) Medical Monitoring Project demonstrated that between 2009 and 2012, approximately 18% of people living with HIV (PLWH) who were actively receiving HIV treatment were uninsured and that at least 40% of HIV-infected adults receiving treatment were insured through Medicaid or Medicare, programs they qualified for only once their disease was advanced enough to be covered as a disability under Social Security. Expanded Medicaid coverage of PLWH has been positively associated with health outcomes such as viral suppression, retention of care, hospitalization rates, and morbidity at the time of hospitalization. An analysis of Behavioral Risk Factor Surveillance System (BRFSS) survey data found a 2.8% annual increase in viral suppression rates among all PLWH from 2010 to 2015 due to Medicaid expansion. As an early adopter of Medicaid expansion, Massachusetts found a 65% rate of viral suppression among all PLWH and an 85% rate among those retained in healthcare in 2014, both substantially higher than the national average.


An analysis of hospital discharge data from 2012 to 2014 in four Medicaid expansion states and two non-expansion states revealed hospitalizations of uninsured PLWH fell from 13.7% to 5.5% in the four expansion states and rose from 14.5% to 15.7% in the two non-expansion states. Importantly, uninsured PLWH were 40% more likely to die in the hospital than insured PLWH. Other notable health outcomes associated with Medicaid expansion include improved glucose monitoring rates for patients with diabetes, better hypertension control, and reduced rates of major post-operative morbidity.


A July 2019 study by the National Bureau of Economic Research (NBER) indicated that states enacting Medicaid expansion exhibited statistically significant reductions in mortality rates. From that study, states that took Medicaid expansion "saved the lives of at least 19,200 adults aged 55 to 64 over the four-year period from 2014 to 2017." Further, 15,600 older adults died prematurely in the states that did not enact Medicaid expansion in those years according to the NBER research. "The lifesaving impacts of Medicaid expansion are large: an estimated 39 to 64 percent reduction in annual mortality rates for older adults gaining coverage."


Due to many states' failure to expand, many Democrats co-sponsored the proposed 2021 Cover Now Act that would allow county and municipal governments to fund Medicaid expansion.


Gaps in expansion

Despite the significant increase in access to insurance coverage and healthcare services across the board, the ACA's Medicaid expansion has not fully addressed problems of economic equity. Critics argue that Medicaid expansion has not reduced cost-sharing by a significant margin, as the amount households paid out of pocket for healthcare over the past ten years (in the form of deductibles, co-payments, etc.) rose by 77%. Additionally, 30% of providers deny Medicaid patients, which affects the accessibility of quality care. This increase in denial may be in part because providers receive 62 cents from Medicaid for every dollar received from private insurers. Studies on insurance rates show that economic inequality still persists: a significantly higher proportion of those with income greater than 100% but less than 200% of the federal poverty level were uninsured from 2010 to 2015 than of those with income greater than 200% of the federal poverty level. This is exacerbated by the 2012 Supreme Court decision allowing states to opt out of Medicaid, since many of the states that have opted out have more vulnerable populations, with large numbers of minorities or low-income people.


Medicaid patients have also reported receiving "second-class" treatment compared to privately insured patients, with longer wait times and lower quality of care.


Medicaid expansion by state

Insurance costs

National health care expenditures rose faster than national income both before (2009–2013: 3.73%) and after (2014–2018: 4.82%) ACA's major provisions took effect. Premium prices rose considerably before and after. For example, a study published in 2016 found that the average requested 2017 premium increase among 40-year-old non-smokers was about 9 percent, although Blue Cross Blue Shield proposed increases of 40 percent in Alabama and 60 percent in Texas. However, some or all these costs were offset by tax credits. For example, the Kaiser Family Foundation reported that for the second-lowest cost "Silver plan", a 40-year old non-smoker making $30,000 per year would pay effectively the same amount in 2017 as they did in 2016 (about $208/month) after the tax credit, despite a large increase in the list price. This was consistent nationally. In other words, the subsidies increased along with the premium price, fully offsetting the increases for subsidy-eligible enrollees.


Premium cost increases in the employer market moderated after 2009. For example, healthcare premiums for those covered by employers rose by 69% from 2000 to 2005, but only 27% from 2010 to 2015, with only a 3% increase from 2015 to 2016. From 2008 to 2010 (before passage of ACA) health insurance premiums rose by an average of 10% per year.


Several studies found that the 2008 financial crisis and accompanying Great Recession could not account for the entirety of the slowdown and that structural changes likely shared at least partial credit. A 2013 study estimated that changes to the health system had been responsible for about a quarter of the recent reduction in inflation. Paul Krawzak claimed that even if cost controls succeed in reducing the amount spent on healthcare, such efforts on their own may be insufficient to outweigh the long-term burden placed by demographic changes, particularly the growth of the population on Medicare.


In a 2016 review, Barack Obama claimed that from 2010 through 2014 mean annual growth in real per-enrollee Medicare spending was negative, down from a mean of 4.7% per year from 2000 through 2005 and 2.4% per year from 2006 to 2010; similarly, mean real per-enrollee growth in private insurance spending was 1.1% per year over the period, compared with a mean of 6.5% from 2000 through 2005 and 3.4% from 2005 to 2010.


Deductibles and co-payments

A contributing factor to premium cost moderation was that the insured faced higher deductibles, copayments and out-of-pocket maximums. In addition, many employees chose to combine a health savings account with higher deductible plans, making the net impact of ACA difficult to determine precisely.


For the group market (employer insurance), a 2016 survey found that:


For the non-group market, of which two-thirds are covered by ACA exchanges, a survey of 2015 data found that:


Health outcomes

According to a 2014 study, ACA likely prevented an estimated 50,000 preventable patient deaths from 2010 to 2013. Himmelstein and Woolhandler wrote in January 2017 that a rollback of ACA's Medicaid expansion alone would cause an estimated 43,956 deaths annually.


According to the Kaiser Foundation, expanding Medicaid in the remaining states would cover up to 4.5 million persons. A 2021 study found a significant decline in mortality rates in the states that opted in to the Medicaid expansion program compared with those states that did not do so. The study reported that states decisions' not to expand Medicaid resulted in approximately 15,600 excess deaths from 2014 through 2017.


Dependent Coverage Expansion (DCE) under the ACA has had a demonstrable effect on various health metrics of young adults, a group with a historically low level of insurance coverage and utilization of care. Numerous studies have shown the target age group gained private health insurance relative to an older group after the policy was implemented, with an accompanying improvement in having a usual source of care, reduction in out-of-pocket costs of high-end medical expenditures, reduction in frequency of Emergency Department visits, 3.5% increase in hospitalizations and 9% increase in hospitalizations with a psychiatric diagnosis, 5.3% increase in utilizing specialty mental health care by those with a probable mental illness, 4% increase in reporting excellent mental health, and a 1.5-6.2% increase in reporting excellent physical health. Studies have also found that DCE was associated with improvements in cancer prevention, detection, and treatment among young adult patients. A study of 10,010 women aged 18–26 identified through the 2008-12 National Health Interview Survey found that the likelihood of HPV vaccination initiation and completion increased by 7.7 and 5.8 percentage points respectively when comparing before and after October 1, 2010. Another study using National Cancer Database (NCDB) data from 2007 to 2012 found a 5.5 percentage point decrease in late-stage (stages III/IV) cervical cancer diagnosis for women aged 21–25 after DCE, and an overall decrease of 7.3 percentage points in late-stage diagnosis compared to those aged 26–34. A study using SEER Program data from 2007 to 2012 found a 2.7 percentage point increase in diagnosis at stage I disease for patients aged 19–25 compared with those aged 26–34 for all cancers combined. Studies focusing on cancer treatment after DCE found a 12.8 percentage point increase in the receipt of fertility-sparing treatment among cervical cancer patients aged 21–25 and an overall increase of 13.4 percentage points compared to those aged 26–34, as well as an increased likelihood that patients aged 19–25 with stage IIB-IIIC colorectal cancer receive timely adjuvant chemotherapy compared to those aged 27–34.


Two 2018 JAMA studies found the Hospital Readmissions Reduction Program (HRRP) was associated with increased post-discharge mortality for patients hospitalized for heart failure and pneumonia. A 2019 JAMA study found that ACA decreased emergency department and hospital use by uninsured individuals. Several studies have indicated that increased 30-day, 90-day, and 1-year post-discharge mortality of heart failure patients can be attributed to "gaming the system" through inappropriate triage systems in emergency departments, use of observation stays when admissions are warranted, and delay of readmission beyond the 30th day post-discharge, strategies that can reduce readmission rates at the expense of quality of care and patient survival. The HRRP was also shown to disproportionately penalize safety-net hospitals that predominately serve low-income patients. A 2020 study by Treasury Department economists in the Quarterly Journal of Economics using a randomized controlled trial (the IRS sent letters to some taxpayers noting that they had paid a fine for not signing up for health insurance but not to other taxpayers) found that over two years, obtaining health insurance reduced mortality by 12 percent. The study concluded that the letters, sent to 3.9 million people, may have saved 700 lives.


A 2020 JAMA study found that Medicare expansion under the ACA was associated with reduced incidence of advanced-stage breast cancer, indicating that Medicaid accessibility led to early detection of breast cancer and higher survival rates. Recent studies have also attributed to Medicaid expansion an increase in use of smoking cessation medications, cervical cancer screening, and colonoscopy, as well as an increase in the percentage of early-stage diagnosis of all cancers and the rate of cancer surgery for low-income patients. These studies include a 2.1% increase in the probability of smoking cessation in Medicaid expansion states compared to non-expansion states, a 24% increase in smoking cessation medication use due to increased Medicaid-financed smoking cessation prescriptions, a 27.7% increase in the rate of colorectal cancer screening in Kentucky following Medicaid expansion with an accompanying improvement in colorectal cancer survival, and a 3.4% increase in cancer incidence following Medicaid expansion that was attributed to an increase in early-stage diagnoses.


Transition-of-care interventions and Alternative Payment Models under the ACA have also shown promise in improving health outcomes. Post-discharge provider appointment and telephone follow-up interventions have been shown to reduce 30-day readmission rates among general medical-surgical inpatients. Reductions in 60, 90, and 180 post-discharge day readmission rates due to transition-of-care interventions have also been demonstrated, and a reduction in 30-day mortality has been suggested. Total joint arthroplasty bundles as part of the Bundled Payments for Care Improvement initiative have been shown to reduce discharge to inpatient rehabilitation facilities and post-acute care facilities, decrease hospital length of stay by 18% without sacrificing quality of care, and reduce the rate of total joint arthroplasty readmissions, half of which were due to surgical complications. The Hospital Value-Based Purchasing Program in Medicaid has also shown the potential to improve health outcomes, with early studies reporting positive and significant effects on total patient experience score, 30-day readmission rates, incidences of pneumonia and pressure ulcers, and 30-day mortality rates for pneumonia. The patient-centered medical home (PCMH) payment and care model, a team-based approach to population health management that risk-stratifies patients and provides focused care management and outreach to high-risk patients, has been shown to improve diabetes outcomes. A widespread PCMH demonstration program focusing on diabetes, known as the Chronic Care Initiative in the Commonwealth of Pennsylvania, found statistically significant improvements in A1C testing, LDL-C testing, nephropathy screening and monitoring, and eye examinations, with an accompanying reduction in all-cause emergency department visits, ambulatory care-sensitive emergency department visits, ambulatory visits to specialists, and a higher rate of ambulatory visits to primary care providers. The ACA overall has improved coverage and care of diabetes, with a significant portion of the 3.5 million uninsured US adults aged 18–64 with diabetes in 2009-10 likely gaining coverage and benefits such as closure of the Medicaid Part D coverage gap for insulin. 2.3 million of the approximately 4.6 million people aged 18–64 with undiagnosed diabetes in 2009–2010 may also have gained access to zero-cost preventative care due to section 2713 of the ACA, which prohibits cost sharing for United States Preventive Services Taskforce grade A or B recommended services, such as diabetes screenings.


Distributional impact

In March 2018, the CBO reported that ACA had reduced income inequality in 2014, saying the law led the lowest and second quintiles (the bottom 40%) to receive an average of an additional $690 and $560 respectively while causing households in the top 1% to pay an additional $21,000 due mostly to the net investment income tax and the additional Medicare tax. The law placed relatively little burden on households in the top quintile (top 20%) outside of the top 1%.


Federal deficit

CBO estimates of revenue and impact on deficit

The CBO reported in multiple studies that ACA would reduce the deficit, and repealing it would increase the deficit, primarily because of the elimination of Medicare reimbursement cuts. The 2011 comprehensive CBO estimate projected a net deficit reduction of more than $200 billion during the 2012–2021 period: it calculated the law would result in $604 billion in total outlays offset by $813 billion in total receipts, resulting in a $210 billion net deficit reduction. The CBO separately predicted that while most of the spending provisions do not begin until 2014, revenue would exceed spending in those subsequent years. The CBO claimed the bill would "substantially reduce the growth of Medicare's payment rates for most services; impose an excise tax on insurance plans with relatively high premiums; and make various other changes to the federal tax code, Medicare, Medicaid, and other programs"—ultimately extending the solvency of the Medicare trust fund by eight years.


This estimate was made prior to the Supreme Court's ruling that enabled states to opt out of the Medicaid expansion, thereby forgoing the related federal funding. The CBO and JCT subsequently updated the budget projection, estimating the impact of the ruling would reduce the cost estimate of the insurance coverage provisions by $84 billion.


The CBO in June 2015 forecast that repeal of ACA would increase the deficit between $137 billion and $353 billion over the 2016–2025 period, depending on the impact of macroeconomic feedback effects. The CBO also forecast that repeal of ACA would likely cause an increase in GDP by an average of 0.7% in the period from 2021 to 2025, mainly by boosting the supply of labor.


Although the CBO generally does not provide cost estimates beyond the 10-year budget projection period because of the degree of uncertainty involved in the projection, it decided to do so in this case at the request of lawmakers, and estimated a second decade deficit reduction of $1.2 trillion. CBO predicted deficit reduction around a broad range of one-half percent of GDP over the 2020s while cautioning that "a wide range of changes could occur".


In 2017 CBO estimated that repealing the individual mandate alone would reduce the 10-year deficit by $338 billion.


Opinions on CBO projections

The CBO cost estimates were criticized because they excluded the effects of potential legislation that would increase Medicare payments by more than $200 billion from 2010 to 2019. However, the so-called "doc fix" is a separate issue that would have existed with or without ACA. The Center on Budget and Policy Priorities objected that Congress had a good record of implementing Medicare savings. According to their study, Congress followed through on the implementation of the vast majority of provisions enacted in the past 20 years to produce Medicare savings, although not the doc fix. The doc fix became obsolete in 2015 when the savings provision was eliminated, permanently removing that spending restraint.


Health economist Uwe Reinhardt, wrote, "The rigid, artificial rules under which the Congressional Budget Office must score proposed legislation unfortunately cannot produce the best unbiased forecasts of the likely fiscal impact of any legislation." Douglas Holtz-Eakin alleged that the bill would increase the deficit by $562 billion because, he argued, it front-loaded revenue and back-loaded benefits.


Scheiber and Cohn rejected critical assessments of the law's deficit impact, arguing that predictions were biased towards underestimating deficit reduction. They noted, for example, it is easier to account for the cost of definite levels of subsidies to specified numbers of people than to account for savings from preventive healthcare, and that the CBO had a track record of overestimating costs and underestimating savings of health legislation; stating, "innovations in the delivery of medical care, like greater use of electronic medical records and financial incentives for more coordination of care among doctors, would produce substantial savings while also slowing the relentless climb of medical expenses ... But the CBO would not consider such savings in its calculations, because the innovations hadn't really been tried on such large scale or in concert with one another—and that meant there wasn't much hard data to prove the savings would materialize."


In 2010 David Walker said the CBO estimates were not likely to be accurate, because they were based on the assumption that the law would not change.


Employer mandate and part-time work

The employer mandate applies to employers of more than fifty where health insurance is provided only to the full-time workers. Critics claimed it created a perverse incentive to hire part-timers instead. However, between March 2010 and 2014, the number of part-time jobs declined by 230,000 while the number of full-time jobs increased by two million. In the public sector full-time jobs turned into part-time jobs much more than in the private sector. A 2016 study found only limited evidence that ACA had increased part-time employment.


Several businesses and the state of Virginia added a 29-hour-a-week cap for their part-time employees, to reflect the 30-hour-or-more definition for full-time worker. As of 2013, few companies had shifted their workforce towards more part-time hours (4% in a survey from the Federal Reserve Bank of Minneapolis). Trends in working hours and the recovery from the Great Recession correlate with the shift from part-time to full-time work. Other confounding impacts include that health insurance helps attract and retain employees, increases productivity and reduces absenteeism; and lowers corresponding training and administration costs from a smaller, more stable workforce. Relatively few firms employ over 50 employees and more than 90% of them already offered insurance.


Most policy analysts (both right and left) were critical of the employer mandate provision. They argued that the perverse incentives regarding part-time hours, even if they did not change existing plans, were real and harmful; that the raised marginal cost of the 50th worker for businesses could limit companies' growth; that the costs of reporting and administration were not worth the costs of maintaining employer plans; and noted that the employer mandate was not essential to maintain adequate risk pools. The provision generated vocal opposition from business interests and some unions who were not granted exemptions.


Hospitals

From the start of 2010 to November 2014, 43 hospitals in rural areas closed. Critics claimed the new law had caused these closures. Many rural hospitals were built using funds from the 1946 Hill–Burton Act. Some of these hospitals reopened as other medical facilities, but only a small number operated emergency rooms (ER) or urgent care centers.


Between January 2010 and 2015, a quarter of ER doctors said they had seen a major surge in patients, while nearly half had seen a smaller increase. Seven in ten ER doctors claimed they lacked the resources to deal with large increases in the number of patients. The biggest factor in the increased number of ER patients was insufficient primary care providers to handle the larger number of insured. Michael Lee Jr. and Michael C. Monuteaux at Boston Children's Hospital analyzed national emergency department visits among children aged 0 to 17 from 2009 to 2016 using the American Community Survey (ACS) and Nationwide Emergency Department Sample (NEDS). They found no immediate change in pediatric emergency department visit rates the year after the ACA took full effect in 2014, but the rate of change from 2014 to 2016 was significantly higher than previous rate trends, almost 10%.


Several large insurers formed ACOs. Many hospitals merged and purchased physician practices, amounting to a significant consolidation of the provider industry. The increased market share gave them more leverage with insurers and reduced patient care options.


Economic consequences

CBO estimated in June 2015 that repealing ACA would:


In 2015 the progressive Center for Economic and Policy Research found no evidence that companies were reducing worker hours to avoid ACA requirements for employees working more than 30 hours per week.


CBO estimated that ACA would slightly reduce the size of the labor force and number of hours worked, as some would no longer be tethered to employers for their insurance. Jonathan Cohn claimed that ACA's primary employment effect was to alleviate job lock and the reform's only significant employment impact was the retirement of those who were working only to stay insured.


Public opinion

Public views became increasingly negative in reaction to specific plans discussed during the legislative debate over 2009 and 2010. Approval varied by party, race and age. Some elements were more widely favored (preexisting conditions) or opposed (individual mandate).


In a 2010 poll, 62% of respondents said they thought ACA would "increase the amount of money they personally spend on health care", 56% said the bill "gives the government too much involvement in health care", and 19% said they thought they and their families would be better off with the legislation. Other polls found that people were concerned the law would cost more than projected and would not do enough to control costs.


In a 2012 poll 44% supported the law, with 56% against. By 75% of Democrats, 27% of Independents and 14% of Republicans favored the law. 82% favored banning insurance companies from denying coverage to people with preexisting conditions, 61% favored allowing children to stay on their parents' insurance until age 26, 72% supported requiring companies with more than 50 employees to provide insurance for their employees, and 39% supported the individual mandate to own insurance or pay a penalty. By party affiliation, 19% of Republicans, 27% of Independents, and 59% of Democrats favored the mandate. Other polls showed additional provisions receiving majority support, including the exchanges, pooling small businesses and the uninsured with other consumers and providing subsidies.


Some opponents believed the reform did not go far enough: a 2012 poll indicated that 71% of Republican opponents rejected it overall, while 29% believed it did not go far enough; independent opponents were divided 67% to 33%; and among the much smaller group of Democratic opponents, 49% rejected it overall and 51% wanted more.


In June 2013, a majority of the public (52–34%) indicated a desire for "Congress to implement or tinker with the law rather than repeal it". After the Supreme Court upheld the individual mandate, a 2012 poll held that "most Americans (56%) want to see critics of President Obama's health care law drop efforts to block it and move on to other national issues".


As of October 2013, approximately 40% were in favor while 51% were against. About 29% of whites approved of the law, compared with 61% of Hispanics and 91% of African Americans. A solid majority of seniors opposed the idea and a solid majority of those under forty were in favor.


A 2014 poll reported that 26% of Americans support ACA. A later 2014 poll reported that 48.9% of respondents had an unfavorable view of ACA versus 38.3% who had a favorable view (of more than 5,500 individuals). Another held that 8% of respondents agreed the Affordable Care Act "is working well the way it is". In late 2014, a Rasmussen poll reported Repeal: 30%, Leave as is: 13%, Improve: 52%.


In 2015, a poll reported that 47% of Americans approved the health care law. This was the first time a major poll indicated that more respondents approved than disapproved. A December 2016 poll reported that: a) 30% wanted to expand what the law does; b) 26% wanted to repeal the entire law; c) 19% wanted to move forward with implementing the law as it is; and d) 17% wanted to scale back what the law does, with the remainder undecided.


Separate polls from Fox News and NBC/WSJ, both taken during January 2017, indicated more people viewed the law favorably than did not for the first time. One of the reasons for the improving popularity of the law is that Democrats who had once opposed it (many still prefer "Medicare for all") shifted their positions because ACA was under threat of repeal. Another January 2017 poll reported that 35% of respondents believed "Obamacare" and the "Affordable Care Act" were different or did not know. (About 45% were unsure whether "repeal of Obamacare" also meant "repeal of the Affordable Care Act".) 39% did not know that "many people would lose coverage through Medicaid or subsidies for private health insurance if the ACA were repealed and no replacement enacted", with Democrats far more likely (79%) to know that fact than Republicans (47%). A 2017 study found that personal experience with public health insurance programs led to greater support for the ACA, most prominently among Republicans and low-information voters.


By the end of 2023, a Morning Consult poll of registered voters found that 57% approved of the Affordable Care Act, while 30% disapproved of it. 85% of Democrats, 56% of independents, and 28% of Republicans supported the law.


Political aspects

"Obamacare"

The term "Obamacare" was originally coined by opponents as a pejorative. According to research by Elspeth Reeve, the expression was used in early 2007, generally by writers describing the candidate's proposal for expanding coverage for the uninsured. The term officially emerged in March 2007 when healthcare lobbyist Jeanne Schulte Scott wrote, "We will soon see a 'Giuliani-care' and 'Obama-care' to go along with 'McCain-care', 'Edwards-care', and a totally revamped and remodeled 'Hillary-care' from the 1990s".


In May 2007, Mitt Romney introduced it to political discourse, saying, "How can we get those people insured without raising taxes and without having government take over healthcare?' And let me tell you, if we don't do it, the Democrats will. If the Democrats do it, it will be socialized medicine; it'll be government-managed care. It'll be what's known as Hillarycare or Barack Obamacare, or whatever you want to call it."


By mid-2012, Obamacare had become the colloquial term used both by supporters and opponents. Obama eventually endorsed the nickname, saying, "I have no problem with people saying Obama cares. I do care." By 2016 Obama explicitly and repeatedly referred to the ACA as "Obamacare".


The use of "Obamacare" became increasingly rare, and at the 2024 Democratic National Convention, Obama said, "I noticed, by the way, since it became popular, they don't call it 'Obamacare' no more."


Common misconceptions

"Death panels"

On August 7, 2009, Sarah Palin created the term "death panels" to describe groups who would decide whether sick patients were "worthy" of medical care. "Death panel" referred to two claims about early drafts.


One was that under the law, seniors could be denied care due to their age and the other that the government would advise seniors to end their lives instead of receiving care. The ostensible basis of these claims was the provision for an Independent Payment Advisory Board (IPAB). IPAB was given the authority to recommend cost-saving changes to Medicare by facilitating the adoption of cost-effective treatments and cost-recovering measures when statutory expenditure levels were exceeded within any given three-year period. In fact, the Board was prohibited from recommending changes that would reduce payments before 2020, and was prohibited from recommending changes in premiums, benefits, eligibility and taxes, or other changes that would result in rationing.


The other related issue concerned advance-care planning consultation: a section of the House reform proposal would have reimbursed physicians for providing patient-requested consultations for Medicare recipients on end-of-life health planning (which is covered by many private plans), enabling patients to specify, on request, the kind of care they wished to receive. The provision was not included in ACA.


In 2010, the Pew Research Center reported that 85% of Americans were familiar with the claim, and 30% believed it was true, backed by three contemporaneous polls. The allegation was named PolitiFact's 2009 "Lie of the Year", one of FactCheck.org's "whoppers" and the most outrageous term by the American Dialect Society. AARP described such rumors as "rife with gross—and even cruel—distortions".


Members of Congress

ACA requires members of Congress and their staffs to obtain health insurance either through an exchange or some other program approved by the law (such as Medicare), instead of using the insurance offered to federal employees (the Federal Employees Health Benefits Program).


Illegal immigrants

ACA explicitly denies insurance subsidies to "unauthorized (illegal) aliens".


Exchange "death spiral"

Opponents claimed that combining immediate coverage with no provision for preexisting conditions would lead people to wait to get insured until they got sick. The individual mandate was designed to push people to get insured without waiting. This has been called a "death spiral". In the years after 2013, many insurers did leave specific marketplaces, claiming the risk pools were too small.


The median number of insurers per state was 4.0 in 2014, 5.0 in 2015, 4.0 in 2016 and 3.0 in 2017. Five states had one insurer in 2017, 13 had two, 11 had three; the remainder had four or more.


"If you like your plan"

At various times during and after ACA debate Obama said, "If you like your health care plan, you'll be able to keep your health care plan." However, in fall 2013 millions of Americans with individual policies received notices that their insurance plans were terminated, and several million more risked seeing their current plans canceled.


Obama's previous unambiguous assurance that consumers could keep their own plans became a focal point for critics, who challenged his truthfulness. Various bills were introduced in Congress to allow people to keep their plans.


PolitiFact initially cited various estimates that only about 2% of the total insured population (4 million out of 262 million) received such notices, but readers later voted Obama's claims as the 2013 "Lie of the Year".


Criticism and opposition

Opposition and efforts to repeal the legislation have drawn support from sources that include labor unions, conservative advocacy groups, Republicans, small business organizations and the Tea Party movement. These groups claimed the law would disrupt existing health plans, increase costs from new insurance standards, and increase the deficit. Some opposed the idea of universal healthcare. President Donald Trump repeatedly promised to "repeal and replace" it.


As of 2013 unions that expressed concerns included the AFL–CIO, which called ACA "highly disruptive", claiming it would drive up costs of union-sponsored plans; the International Brotherhood of Teamsters, United Food and Commercial Workers International Union, and UNITE-HERE, whose leaders argued "PPACA will shatter not only our hard-earned health benefits, but destroy the foundation of the 40-hour work week that is the backbone of the American middle class." In 2014, Laborers' International Union of North America (LIUNA) president Terry O'Sullivan and Unite Here president D. Taylor wrote, "ACA, as implemented, undermines fair marketplace competition in the health care industry."


In October 2016, Mark Dayton, the governor of Minnesota, said ACA had "many good features" but it was "no longer affordable for increasing numbers of people"; he called on the state legislature to provide emergency relief to policyholders. Dayton later said he regretted his remarks after they were seized on by Republicans seeking to repeal the law.


Socialism debate

Many economically conservative opponents called the ACA "socialist" or "socialized medicine", pointing to the government redistribution of wealth via subsidies for low-income purchasers of private insurance, expansion of Medicaid, government requirements as to what products can be sold on the exchanges, and the individual mandate, which reduces freedom of consumer choice to be uninsured.


Other observers considered the law a relatively capitalist or "regulated free-market" means of paying for near-universal health care, because it creates new marketplaces with choices for consumers, largely relies on private employers and private health insurance companies, maintains private ownership of hospitals and doctor's offices, and was originally advocated by economic conservatives as a capitalist alternative to single-payer health care. Some pointed out that the previous system also had socialist aspects. Even for-profit private health insurance companies socialize risk and redistribute wealth from people who have it (all premium payers) to those who need it (by paying for medically necessary healthcare). The requirement to provide emergency care also forced redistribution from people who pay insurance premiums to those who choose to be uninsured, when they visit the emergency room.


Some ACA supporters accused conservatives of using the term "socialism" as a negative buzzword for ACA as it was for Medicare and Medicaid, and some embraced the label "socialism" as desirable, distinguishing democratic socialism as desirable for education and health care and communism as undesirable. Milos Forman opined that critics "falsely equate Western European-style socialism, and its government provision of social insurance and health care, with Marxist–Leninist totalitarianism".


Legal challenges

National Federation of Independent Business v. Sebelius

Opponents challenged ACA's constitutionality in multiple lawsuits on multiple grounds. The Supreme Court ruled, 5–4, that the individual mandate was constitutional when viewed as a tax, although not under the Commerce Clause.


ACA withheld all Medicaid funding from states declining to participate in the expansion. The Court ruled that this was unconstitutionally coercive and that individual states had the right to opt out without losing preexisting Medicaid funding.


Contraception mandate

In March 2012, the Catholic Church, while supportive of ACA's objectives, voiced concern through the United States Conference of Catholic Bishops that aspects of the mandate covering contraception and sterilization and HHS's narrow definition of a religious organization violated the First Amendment right to free exercise of religion and conscience. Various lawsuits addressed these concerns, including Burwell v. Hobby Lobby Stores, Inc., which looked at private corporations and their duties under the ACA.


In Little Sisters of the Poor Saints Peter and Paul Home v. Pennsylvania, the Supreme Court ruled 7–2 on July 8, 2020, that employers with religious or moral objections to contraceptives can exclude such coverage from an employee's insurance plan. Writing for the majority, Justice Clarence Thomas said, "No language in the statute itself even hints that Congress intended that contraception should or must be covered. It was Congress, not the , that declined to expressly require contraceptive coverage in the ACA itself." Justices Roberts, Alito, Gorsuch, and Kavanaugh joined Thomas's opinion. Justice Elena Kagan filed a concurring opinion in the judgment, in which Stephen Breyer joined. Justices Ginsburg and Sotomayor dissented, saying the court's ruling "leaves women workers to fend for themselves."


In a later lawsuit brought by private health insurance buyers and businesses, Judge Reed O'Connor of the Federal District Court for the Northern District of Texas ruled in March 2023 that the ACA's provision of contraceptives, HIV testing, and screenings for cancer, diabetes, and mental health violated the plaintiffs' freedom of religious exercise, and placed an injunction on that portion of the ACA. The Biden administration planned to seek a hold on O'Connor's decision.


King v Burwell

On June 25, 2015, the U.S. Supreme Court ruled, 6–3, that federal subsidies for health insurance premiums could be used in the 34 states that did not set up their own insurance exchanges.


House v. Price

House Republicans sued the Obama administration in 2014, alleging that cost-sharing reduction subsidy payments to insurers were unlawful because Congress had not appropriated funds to pay for them. The argument classified the CSR subsidy as discretionary spending subject to annual appropriation. In May 2016, a federal judge ruled for the plaintiffs, but the Obama administration appealed. Later, President Trump ended the payments. This led to further litigation.


United States House of Representatives v. Azar

The House sued the administration, alleging that the money for CSRs to insurers had not been appropriated, as required for any federal government spending. The ACA subsidy that helps customers pay premiums was not part of the suit.


Without the CSRs, the government estimated that premiums would increase by 20% to 30% for silver plans. In 2017, the uncertainty about whether the payments would continue caused Blue Cross Blue Shield of North Carolina to try to raise premiums by 22.9% the next year, as opposed to an increase of 8.8% that it would have sought if the payments were assured.


U.S. District Judge Rosemary M. Collyer ruled that the cost-sharing program was unconstitutional for spending money that has not been specifically provided by an act of Congress, but concluded that Congress had in fact authorized that program to be created. The judge also found that Congress had provided authority to cover the spending for the tax credits to consumers who use them to help afford health coverage. Collyer enjoined further cost-sharing payments, but stayed the order pending appeal to the United States Court of Appeals for the District of Columbia Circuit. The case ended in a settlement before the Circuit Court.


California v. Texas

Texas and 19 other states filed a civil suit in the United States District Court for the Northern District of Texas in February 2018, arguing that with the passage of the Tax Cuts and Jobs Act of 2017, which eliminated the tax for not having health insurance, the individual mandate no longer had a constitutional basis and thus the entire ACA was no longer constitutional. The Department of Justice said it would no longer defend the ACA in court, but 17 states led by California stepped in to do so.


District Judge Reed O'Connor of Texas ruled for the plaintiffs on December 14, 2018, writing that the "Individual Mandate can no longer be fairly read as an exercise of Congress's Tax Power and is still impermissible under the Interstate Commerce Clause—meaning the Individual Mandate is unconstitutional." He then further reasoned that the individual mandate is an essential part of the entire law, and thus was not severable, making the entire law unconstitutional. O'Connor's decision regarding severability turned on several passages from the Congressional debate that focused on the importance of the mandate. While he ruled the law unconstitutional, he did not overturn the law.


The intervening states appealed the decision to the Fifth Circuit. These states argued that Congress's change in the tax was only reducing the amount of the tax, and that Congress had the power to write a stronger law to this end. O'Connor stayed his decision pending the appeal. The Fifth Circuit heard the appeal on July 9, 2019; in the interim, the U.S. Department of Justice joined with Republican states to argue that the ACA was unconstitutional, while the Democratic states were joined by the Democrat-controlled U.S. House of Representatives. An additional question was addressed, as the Republican plaintiffs challenged the Democratic states' standing to defend the ACA.


In December 2019, the Fifth Circuit agreed the individual mandate was unconstitutional, but did not agree that the entire law should be voided. Instead, it remanded the case to the District Court for reconsideration of that question. The Supreme Court accepted the case in March 2020, to be heard in the 2020–2021 term, with the ruling likely falling after the 2020 elections.


Democrats pointed out that the effect of invalidating the entire law would be to remove popular provisions such as the protection for preexisting conditions, and that the Republicans had still not offered any replacement plan—important issues in the 2020 elections.


On June 17, 2021, the Court rejected the challenge in a 7–2 decision, ruling that Texas and the other plaintiff states did not have standing to challenge the provision, leaving the full ACA intact.


Section 1557

In April 2024, the Biden administration issued a final rule implementing Section 1557 of the ACA, adding gender identity to Title IX's definition of discrimination based on sex. In May 2024, 15 states led by Tennessee sued HHS in the U.S. District Court for the Southern District of Mississippi, arguing that the 2024 rule violated the Administrative Procedure Act and the U.S. Constitution. In October 2025, a federal judge struck down the 2024 rule to the extent that it prohibits discrimination against transgender people in education and health-care programs.


Risk corridors

The Supreme Court ruled that promised risk corridor payments must be made even in the absence of specific appropriation of money by Congress.


Non-cooperation

Officials in Texas, Florida, Alabama, Wyoming, Arizona, Oklahoma and Missouri opposed those elements over which they had discretion. For example, Missouri declined to expand Medicaid or establish a health insurance marketplace engaging in active non-cooperation, enacting a statute forbidding any state or local official to render any aid not specifically required by federal law. Other Republicans discouraged efforts to advertise the law's benefits. Some conservative political groups launched ad campaigns to discourage enrollment.


Repeal and modification efforts

ACA was the subject of many unsuccessful repeal efforts by Republicans in the 111th, 112th, and 113th Congresses: Representatives Steve King and Michele Bachmann introduced bills in the House to repeal the ACA the day after it was signed, as did Senator Jim DeMint in the Senate. In 2011, after Republicans gained control of the House, one of the first votes held was on a bill titled "Repealing the Job-Killing Health Care Law Act" (H.R. 2), which the House passed 245–189. All Republicans and three Democrats voted for repeal. In the Senate, the bill was offered as an amendment to an unrelated bill, but was voted down. President Obama said he would veto the bill had it passed.


On February 3, 2015, the House of Representatives added its 67th repeal vote to the record (239 to 186). This attempt also failed.


2013 federal government shutdown

Strong partisan disagreement in Congress prevented adjustments to the Act's provisions. But at least one change, a proposed repeal of a tax on medical devices, received bipartisan support. Some Congressional Republicans argued against improvements to the law on the grounds that they would weaken the arguments for repeal.


Republicans attempted to defund the ACA's implementation, and in October 2013 House Republicans refused to fund the federal government unless it came with an implementation delay, after Obama unilaterally deferred the employer mandate by one year, which critics claimed he had no power to do. The House passed three versions of a bill funding the government while submitting various versions that would repeal or delay the ACA, with the last version delaying enforcement of the individual mandate. The Democratic Senate leadership said the Senate would pass only a bill without any restrictions on ACA. The government shutdown lasted from October 1 to October 17.


2017 repeal effort

During a midnight congressional session starting January 11, the Senate of the 115th Congress of the United States voted to approve a "budget blueprint" that would allow Republicans to repeal parts of the law "without threat of a Democratic filibuster". The plan, which passed 51–48, was named by Senate Republicans the "Obamacare 'repeal resolution.'" Democrats opposing the resolution staged a protest during the vote.


House Republicans announced their replacement, the American Health Care Act, on March 6. On March 24, the AHCA failed amid a revolt among Republican representatives.


On May 4 the House voted to pass the AHCA by a margin of 217 to 213. The Senate Republican leadership announced that Senate Republicans would write their own version of the bill instead of voting on the House version.


Leader McConnell named a group of 13 Republicans to draft the substitute version in private, raising bipartisan concerns about lack of transparency. On June 22, Republicans released the first discussion draft, which renamed it the "Better Care Reconciliation Act of 2017" (BCRA). On July 25, although no amendment proposal had garnered majority support, Republicans voted to advance the bill to the floor and begin formal consideration of amendments. Senators Susan Collins and Lisa Murkowski were the only two dissenting Republicans, making the vote a 50–50 tie. Vice President Mike Pence then cast the tie-breaking vote in the affirmative.


The revised BCRA failed, 43–57. A subsequent "Obamacare Repeal and Reconciliation Act" abandoned the "repeal and replace" approach in favor of a straight repeal, but that too failed, 45–55. Finally, the "Health Care Freedom Act", nicknamed "skinny repeal" because it would have made the least change to ACA, failed by 49–51, with Collins, Murkowski, and McCain joining all Democrats and independents in voting against it.


Proposed changes in 2024

Donald Trump, who has historically opposed the ACA, said during the 2024 United States presidential debates that he had "concepts of a plan" to modify or scrap it. JD Vance has said that Trump intends to allow insurance companies to discriminate against people with preexisting conditions or disability, with subsidized insurance replaced with private insurance. Kamala Harris said she would "maintain and grow" the ACA.


2025 federal government shutdown

During the 2025 federal government shutdown, the Affordable Care Act once again became a focal point of partisan disagreement. On November 8–9, 2025, as the shutdown reached its 39th day, President Donald Trump urged Senate Republicans to redirect federal funds used for ACA insurance subsidies toward direct cash payments to individuals, framing the proposal as a strategy to resolve the budgetary impasse. Several Republican senators expressed openness to considering the idea, while Democrats rejected it. Analysts noted that the proposal lacked the bipartisan support necessary to reopen the federal government. Contemporary reports described the dispute over the ACA's subsidy structure as one of the key issues contributing to the protracted shutdown.
Trump believes that this law will expire once its tax credit is eliminated and is not renewed.


The disagreement focused on whether to continue the enhanced premium tax credits expanded under previous legislation, including the American Rescue Plan, which lowered the portion of health insurance costs paid by consumers. Observers noted that if these subsidies were allowed to expire, many individuals could face significant premium increases, potentially more than doubling for some enrollees. This raised concerns about decreased enrollment in the ACA marketplaces, especially among individuals who previously received little or no subsidy.


In addition to the impacts on consumers, small businesses and self-employed individuals relying on ACA marketplace plans faced potential financial strain. Reports also highlighted that certain federal health agency employees were furloughed during the shutdown, temporarily affecting the oversight and administration of health programs. The debate over ACA funding illustrated how federal budget impasses can have immediate effects on both insurance markets and the delivery of health services.


Actions to hinder implementation

Under both the ACA (current law) and the AHCA, the CBO reported that the health exchange marketplaces would remain stable. But Republican politicians took a variety of steps to undermine it, creating uncertainty that adversely impacted enrollment and insurer participation while increasing premiums. Concern about the exchanges became another argument for reforms. Past and ongoing Republican attempts to weaken the law have included:


Perceived inadequacies

In December 2009, former DNC chairman and former Vermont governor Howard Dean called the ACA "a bigger bailout for the insurance industry than AIG" and "an insurance company's dream". He viewed the bill's end form as a death of the health care reform effort.


In his 2011 book Remedy and Reaction, Paul Starr, the former senior advisor for Bill Clinton's health care reform plan, notes that the ACA did not make health insurance a right and did not make medical care free at the point of service. He criticizies the ACA on the grounds that some lower-income individuals still cannot afford treatment and go without care "if health care and insurance are treated as ordinary commodities".


The ACA's critics often cite its inability to control costs and lower deductibles, the difficulty for average people to compare plans, lack of a strong public option, and inadequate regulations on or alternatives (such as co-operatives) to large corporate health care companies.


Implementation

In 2010 small business tax credits took effect. Then Pre-Existing Condition Insurance Plan (PCIP) took effect to offer insurance to those who had been denied coverage by private insurance companies because of a preexisting condition. By 2011, insurers had stopped marketing child-only policies in 17 states, as they sought to escape this requirement. In National Federation of Independent Business v. Sebelius the Supreme Court allowed states to opt out of the Medicaid expansion.


In 2013, the Internal Revenue Service ruled that the cost of covering only the individual employee would be considered in determining whether the cost of coverage exceeded 9.5% of income. Family plans would not be considered even if the cost was above the 9.5% income threshold. On July 2 Obama delayed the employer mandate until 2015. The launch for both the state and federal exchanges was beset by management and technical failings. HealthCare.gov, the website that offers insurance through the exchanges operated by the federal government, crashed on opening and suffered many problems. Operations stabilized in 2014, although not all planned features were complete.


The Government Accountability Office released a non-partisan study in 2014 that concluded the administration had not provided "effective planning or oversight practices" in developing the exchanges. In Burwell v. Hobby Lobby the Supreme Court exempted closely held corporations with religious convictions from the contraception rule. At the beginning of the 2015, 11.7 million had signed up (ex-Medicaid). By the end of the year about 8.8 million consumers had stayed in the program. Congress repeatedly delayed the onset of the "Cadillac tax" on expensive insurance plans first until 2020 and later until 2022 and repealed it in late 2019.


An estimated 9 to 10 million people had gained Medicaid coverage in 2016, mostly low-income adults. The five major national insurers expected to lose money on ACA policies in 2016, in part because the enrollees were lower income, older and sicker than expected.


More than 9.2 million people (3.0 million new customers and 6.2 million returning) enrolled on the national exchange in 2017, down some 400,000 from 2016. This decline was due primarily to the election of President Trump. The eleven states that run their own exchanges signed up about 3 million more. The IRS announced that it would not require that tax returns indicate a person has health insurance, reducing the effectiveness of the individual mandate, in response to Trump's executive order. The CBO reported in March that the healthcare exchanges were expected to be stable. In May the House voted to repeal the ACA using the American Health Care Act (AHCA), but the AHCA was defeated in the Senate. The Tax Cuts and Jobs Act set the individual mandate penalty at $0 starting in 2019. The CBO estimated that the change would cause 13 million fewer people to have health insurance in 2027.


The 2017 Individual Market Stabilization Bill was proposed to fund cost-sharing reductions, provide more flexibility for state waivers, allow a new "Copper Plan" offering only catastrophic coverage, allow interstate insurance compacts, and redirect consumer fees to states for outreach. The bill failed.


By 2019, 35 states and the District of Columbia had either expanded coverage via traditional Medicaid or via an alternative program.


In popular culture

SNL presented a sketch in October 2009 about the legislation's gridlock, with Dwayne Johnson playing an angry President Obama confronting three senators opposing the plan.


The show aired another sketch in September 2013 with Jay Pharoah as President Obama rolling out the plan to the public, and Aaron Paul and other cast members playing ordinary Americans helping him in advocating for the legislation.


See also

References

Further reading

Preliminary CBO documents

CMS Estimates of the impact of P.L. 111-148

CMS Estimates of the impact of H.R. 3590

Senate Finance Committee meetings

Senate Finance Committee Hearings for the 111th Congress recorded by C-SPAN; also available from Finance.Senate.Gov (accessed April 1, 2012).


External links



The African humid period (AHP; also known by other names) was a climate period in Africa during the late Pleistocene and Holocene geologic epochs, when northern Africa was wetter than today. The covering of much of the Sahara desert by grasses, trees and lakes was caused by changes in the Earth's axial tilt, changes in vegetation and dust in the Sahara which strengthened the African monsoon, and increased greenhouse gases.
During the preceding Last Glacial Maximum, the Sahara contained extensive dune fields and was mostly uninhabited. It was much larger than today, and its lakes and rivers such as Lake Victoria and the White Nile were either dry or at low levels. The humid period began about 14,600–14,500 years ago at the end of Heinrich event 1, simultaneously to the Bølling–Allerød warming. Rivers and lakes such as Lake Chad formed or expanded, glaciers grew on Mount Kilimanjaro and the Sahara retreated. Two major dry fluctuations occurred; during the Younger Dryas and the short 8.2 kiloyear event. The African humid period ended 6,000–5,000 years ago during the Piora Oscillation cold period. While some evidence points to an end 5,500 years ago, in the Sahel, Arabia and East Africa, the end of the period appears to have taken place in several steps, such as the 4.2-kiloyear event.


The AHP led to a widespread settlement of the Sahara and the Arabian Desert, and had a profound effect on African cultures, such as the birth of the Ancient Egyptian civilization. People in the Sahara lived as hunter-gatherers and domesticated cattle, goats and sheep. They left archaeological sites and artifacts such as one of the oldest canoes in the world, and rock paintings such as those in the Cave of Swimmers and in the Acacus Mountains. Earlier humid periods in Africa were postulated after the discovery of these rock paintings in now-inhospitable parts of the Sahara. When the period ended, humans gradually abandoned the desert in favour of regions with more secure water supplies, such as the Nile Valley and Mesopotamia, where they gave rise to early complex societies.


Research history

In 1850 the researcher Heinrich Barth discussed the possibility of past climate change leading to increased wetness in the Sahara after discovering petroglyphs in the Murzuq Desert, as did Ahmed Hassanein following his 1923 exploration of the Libyan Desert when he saw depictions of savanna animals at Gabal El Uweinat. Further discoveries of petroglyphs led desert explorer László Almásy to coin the concept of a Green Sahara in the 1930s. Later in the 20th century, conclusive evidence of a past greener Sahara, the existence of lakes and higher Nile flow levels was increasingly reported and it was recognized that the Holocene featured a humid period in the Sahara.


The idea that changes in Earth's orbit around the Sun influence the strength of the monsoons was already advanced in 1921, and while the original description was partly inaccurate, later widespread evidence for such orbital controls on climate was found. At first it was believed that humid periods in Africa correlate with glacial stages ("pluvial hypothesis") before radiocarbon dating became widespread. Beginning in the 1970s, the humidification was attributed to precessional changes.


The development and existence of the African humid period has been investigated with archaeology, climate modelling and paleoproxies. with archaeological sites, Deposits left by wind, vegetation (e.g leaf wax), lakes and wetlands, and archaeological sites also played an important role. Pollen, lake deposits and former levels of lakes have been used to study the ecosystems of the African humid period, and charcoal and leaf impressions have been used to identify vegetation changes. Many unresolved questions concerning the AHP remain: its beginning, cause, intensity, end, land feedbacks, and the fluctuations during the period.


Recently, the hypothesized AHP end point of ~6000 years ago has been used experimentally in the Paleoclimate Modelling Intercomparison Project and the effects of the Sahara's greening on other continents has drawn scientific attention. The concept of a Sahara significantly different than today, and the rich record it left, has driven the imagination of the public and scientists alike. It has been used as an analogue for the drying of Mars after the Amazonian-Hesperian.


Research issues

While the precipitation changes since the last glacial cycle are well established, the magnitude and timing of the changes are unclear. Depending on how and where measurements and reconstructions are made, different beginning dates, ending dates, durations and precipitation levels have been determined for the African humid period. The amounts of precipitation reconstructed from paleoclimate records and simulated by climate modelling are often inconsistent with each other; in general, the simulation of the Green Sahara is considered a problem for earth system models. There is more evidence of the late phase of the AHP than its beginning. Erosion of lake sediments and carbon reservoir effects make it difficult to date when they dried up. Vegetation changes by themselves do not necessarily indicate precipitation changes, as changes in seasonality, plant species composition and changes in land use also play a role in vegetation changes. Isotope ratios such as the hydrogen/deuterium ratio that have been used to reconstruct past precipitation values likewise are under the influence of various physical effects, which complicates their interpretation. Most records of Holocene precipitation in eastern Africa come from low altitudes.


Terminology

The term "African humid period" (AHP) was coined in 2000 by Peter B. de Menocal et al. Earlier humid periods are sometimes known as "African humid periods" and a number of dry/wet periods have been defined for the Central Africa region. In general, these types of climate fluctuations between wetter and drier periods are known as "pluvials" and "interpluvials", respectively. The term "Green Sahara" is frequently used to describe the AHPs. Because the AHP did not affect all of Africa, some scientists have instead used and recommended "North African humid period" and "Northern African humid period".


Other terms that have been applied to the Holocene AHP or correlative climate phases are "Holocene humid period", which also covers an analogous episode in Arabia and Asia; "Early Holocene Humid Period"; "early to mid-Holocene humid episode"; "African Holocene Humid Period" (AHHP); " "Holocene Pluvial"; "Holocene Wet Phase"; "Kibangien A" in Central Africa; "Makalian" for the Neolithic period of northern Sudan; "Nabtian Pluvial", "Nabtian Wet Phase" or "Nabtian period" for the 14,000–6,000 humid period over the Eastern Mediterranean and Levant; "Neolithic pluvial"; "Neolithic Subpluvial"; "Neolithic wet phase"; "Nouakchottien" of the Western Sahara 6,500 – 4,000 years before present; "Subpluvial II" and "Tchadien" in the Central Sahara 14,000 – 7,500 years before present. The terms "Big Dry", "Léopoldvillien" and Ogolien  have been applied to the dry period in the last glacial maximum, the latter is equivalent to the "Kanemian"; "Kanemian dry period" refers to a dry period between 20,000 and 13,000 years before present in the Lake Chad area.


Background and beginning

The African humid period took place in the late Pleistocene and early-middle Holocene, and saw increased precipitation in Northern and Western Africa due to a northward migration of the tropical rainbelt. The AHP stands out within the otherwise relatively climatically stable Holocene. It is part of the so-called Holocene climatic optimum and coincides with a global warm phase, the Holocene Thermal Maximum. Liu et al. 2017 subdivided the humid period into an "AHP I" which lasted until 8,000 years ago, and an "AHP II" from 8,000 years onward, with the former being wetter than the latter.


The African humid period was not the first such phase; some evidence exists for as many as 230 older such "green Sahara"/wet periods going back perhaps to the first appearance of the Sahara 7–8 million years ago. Earlier humid periods appear to have been more intense than the AHP of the Holocene, including the exceptionally intense Eemian humid period. This humid period provided the pathways for early humans to cross Arabia and Northern Africa and which, together with later moist periods, has been linked to expansions of the Aterian populations and the speciation of insect species. Such humid periods are usually associated with interglacials, while glacial stages correlate to dry periods; they occur during precession minima, unless large ice sheets or insufficient greenhouse gas concentrations suppress their onset.


The Bølling–Allerød warming appears to be synchronous with the onset of the African humid period as well as to increased humidity in Arabia. Later, in the Blytt–Sernander sequence the humid period coincides with the Atlantic period.


Conditions before the African humid period

During the Last Glacial Maximum, the Sahara and Sahel had been extremely dry. The extent of dune sheets and water levels in closed lakes indicate that less precipitation fell than today. The Sahara was much larger, extending 500–800 kilometres (310–500 mi) farther south to about 12° northern latitude. Dunes were active much closer to the equator, and rainforests had retreated in favour of afromontane and savannah landscapes as temperatures, rainfall, and humidity decreased.


There is little and often equivocal evidence of human activity in the Sahara or Arabia at that time, reflecting their drier nature; in the Acacus Mountains the last human presence was recorded 70,000–61,000 years ago, and by then the LGM humans had largely retreated to the Mediterranean coast and the Nile Valley. The aridity during the Last Glacial Maximum appears to have been the consequence of the colder climate and larger polar ice sheets, which squeezed the monsoon belt to the equator and weakened the West African Monsoon. The atmospheric water cycle and the Walker and Hadley circulations were weaker as well. Exceptional dry phases are linked to Heinrich events when there are a large number of icebergs in the North Atlantic; the discharge of large amounts of such icebergs between 11,500 and 21,000 years before present coincided with droughts in the subtropics.


Before the onset of the AHP, it is thought that Lake Victoria, Lake Albert, Lake Edward, Lake Turkana and the Sudd swamps had dried out. The White Nile had become a seasonal river whose course along with that of the main Nile may have been dammed by dunes. The Nile Delta was partially dry, with sandy plains extending between ephemeral channels and exposed seafloor, and it became a source of sand for ergs farther east. Other lakes across Africa, such as Lake Chad and Lake Tanganyika, also had shrunk during this time, and both the Niger River and Senegal River were stunted.


Early humidity increases

Whether some parts of the desert such as highlands like the Red Sea Hills were reached by the westerlies or weather systems associated with the subtropical jet stream—and thus received precipitation—is contentious. It is only clearly supported for the Maghreb in northwestern Africa and parts of northeastern Africa, though river flow/terrace formation and lake development in the Tibesti and Jebel Marra mountains. Residual Nile flow may be explained in this way. The highlands of Africa appear to have been less affected by drought during the last glacial maximum.


The end of the glacial drought occurred between 17,000 and 11,000 years ago, with an earlier beginning noted in the Acacus, Sinai and Saharan mountains 26,500–22,500 and (possibly) 18,500 years ago, respectively. In southern and central Africa earlier starts 17,000 and 17,500 years ago, respectively, may be linked to Antarctic warming, while Lake Malawi appears to have been low until about 10,000 years ago.


High lake levels occurred in the Jebel Marra and Tibesti Mountains between 15,000 and 14,000 years ago and the youngest stage of glaciation in the High Atlas mountains took place at the same time as the Younger Dryas and early African humid period. Around 14,500 years ago, lakes started to appear in the arid areas.


Onset

The humid period began about 15,000–14,500 years ago. The onset of the humid period took place almost simultaneously over all of Northern and Tropical Africa, with impacts as far as Santo Antão on Cape Verde. Wet conditions apparently took about one to two millennia to advance northward in the Sahara and Arabia, respectively. The terrestrial system (e.g groundwater bodies) took time to respond to changed conditions.


Lake Victoria reappeared and overflowed; Lake Albert also overflowed into the White Nile 15,000–14,500 years ago and so did Lake Tana, into the Blue Nile. The White Nile flooded part of its valley and reconnected to the main Nile. In Egypt widespread flooding by the "Wild Nile" took place; this "Wild Nile" period led to the largest recorded floods on this river and sedimentation in floodplains. Even earlier, 17,000–16,800 years ago, meltwater from glaciers in Ethiopia – which were retreating at that time – may have begun to increase the flow of water and sediment in the Nile. In the East African Rift water levels in lakes began to rise by about 15,500/15,000-12,000 years ago; Lake Kivu began overflowing into Lake Tanganyika by about 10,500 years ago.


About the same time that the AHP started, the cold glacial climate in Europe associated with Heinrich event 1 ended with climate changing as far as Australasia. A warming and retreat of sea ice around Antarctica coincides with the start of the African humid period, although the Antarctic Cold Reversal also falls into this time and may relate to a drought interval recorded in the Gulf of Guinea.


Causes

The African humid period was caused by a stronger West African Monsoon directed by changes in solar irradiance and in albedo feedbacks. These led to increased moisture import from both the equatorial Atlantic into West Africa, as well as from the North Atlantic and the Mediterranean Sea towards the Mediterranean coasts of Africa and the Tibesti Mountains. There were complex interactions with the atmospheric circulation of the extratropics and between moisture coming from the Atlantic Ocean and the Indian Ocean, and an increased overlap between the areas wetted by the monsoon and those wetted by extratropical cyclones.


Climate models indicate that changes from a dry to a "green" Sahara and back have threshold behaviour, with the change occurring once a certain level of insolation is exceeded; likewise, a gradual drop of insolation often leads to a sudden transition back to a dry Sahara. This is due to various feedback processes which are at work, and in climate models there is often more than one stable climate-vegetation state. Sea surface temperature and greenhouse gas changes synchronized the beginning of the AHP across Africa.


Orbital changes

The African humid period has been explained by increased insolation during Northern Hemisphere summer. Due to precession, the season at which Earth passes closest to the Sun on its elliptical orbit – the perihelion – changes, with maximum summer insolation occurring when this happens during Northern Hemisphere summer. Between 11,000 and 10,000 years ago, Earth passed through the perihelion at the time of summer solstice, increasing the amount of solar radiation by about 8%, resulting in the African monsoon becoming both stronger and reaching farther north. Between 15,000 and 5,000 years ago, summer insolation was at least 4% higher than today. The obliquity also decreased during the Holocene but the effect of obliquity changes on the climate is focused on the high latitudes and its influence on the monsoon is unclear.


During summer, solar heating is stronger over the North African land than over the ocean, forming a low pressure area that draws moist air and precipitation in from the Atlantic Ocean. This effect was strengthened by the increased summer insolation, leading to a stronger monsoon that also reached farther north. The effects of these circulation changes reached as far as the subtropics.


Obliquity and precession are responsible for two of the foremost Milankovich cycles and are responsible not only for the onset and cessation of ice ages but also for monsoon strength variations. Southern Hemisphere monsoons are expected to have the opposite response of Northern Hemisphere monsoons to precession, as the insolation changes are reversed; this observation is borne out by data from South America. The precession change increased seasonality in the Northern Hemisphere while decreasing it in the Southern Hemisphere.


Albedo feedbacks

According to climate modelling, orbital changes by themselves cannot increase precipitation over Africa enough to explain the formation of the large desert lakes such as 330,000 square kilometres (130,000 sq mi) Lake Megachad, the climate proxies for precipitation, or the northward expansion of vegetation unless ocean and land surface changes are factored in.


Decreasing albedo resulting from vegetation changes is an important factor in the precipitation increase. Specifically, increased precipitation increases the amount of vegetation; vegetation absorbs more sunlight and thus more energy is available for the monsoon. In addition, evapotranspiration from vegetation adds more moisture, although this effect is less pronounced than the albedo effect. Heat fluxes in the soil and evaporation are also altered by the vegetation.


Reduced dust generation from a wetter Sahara, where major dust-generating regions were submerged by lakes, influences the climate by reducing the amount of light absorbed by dust. Decreased dust emissions also modify cloud properties, making them less reflective and more efficient at inducing precipitation. In climate models, reduced amounts of dust in the troposphere together with vegetation changes can often but not always explain the northward expansion of the monsoon. There is not universal agreement on the effects of dust on precipitation in the Sahel, however, in part because the effects of dust on precipitation may be dependent on its size.


In addition to raw precipitation changes, changes in precipitation seasonality such as the length of dry seasons need to be considered when assessing the effects of climate change on vegetation, as well as the fertilizing effects of increased carbon dioxide concentrations in the atmosphere.


Other sources of albedo changes:


Intertropical Convergence Zone changes

Warmer extratropics during summer may have drawn the Intertropical Convergence Zone (ITCZ) northward by about five or seven degrees latitude, resulting in precipitation changes. Sea surface temperatures off North Africa warmed under orbital effects and through weaker trade winds, leading to a northward movement of the ITCZ and increasing moisture gradients between land and sea. Two temperature gradients, one between a cooler Atlantic during spring and an already warming African continent, the other between warmer temperatures north of 10° latitude and cooler south, may have assisted in this change. In Eastern Africa, ITCZ changes had relatively little effect on precipitation changes. The past position of the ITCZ in Arabia is also contentious.


Precipitation changes in East Africa

The African humid period that took place in East Africa appears to have been caused by different mechanisms. Among the proposed mechanisms are decreased seasonality of precipitation due to increased dry season precipitation, shortening of the dry season, increased precipitation and increased inflow of moisture from the Atlantic and Indian Oceans. The Atlantic moisture inflow was in part triggered by a stronger West African and Indian monsoon, perhaps explaining why the effects of the AHP extended into the Southern Hemisphere.The behaviour of the easterly trade winds is unclear; increased moisture transport by easterly trade winds may have aided in the development of the AHP but alternatively a stronger Indian Monsoon that draws easterly winds away from East Africa may have occurred.


Changes in the Congo Air Boundary or increased convergence along this boundary may have contributed; the Congo Air Boundary would have been shifted east by the stronger westerly winds directed by lower atmospheric pressure over Northern Africa, allowing additional moisture from the Atlantic to reach East Africa. The parts of East Africa that were isolated from Atlantic moisture did not become significantly wetter during the AHP although at one site in Somalia the seasonality of precipitation may or may not have decreased.


Various contributing factors may have led to the increased humidity in East Africa, not all of which were necessarily operating simultaneously during the AHP. Finally, increased greenhouse gas concentrations may have been involved in directing the onset of the AHP in tropical southeastern Africa; there, orbital changes would be expected to lead to climate variations opposite to those in the Northern Hemisphere. The pattern of humidity changes in south-eastern Africa are complex.


Additional factors

Effects

The African humid period extended over most of Africa: The Sahara and eastern, southeastern and equatorial Africa. In general, forests and woodlands expanded through the continent. A similar wet episode took place in the tropical Americas and Asia, including the Makran region, the Middle East and the Arabian Peninsula; the episode appears to relate to the same orbital forcing as the AHP. An early Holocene monsoonal episode extended as far as the Mojave Desert in North America. In contrast, a drier episode is recorded from much of South America where Lake Titicaca, Lake Junin, the discharge of the Amazon River and water availability in the Atacama were lower.


The discharge of the Congo, Niger, Nile, Ntem, Rufiji, and Sanaga rivers increased. Runoff from Algeria, equatorial Africa, northeastern Africa and the western Sahara was also larger. Increased discharge led to changes in the morphology of the river systems and their alluvial plains, and the Senegal River expanded its riverbed, breached dunes and re-entered the Atlantic Ocean.


Flora and fauna of the Sahara

During the African humid period, lakes, rivers, wetlands and vegetation including grass and trees covered the Sahara and Sahel, creating a "Green Sahara" with a land cover that has no modern analogues. Evidence includes pollen data, archaeological sites, evidence of faunal activity such as diatoms, mammals, ostracods, reptiles and snails, buried river valleys, organic-rich mats, mudstones, evaporites as well as travertines and tufas deposited in subaqueous environments.


The vegetation cover then extended over almost all of the Sahara and consisted of an open grass savannah with shrubs and trees, with a moist savanna vegetation getting established in the mountains. In general, the vegetation expanded northward to 27–30° northern latitude in West Africa with a Sahel boundary at about 23° north, as the Sahara was populated by plants that today often occur about 400 kilometres (250 mi)-600 kilometres (370 mi) farther south. The northward movement of vegetation took some time and some plant species moved faster than others. Plants that perform C3 carbon fixation became more common. The fire regime has important effects on the vegetation and fauna; during the AHP, with some northern areas becoming wet enough that vegetation could sustain burns, and more southern areas becoming too wet.


Forests and plants from the humid tropics were concentrated around lakes, rivers and the Atlantic Ocean coast of Senegal; waterbodies were also settled by aquatic and partially aquatic plants and the Senegalese coast by mangroves. The landscape during the AHP has been described as a mosaic between various vegetation types of semi-desert, humid or tropical origin rather than a simple northward displacement of plant species, There was no southward displacement of Mediterranean plants during the Holocene and on the Tibesti Mountains cold temperatures may have restricted the expansion of tropical plants. Pollen data often show a dominance of grasses over humid tropics trees. The tree Lophira alata and others may have spread out of the African forests during the AHP, and the Lactuca plants may have split into two species under the effects of the AHP and other climate changes in Africa during the Holocene.


The Sahara climate did not become entirely homogeneous; its central-eastern parts were probably drier than the western and central sectors and the Libyan sand sea was still a desert although pure desert areas retreated to small core areas or became arid/semiarid. An arid belt may have existed north of 22° latitude and towards the Nile Delta, or the vegetation and the African monsoon might have reached 28–31° northern latitude; in general conditions between 21° and 28° northern latitude are poorly known. Dry areas may have persisted in the rain shadows of mountains and could have supported arid climate vegetation, explaining the presence of its pollen in sediment cores. In addition, north–south gradations in vegetation patterns have been reconstructed from charcoal and pollen data.


Fossils record changes in the animal fauna of the Sahara. This fauna included antelopes, baboons, birds, cane rats, catfish, clams, cormorants, crocodiles, elephants, frogs, gazelles, giraffes, hartebeest, hares, hippos, molluscs, Nile perches, pelicans, rhinoceroses, snake-eagles, snakes, tilapia, toads, turtles and many more animals, and in Egypt there were African buffaloes, spotted hyenas, warthogs, wildebeest and zebra. Additional birds include brown-necked raven, coot, common moorhen, crested grebe, glossy ibis, long-legged buzzard, rock dove, spur-winged goose and tufted duck. Some migratory birds may have changed their flying route in response to the AHP. Large herds of animals lived in the Sahara. Some animals expanded over the whole desert, while others were limited to places with deep water. Humid periods in the Sahara may have allowed species to cross the now-desert. A reduction in open grasslands at the beginning of the AHP may explain the decline of the populations of some mammals during and a population bottleneck in cheetahs at the start of the humid period, while leading to the expansion of the population of other animals such as Hubert's multimammate mouse and Natal multimammate mouse.


Lakes and rivers of the Sahara

A number of lakes formed or expanded in the Sahara and the Hoggar and Tibesti Mountains. The largest of them was Lake Chad which increased to at least ten times its present-day size to form Lake Megachad or Megalake Chad, then the largest lake on Earth. This enlarged Lake Chad reached dimensions of 1,000 by 600 kilometres (620 mi × 370 mi) in north–south and east–west direction respectively, covering the Bodélé Depression and perhaps as much as 8% of the present-day Sahara desert. It influenced the climate itself; for example rainfall would have been reduced at the centre of the lake and increased at its margins. Lake Chad was possibly fed from the north by rivers draining the Hoggar (Taffassasset drainage) and Tibesti Mountains, from the Ennedi Mountains in the east through the "Eastern palaeorivers" and from the south by the Chari-Logone and Komadugu Rivers. The Chari River was the main tributary while the rivers draining the Tibesti formed alluvial fans/the Angamma river delta at their entry into northern Lake Chad. Skeletons of elephants, hippos and hominins have been found in the Angamma delta, which is the dominant shoreline feature of northern Lake Chad. The lake overflowed into the Niger River during highstand through the Mayo Kebbi and the Benue River, eventually reaching the Gulf of Guinea. Older dune systems were submerged by Lake Chad.


Among the large lakes which may have formed in the Sahara are Lake Megafezzan in Libya and Lake Ptolemy in Sudan. Quade et al. 2018 raised some doubts about the size and existence of some of these lakes such as Lake Ptolemy, Lake Megafezzan, Lake Ahnet-Mouydir; it is possible that giant lakes only formed in the southern part of the Sahara. Other lakes are known from Adrar Bous in Niger, Era Kohor and Trou au Natron in the Tibesti Mountains, I-n-Atei in the Hoggar, at Ine Sakane and in Taoudenni in Mali, the Garat Ouda and Takarkori Lakes in the Acacus Mountains, Chemchane in Mauretania, at Guern El Louläilet in the Great Western Erg, and Hassi el Mejnah and Sebkha Mellala both in Algeria, at Wadi Shati and elsewhere in the Fezzan in Libya, at Bilma, Dibella, Fachi and Gobero in the Ténéré, Seeterrassental in Niger and at "Eight Ridges", El Atrun, Lake Gureinat, Merga, "Ridge", Sidigh, Wadi Mansurab, Selima and Oyo in Sudan. The lakes of Ounianga merged into two large lakes and overflowed, either above surface or underground. Mosaics of small lakes developed in some regions, such as the Grand Erg Occidental. Wetlands also expanded during the AHP, but both their expansion and subsequent retreat were slower than that of lakes. The Saharan topography prevents rapid drainage of accumulated water, thus favouring the development of waterbodies. The Niger River, which had been dammed by dunes during the LGM, formed a lake in the Timbuktu region that eventually overflowed and drained at some point during the AHP.


In some parts of the Sahara ephemeral lakes formed such as at Abu Ballas, Bir Kiseiba, Bir Sahara, Bir Tarfawi and Nabta Playa in Egypt, which may relate to later Egyptian religions, or swamp-lakes such as at Adrar Bous close to the Air Mountains. Ephemeral lakes developed between dunes, and a "freshwater archipelago" appears to have existed in the Murzuq basin. All these lake systems left fossils such as fish, limnic sediments and fertile soils that were later used for agriculture (El Deir, Kharga Oasis). Finally, crater lakes formed in volcanic fields such as Trou au Natron and Era Kohor in the Tibesti, and sometimes survive to this day as smaller remnant lakes such as Malha crater in the Meidob volcanic field. Potentially, the increased availability of water during the AHP may have facilitated the onset of phreatomagmatic eruptions such as maar formation in the Bayuda volcanic field, although the chronology of volcanic eruptions there is not well known enough to substantiate a link to the AHP.


Increased precipitation resulted in the formation or reactivation of river systems in the Sahara. The large Tamanrasset River flowed from the Atlas Mountains and Hoggar westward towards the Atlantic and entered it in the Bay of Arguin in Mauritania. It once formed the 12th largest watershed in the world and left a submarine canyon and riverine sediments. Together with other rivers it formed estuaries and mangroves in the Bay of Arguin. Other rivers in the same area also formed submarine canyons, and sediment patterns in marine sediment cores and the occurrence of submarine landslides in the area have been related to the activity of these rivers.


Rivers such as the Irharhar in Algeria, Libya and Tunisia and the Sahabi and Kufra rivers in Libya were active during this time although there is some doubt that they had perennial flow or that they reached the sea directly; they appear to have been more important in earlier humid periods. Small watersheds, wadis and rivers discharging into endorheic basins such as Wadi Tanezzuft also carried water during the AHP, leading to increased erosion. In Egypt, some rivers active during the AHP are now gravel ridges. In the Air, Hoggar and Tibesti Mountains, the so-called "Middle Terrace" was emplaced at this time. The rivers and lakes of the Sahara may have acted as pathways for the spread of humans and animals; animals that might have propagated through the Sahara in these waterbodies are Nile crocodile and the fish Clarias gariepinus and Tilapia zillii. The name Tassili n'Ajjer, which means "plateau of the rivers" in Berber, may be a reference to past river flows. On the other hand, intense flows of these rivers may have made their shores dangerous to humans and thus created additional impetus for human movement. Now-dry river valleys from the AHP in the eastern Sahara have been used as analogues for former river systems on Mars.


Humans of the Sahara

Conditions and resources were ripe for first hunter-gatherers, fishermen and, later, pastoralists; the exact chronology – when humans returned in the Sahara after the onset of the AHP – is disputed. They may have come either from the north (Maghreb or Cyrenaica) where the Capsian culture was located, the south (Sub-Saharan Africa), or the east (Nile Valley). The human population in the Sahara rapidly increased during the AHP, interrupted by a brief decline between 7,600 and 6,700 years ago. Traces of human activity have been found in the Acacus Mountains where caves and rock shelters were used as basecamps for humans, such as the Uan Afuda cave and the Uan Tabu and Takarkori rock shelters. The first occupation in Takarkori took place between 10,000 and 9,000 years ago; about five millennia of human cultural evolution are recorded there. At Gobero in the Ténéré desert a cemetery has been found, which has been used to reconstruct the lifestyle of these former inhabitants of the Sahara, and at Lake Ptolemy in Nubia humans settled close to the lake shore, using its resources and perhaps even engaging in leisure activities. At that time, many humans appear to have depended on water-bound resources, seeing as many of the tools left by the early humans are associated with fishery; hence this culture is also known as "aqualithic" although substantial differences between the cultures of various places have been found. The greening of the Sahara led to a demographic expansion and especially in the Eastern Sahara human occupancy coincides with the AHP. Conversely occupation decreased along the Nile valley, as it became inhospitable to human settlement due to flooding extending to the Nile delta. Humans moved into the Sudanese segment of the Nile valley only by about 11,000 years ago.


Humans were hunting large animals with weapons that have been found in archaeological sites and wild cereals occurring in the Sahara during the AHP such as brachiaria, sorghum and urochloa were an additional source of food. Humans also domesticated cattle, goats and sheep. Cattle domestication may have occurred especially in the more environmentally variable Eastern Sahara, where the lack of lakes (cattle having high requirements of drinking water) may however have limited the occurrence of cattle. Animal husbandry picked up in earnest around 7,000 years ago when domestic animals came to the Sahara, and a population boom may be linked to this change in cultural practice; cattle and goats spread southwestwards from northeasternmost Africa from 8,000 years before present. Dairying has been demonstrated in some locations and cattle-husbandry is supported by the frequent depiction of cattle in rock paintings. The relative importance of hunter-gatherer practices and pastoralism, and whether people were sedentary or migratory, is unclear. The Dufuna canoe, one of the oldest known ships in the world, appears to date to the Holocene humid period and implies that the waterbodies of that time were navigated by humans. The cultural units "Masara" and "Bashendi" existed in Dakhleh Oasis during the AHP. In the Acacus Mountains, several cultural horizons known as Early and Late Acacus and Early, Middle, Late and Final Pastoral have been identified while in Niger the Kiffian culture has been related to the beginning of the AHP. Ancient civilizations thrived, with farming and animal husbandry taking place in Neolithic settlements. Possibly, the domestication of plants in Africa was delayed by the increased food availability during the AHP, it only took place around 2,500 BC.


Humans created rock art such as petroglyphs and rock paintings in the Sahara, perhaps the largest density of such creations in the world. Scenes include animals and everyday life such as swimming which supports the presence of past wetter climates. One well-known such petroglyph location is the Cave of Swimmers in the Gilf Kebir mountains of Egypt; other well known sites are the Gabal El Uweinat mountains also of Egypt, Arabia and the Tassili n'Ajjer in Algeria where rock paintings from this time have been discovered. Humans also left artifacts such as Fesselsteine and ceramics in what today are inhospitable deserts. North Africa together with East Asia is one of the first places where pottery was developed probably under the influence of increased availability of resources during the AHP. The humid period also favoured its development and spread in West Africa during the 10th millennium BC; the so-called "wavy line" or "dotted wavy-line" motif was widespread across Northern Africa and as far as Lake Turkana. A similar but more circumscribed spread occurred with the Ounan arrow points.


These populations have been described as Epipaleolithic, Mesolithic and Neolithic and produced a variety of lithic tools and other assemblages. In West Africa, the cultural change from the African Middle Stone Age to the Late Stone Age accompanied the beginning of the AHP. In Sudan, the beginning of the early Khartoum culture coincides with the initiation of the AHP. Genetic and archaeological data indicate that these populations which exploited the resources of the AHP Sahara probably originated in Sub-Saharan Africa and moved north after some time, after the desert got wetter; in return, the AHP facilitated the movement of some Eurasian populations into Africa, and bidirectional travel across the Sahara more generally. The northward spread of Macrohaplogroup L and Haplogroup U6 genomic lineages may reflect this tendency but did not result in widespread genetic exchanges between Northern and sub-Saharan Africa.  Elsewhere, newly formed or expanded water courses may have restricted human mobility and isolated populations. These favourable conditions for human populations may be reflected in paradise myths such as the Garden of Eden in The Bible and Elysium and the Golden Age in Classical Antiquity, while a possible role in the development of the Afroasiatic and the spread of the Niger-Congo and Nilo-Saharan languages is debatable. References in Egyptian chronicles about moist lands along the Red Sea may record the wet conditions of the late AHP.


Additional manifestations in the Sahara

The expanded vegetation and soil formation stabilized previously active dunes, eventually giving rise to the present-day draa dunes in the Great Sand Sea of Egypt for example, although there is uncertainty about whether this stabilization was widespread. Soil development and biological activity in soils are attested in the Acacus Mountains and the Mesak Settafet area of Libya, but evidence of soil formation/pedogenesis such as bog iron or kaolinite formation are described from other parts of the Sahara and Sahel as well. In the Selima Sand Sheet, the landscape underwent erosional truncation and bioturbation. Erosion by increased runoff delayed soil development until after a few millennia after the onset of the AHP. The Central and Southern Sahara saw the development of alluvial deposits while sebkha deposits are known from the Western Sahara. Lightning strikes into soil left lightning-altered rocks in parts of the Central Sahara.


The increased precipitation recharged aquifers such as the Nubian Sandstone Aquifer; presently, water from this aquifer maintains several lakes in the Sahara, such as the Lakes of Ounianga. Other groundwater systems were active at that time in the Acacus Mountains, Air Mountains, in the Fezzan and elsewhere in Libya and the Sahel. Raised groundwater tables provided water to plants and was discharged in depressions, lakes, springs and valleys, forming widespread carbonate deposits and feeding lakes and wetlands.


The formation of lakes and vegetation reduced the export of dust from the Sahara. This has been recorded in marine cores, including one core where dust export decreased by almost half, and in Italian lakes. In coastal places, such as in Oman, sea level rise also reduced the production of dust. In the Mediterranean, a decreased dust supply was accompanied by increased sediment input from the Nile, leading to changes in marine sediment composition. Conversely, the increased vegetation may have yielded more volatile organic compounds in the air.


Whether the strengthening of the monsoon enhanced or reduced upwelling off Northwestern Africa is debatable, with some research suggesting that the strengthening in upwelling decreased sea surface temperatures and increased the biological productivity of the sea, while other research suggests that the opposite occurred; less upwelling with more moisture. However, regardless of whether upwelling increased or decreased, it is possible that the strengthening of the monsoon boosted productivity off the coasts of Northern and Western Africa because the increased river discharge delivered more nutrients to the sea. The decline of dust input may have caused the cessation of deep-water coral growth in the eastern Atlantic during the AHP by starving them of nutrients.


Arabia

Precipitation in Dhofar and southwestern Arabia is brought by the African monsoon, and a change to a wetter climate resembling Africa has been noted in southern Arabia and Socotra from cave and river deposits. It possibly reached as far as Qatar. Holocene paleolakes are recorded at Tayma, Jubbah, in the Wahiba Sands of Oman and at Mundafan. In the Rub al-Khali lakes formed between 9,000 and 7,000 years ago and dunes were stabilized by vegetation, although the formation of lakes there was less pronounced than in the Pleistocene. One of these lakes eventually overflowed to produce an outburst flood north of the Rub al-Khali. The Wadi ad-Dawasir river system in central Saudi Arabia became active again with increased river runoff into the Persian Gulf. Wadis in Oman eroded across LGM dunes and formed accumulation terraces. Episodes of increased river discharge occurred in Yemen and increased precipitation is recorded in the caves of Hoti, Qunf in Oman, Mukalla in Yemen and Hoq Cave in Socotra. Increased precipitation resulted in increased groundwater flow, generating groundwater-fed lakes and carbonate deposits.


Forests and wildfire activity expanded across parts of Arabia. Freshwater sources in Arabia during the AHP became focus points of human activity and herding activity between mountains and lowlands occurred. In addition, karstic activity took place on exposed coral reefs in the Red Sea and traces of it are still recognizable today. Increased precipitation has been also invoked to explain decreased salinities in the Red Sea, increased sedimentation and increased river inflow, while dust input decreased. Rock art depicts wildlife that existed in Arabia during the humid period. The Arabian neolithic coincides with the humid period, with archaeological sites such as cairns appearing with its beginning, and settlement of southern Mesopotamia during the Ubaid period may coincide with the wet epoch.


The humid period in Arabia did not last as long as in Africa, deserts did not retreat as much and precipitation may not have reached the central and northern part of the peninsula past Oman and the Yemen Highlands; northern Arabia remained somewhat drier than southern Arabia, droughts were still common and the land and still produced dust. One study has estimated that the amount of rainfall in the Red Sea did increase to no more than 1 metre per year (39 in/year). Whether some former lakes in Arabia were actually marshes is contentious.


East Africa

Nile discharge was higher than today and during the early African humid period, the Nile in Egypt flooded up to 3–5 metres (9.8–16.4 ft) higher than it did recently before flood control. The increased flooding may have turned the Nile Delta and Nile Valley marshy and inhospitable and could explain why many archaeological sites along the Nile were abandoned during the AHP, with violent conflicts taking place at Jebel Sahaba archaeological site. Early after the Younger Dryas, the Blue Nile would have been the major source of waters for the Nile. Waters from the Nile filled depressions like the Fayum Depression to form a deep lake with anoxic bottom waters and reaching 20 metres (66 ft) above sea level, probably once a geomorphic barrier was breached. Wetlands and anastomosing channels developed in the Nile Delta as sediment supply increased. In addition, Nile tributaries in northwestern Sudan such as Wadi Al-Malik, Wadi Howar and Valley of the Queens became active during the AHP and contributed sediments to the Nile. Wadi Howar was active until 4,500 years ago, and at the time often contained dune-dammed lakes, swamps and wetlands; it was the largest Saharan tributary of the Nile and constituted an important pathway into sub-Saharian Africa. Conversely it appears that Lake Victoria and Lake Albert were not overflowing into the White Nile for all of the AHP, and the White Nile would have been sustained by overflow from Lake Turkana. There appears to be a tendency over the course of the AHP for the discharge of the Blue Nile to decrease relative to that of the White Nile. The Blue Nile built an alluvial fan at its confluence with the White Nile, and incision by the Nile reduced flooding risk in some areas which thus became available for human use.


Closed lakes in East Africa rose, sometimes by hundreds of metres. Lake Suguta developed in the Suguta Valley, accompanied by the formation of river deltas where rivers such as the Baragoi River entered the lake. In turn, Lake Suguta overflowed into the Kerio River, this adding water to Lake Turkana where increased discharge by the Turkwel River led to the formation of a large river delta. The Omo River remained its principal inflow but the relative role of other water sources increased compared to present-day conditions. A 45 metres (148 ft) deep lake filled the Chew Bahir basin and together with Lakes Chamo and Abaya formed a river system flowing into Lake Turkana, which itself overflowed on its northwestern side through the Lotikipi Swamp into the White Nile. Deposits from this lake highstand form the Galana Boi Formation. The increased water depth reduced water mixing in Lake Turkana, allowing organic material to build up. This overflowing large lake was filled with freshwater and was populated by humans, typically in bays, along capes and protected shorelines; the societies there engaged in fishery but could probably also fall back on other resources in the region.


The Ethiopian Lake Abhe expanded to cover an area of 6,000 square kilometres (2,300 sq mi), much larger than the present-day lake, in the "Abhe IV"–"Abhe V" lake cycle. The enlarged lake covered a large area west of the present-day lake, present-day lakes Afambo, Gamari and Tendaho, reducing Borawli, Dama Ale and Kurub to islands, and possibly overflowed to the Red Sea. The maximum water level was reached during the early Holocene as river discharge increased, but was later limited by partial overflow and did not rise above 380 metres (1,250 ft) again. Deep thermal groundwater recharge occurred in the region. About 9,000 years of human occupation are documented at the lake. Archaeological sites indicate that people obtained resources from the lake and followed its rise and decline. The cultural traditions at Lake Abhe appear to be unusual by AHP/African standards.


Lake Zway and Lake Shala in Ethiopia joined with Lake Abiyata and Lake Langano to form a large waterbody which began overflowing into the Awash River. Nearly all lakes in the East African Rift were affected by the AHP: Lake Ashenge and Lake Hayq also in Ethiopia, Lake Bogoria, Lake Naivasha and Lake Nakuru-Lake Elmenteita which merged, all in Kenya, and Lake Masoko in Tanzania. Lakes formed in the caldera of the Menengai volcano and in the Chalbi region east of Lake Turkana; the lake covered an area of about 10,000 square kilometres (3,900 sq mi). A 1,600 square kilometres (620 sq mi) large and 50 metres (160 ft) deep Lake Magadi formed in the early Holocene, generating the "High Magadi Beds" sediments. This lake was fed by now-dry waterfalls and possibly from the neighbouring lake Koora. In the Danakil Depression of Ethiopia freshwater conditions became established. Lakes formed in depressions on the mountains around Lake Kivu. Some of these lakes became connected through overflow: Lake Nakuru-Elmenteita drained northward through the Menengai caldera, Baringo-Bogoria Suguta into Lake Turkana and from there into the Nile, carving gorges along the way. Lake Naivasha drained south through Lake Siriata into Lake Magadi-Natron. Overflow of several of these lakes allowed animals including Nile crocodiles and fish to propagate to the individual lake basins, but at the same time hindered the propagation of many land-based mammals. River systems in the southern Kenya Rift region became active.


Glaciers stopped retreating or briefly expanded in East Africa at the beginning of the AHP before continuing retreat. On Mount Kilimanjaro they may have expanded during the AHP after a phase during the Younger Dryas where the mountain was ice free, but the tree line also rose at that time, accompanied by soil formation. The wetter climate may have destabilized the neighbouring Mount Meru volcano, causing a giant landslide that removed its summit. In the Rwenzori Mountains, increased precipitation during the AHP has been linked to the occurrence of rockfalls.


Erosion in catchments of East Africa increased with the beginning of the humid period but then decreased even before its end, as the increased weathering led to the formation of soils, these in turn to the establishment of a vegetation cover that subsequently reduced additional erosion. Increased weathering resulted in the increased consumption of atmospheric CO2 during the AHP.


Surprisingly, and contrary to the patterns expected from precessional changes, the East African Rift also experienced a wetter climates during the AHP, reaching as far south as Lake Rukwa and Lake Cheshi into the Southern Hemisphere. In the region of the African Rift Valley and African Great Lakes, pollen evidence points to the occurrence of forests including rainforest vegetation due to the increased precipitation, while today they occur only in limited areas there. Denser vegetation also occurred at Lake Turkana, with wooden vegetation covering almost half of the dry land although grasslands remained dominant. Development of forest vegetation around the African Great Lakes created an interconnected environment where species spread, increasing biodiversity with effects on the future when the environment became fragmented. Vegetation cover also increased in the Afar region and Ericaceae plants spread at high elevations. Forests and moisture-requiring vegetation expanded in the Bale Mountains. Different types of vegetation, including dryland vegetation, existed at Lake Malawi and Lake Tanganyika however, and vegetation did not change much. The wetter climate led to the formation of the Halalee paleosoil in the Afar region and decreased wildfire activity in the Rwenzori.


In East Africa, the AHP led to improved environmental conditions in terms of food and water supply, allowing early human populations to survive, grow in size and settle new regions without requiring major changes in food gathering strategies. Pottery techniques such as the "dotted wavy line" and "Kanysore" are associated with fishing and foraging communities. In Somalia, the "Bardaale" lithic industry is linked to the AHP. Earlier wet and dry periods in East Africa may have influenced the evolution of humans and allowed their spread across the Sahara and into Europe. Animal species took advantage too, with midge populations increasing in Lake Victoria.


Other parts of Africa and the rainforest realm

Lake Bosumtwi in Ghana rose during the AHP. Evidence there also suggests a decrease in wildfire activity took place. Tropical forests expanded in Cameroon Highlands and the Adamawa Plateau of Cameroon and moved upward at Lake Bambili also in Cameroon, causing an upward shift of afromontane vegetation. The core of the rainforest was probably unaltered by the African humid period, perhaps with some changes in species and an expansion of their area. There is some evidence that an "Equatorial humid period", mechanistically linked to equatorial insolation and extending into the Amazon, may have taken place in the eastern Congo region at the same time as the AHP or around its beginning and end. The peatlands of Central Congo started developing during the African humid period and peat continues to accumulate there to this day, albeit with a slowdown in the Cuvette Centrale after the end of the African humid period. In the Gulf of Guinea, increased sedimentation and changed sedimentation patterns from increased river runoff decreased the activity of submarine cold seeps offshore present-day Nigeria.


On São Nicolau and Brava in the Cape Verde Islands, precipitation and erosion increased. In the Canary Islands, there is evidence of a moister climate on Fuerteventura, La Gomera and Tenerife, the laurel forests changed perhaps as a consequence of the AHP. Recharge of groundwater levels have been inferred from Gran Canaria also in the Canary Islands, followed by a decrease after the end of the AHP. Choughs may have reached the Canary Islands from North Africa when the latter was wetter.


Levant and Mediterranean

High latitude Africa has not undergone large scale changes in the past 11,700 years; the Atlas Mountains may have blocked the monsoon from expanding further north. However, soil and tufa, river valley and cave deposits showing a moister climate in southern Morocco, increased precipitation in the Algerian highlands, vegetation changes in the Middle Atlas, several floods in Tunisian rivers and ecosystem changes which impacted steppe-dependent rodents of Northern Africa have been linked to the AHP.


In the Pleistocene and Holocene humidity in the Mediterranean is often correlated to humidity in the Sahara, and the early-mid Holocene climate of Iberia, Italy, Negev and Northern Africa was wetter than today; in Sicily wettening correlates with ITCZ changes in Northern Africa.  Mediterranean precipitation is brought by Mediterranean cyclones and the westerlies; either increased precipitation from the westerlies, northward moisture transport from Africa or monsoonal precipitation extending into the Mediterranean may have rendered it wetter. While it is established, the nature of the connection between the African Monsoon and Mediterranean precipitation is unclear and it was winter rainfall that increased predominantly, although separating monsoonal and non-monsoonal precipitation can be difficult.


The Mediterranean Sea became less saline during the AHP, in part due to increased precipitation from the westerlies but also from increased river discharge in Africa, leading to the formation of sapropel layers when the increased runoff led to the Mediterranean becoming more stratified and eutrophied, with changes in the main water masses of the sea. The S1 sapropel layer is specifically associated with the AHP and with increased discharge of the Nile and other African rivers. These processes together with decreased dust transport by wind led to changes in the sediment patterns of the Mediterranean, and increased marine nutrient availability and food web productivity in the Mediterranean, which impacted the development of deep-sea corals.


In the Levant, wetter conditions during the AHP are recorded from Jeita Cave in Lebanon, Soreq Cave in Israel and desert varnish in the Negev, while the Dead Sea has variously been reported to have grown or shrunk during the AHP. Such a decline, if it took place, and a decline of other southern European lakes were low during this period. This is unlike some earlier wet periods in the Sahara, and whether the monsoon reached the southern Levant region during the AHP or there was increased winter precipitation is contentious. The northern Mediterranean may have been drier, with more wildfire activity, during the AHP, but increased summer precipitation in the European Alps has been associated with the AHP.


Southern Africa

The effects, if any, of the African humid period on Southern Africa have been unclear. Originally it was proposed that the orbitally driven changes would imply a dry period in Southern Africa which would have given way to moister conditions as the northern AHP ended, as the ITCZ should shift its average position between the two hemispheres. However, the lack of paleoclimatology data with sufficient time resolution from Southern Africa has made it difficult to assess the climate there during the AHP. More recently obtained paleoclimate data have suggested however that southern Africa was actually wetter during the AHP rather than drier, reaching as far as Rodrigues Island in the Indian Ocean and as far as the catchment of the Orange River. The area between Lake Tanganyika and Lake Malawi has been interpreted as the limit of the AHP's influence.


Conversely, and consistent with the opposite reaction pattern of the Southern Hemisphere, the Zambezi River reached its lowest discharge during the AHP, and precipitation in the Central African Plateau and Zambia decreases in computer simulations of a Green Sahara. Thus, the AHP may not have reach southern or southeastern Africa. There may have been opposite changes in precipitation between southeast Africa and tropical East Africa, separated by a "hinge zone". Particular changes occurred in central southern Africa, where a dry period co-occurred with an expansion of Lake Makgadikgadi; presumably the lake during this dry interval was nourished by increased wetness over the Okavango River catchment in the Angolan Highlands due to the AHP; peatlands formed in Angola during the AHP. In general there is little consistency between Northern and Southern Africa in terms of hydrological changes during the Holocene, and nowhere are both the start and end of the AHP apparent. Orbitally-mediated changes in Northern Hemisphere climate affected the Southern Hemisphere through oceanic pathways involving sea surface temperatures. Additionally, wetter periods unrelated to the AHP may have occurred after deglaciation in Southern Africa.


Numerical estimates

Estimates of the exact amount of increased precipitation vary widely. During the African humid period, Saharan rainfall increased to 300–400 millimetres per year (12–16 in/year), and values exceeding 400 millimetres per year (16 in/year) may have spread to 19–21° northern latitude. In the eastern Sahara, a gradient from 200 millimetres per year (7.9 in/year) increment in the north to 500 millimetres per year (20 in/year) in the south has been identified. An area with less than 100 millimetres per year (3.9 in/year) may have remained in the Eastern Sahara however,  although its driest parts may have received 20-fold more precipitation than today. Precipitation in the Sahara probably reached no more than 500 millimetres per year (20 in/year), with large uncertainty.


Other reconstructed values of the precipitation increase indicate an annual increase of about 150–320 millimetres (5.9–12.6 in) in Africa, with strong regional variation. From lake levels and other proxies, precipitation increases of 20–33%, 25–40%/23-45% or 50–100%/40–150% have been inferred for East Africa, with an increase of 40% reconstructed for Northern Africa. In the early Holocene, there appears to have been an eastward- and northward-decreasing trend of humidity. Additionally, at Tayma in Arabia a threefold increase appears to have occurred and precipitation in the Wahiba Sands of Oman may have reached 250–500 millimetres per year (9.8–19.7 in/year).


Effect on other climate modes

The El Niño–Southern Oscillation is a major climate variability mode. Paleoclimatology records from Ecuador and the Pacific Ocean indicate that during the early and middle Holocene ENSO variability was suppressed by about 30–60%, which can be only partially explained through orbital forcing. The Green Sahara may have suppressed ENSO activity, forcing a La Niña–like climate state, in one climate model this is accompanied by decreased upwelling and deepening of the thermocline in the Eastern Pacific as the Walker circulation expands westward. Easterly winds in the western Pacific Ocean increase, while they decrease in the eastern. In addition, Atlantic Niño sea surface temperature patterns develop in the Atlantic Ocean and the South Atlantic High weakens while the circulation of the South Atlantic Ocean changes. However, the existence of serious model bias in the depictions of the equatorial Atlantic sea surface temperatures is a problem for simulations of its past climate and the Atlantic Niño state may only occur during marginal AHP conditions, being suppressed during its maximum.


Remote effects of the AHP on climate have also been studied, although many changes are model-dependent and may also be inaccurate due to incorrect depictions of atmospheric dust distribution. Whether the reduced albedo of the Sahara during the AHP contributed to, or increased cloud cover counteracted, the warming of the Holocene thermal maximum is model-dependent; dust changes did not have a major effect. The AHP would also influence SSTs in the Indian Ocean, although there is not much evidence about the mid-Holocene sea temperatures there.


The AMOC transports heat from the Southern into the Northern Hemisphere and is implicated in starting the Holocene AHP an earlier AHPs after the end of an ice age. Various studies have been conducted to determine which effects reduced dust supply and the greening of the Sahara would have had on its intensity, with conflicting results on which effects would predominate. Increased heat transport either through the atmosphere or the ocean would result in warming in the Arctic.


Gaetani et al. 2024 found that Green Sahara climate simulations feature warming throughout the Northern Hemisphere and a strengthening of the westerlies and their precipitation in the Atlantic but a decline along the North American West Coast. There are also changes in the North Atlantic Oscillation during winter. The simulated temperature patterns, however, only poorly match temperature reconstructions.


Remote precipitation and the AHP

The Sahara greening intensified the Indian and Asian monsoons, warming and increased precipitation across most of the Tibetan Plateau especially late in the monsoon season, and climate simulations including a green Sahara reproduce the reconstructed palaeoclimates there better than these without. In a climate model, there is a shift in precipitation from snow to rain. The strengthened and expanding monsoons of Africa and Asia alter the atmospheric circulation of the planet, inducing a northward-shifted and wetter East Asian Monsoon and drying across tropical South America and central-eastern North America. In East Asia, a strengthened anticyclone over the West Pacific delivers more moisture to northeastern China and Indochina, and less to central and south-eastern China. The reduced dust emission warms the North Atlantic and increases westerly flow into the North American Monsoon, strengthening it. The far-field precipitation changes reach as far as Europe and Australia. Discrepancies between modelled and reconstructed northward extension and precipitation in the Asian monsoon regions and the North American Monsoon area may be explained through these remote effects.


Sun et al. 2020 proposed that the greening of the Sahara during the AHP can increase precipitation over the Middle East even if neither the African nor the Indian monsoons reach it. During spring, the increased vegetation forces anomalous atmospheric circulations that direct moisture transport from the Mediterranean, the Red Sea and eastern tropical Africa into the Middle East, increasing precipitation and agricultural productivity there. This could explain increased precipitation in the Middle East during the AHP: A wet climate occurred in the Middle East during the early Holocene, leading to the Ubaid period of settlement in Mesopotamia, followed by dry phases around 5,500 years ago and a concomitant reduction in simulated wheat yield.


Hurricanes and the AHP

One climate model has indicated that a greener Sahara and reduced dust output would have increased tropical cyclone activity, especially over the Atlantic but also in most other tropical cyclone basins. Changes in the intensity of the storms, decreases in wind shear, changes in atmospheric circulation and less dust in the atmosphere, which results in warmer oceans, are responsible for this phenomenon, while tropical wave activity may have increased or decreased. The net effect could be a global increase in tropical cyclone activity, a westward shift within the ocean basins and in the Atlantic Ocean a shift towards later dates. While there are no good paleotempestology data for the time of the African humid period that could confirm or refute this theory and many of these records are specific for particular locations, hurricane activity including past strikes in Puerto Rico and in Vieques appear to correlate with the strength of the West African Monsoon and increased precipitation on the northern Yucatan Peninsula during the middle Holocene could be explained by increased hurricane activity during the AHP. On the other hand, at Grand Bahama Bank and the Dry Tortugas of South Florida a decrease of hurricane activity took place during the AHP and dust emission is not always anti-correlated to hurricane activity. Finally, the northward movement of the ITCZ during the AHP may have caused a corresponding northward movement of tropical cyclogenesis areas and storm tracks in the Atlantic Ocean, which could also explain decreased hurricane activity in the Bahamas and Dry Tortugas.


Fluctuations

Climate variability during the AHP is poorly documented, but some gaps with less precipitation took place during the late glacial and the Holocene. During the Younger Dryas 12,500–11,500 years ago, the North Atlantic and Europe became much colder again and there was a phase of drought in the area of the African humid period, extending over both East Africa, where lake levels dropped in eastern Africa, southern Africa, equatorial Africa and West Africa. The dry interval extended to India and the Mediterranean where dune activity occurred in the Negev. At the end of the Younger Dryas, precipitation, lake levels and river runoff increased again, although south of the equator the return of humid conditions was slower than the relatively abrupt change to its north.


Another dry phase took place about 8,200 years ago, spanning East Africa and Northern Africa as documented by various lines of evidence such as decreased water levels in lakes. It coincided with cooling in the Northern Atlantic, in surrounding landmasses such as Greenland and around the world; the drought may be related to the 8.2 kiloyear event which separates the Greenlandian and Northgrippian stages of the Holocene and lasted for about one millennium. The 8,200 year event has also been noted in the Maghreb, where it is associated with a transition of the Capsian culture as well as with cultural changes both in the Sahara and the Mediterranean; at the Gobero cemetery a population change occurred after this dry interruption but the occurrence of widespread cultural changes appears to be questionable. This episode appears to have been caused by the draining of ice-dammed lakes in North America although a low latitude origin has also been suggested.


Cooling of the Northern Atlantic during Heinrich event 1 and the Younger Dryas associated with a weaker Atlantic meridional overturning circulation leads to atmospheric pressure anomalies that shift the African Easterly Jet and precipitation belts south, making Northern Africa drier. Storm tracks shift north away from the Mediterranean. Earlier Heinrich events were also accompanied by drought in North Africa. Likewise, a weakening of moisture transport and a less eastward position of the Congo Air Boundary contributed to reducing precipitation in East Africa although some parts of southern Africa at Lake Malawi were wetter during the Younger Dryas.


Many humidity fluctuations in the early Holocene appear to be caused by the discharge of meltwater from the Laurentide Ice Sheet into the Atlantic, which weakens the Atlantic meridional overturning circulation. Some dry periods in marine cores in the Gulf of Guinea appear to coincide with events recorded in Greenland ice cores. Other variations in precipitation observed in records have been attributed to solar activity changes, water levels of Lake Turkana for example appear to reflect the 11-year solar cycle.


In Lake Turkana, water level fluctuations took place between 8,500 and 4,500 years before present, with highstands before 8,400, around 7,000 and between 5,500 and 5,000 and lowstands around 8,000, 10,000 and 12,000 years before present. In total, five or six separate highstands are recorded in desert varnish around the lake, matching with phases of increased wetness in the Negev. The highstands appear to be controlled by sea surface temperature patterns in the Atlantic and Indian Oceans, but also by overflow of water from Lake Suguta and Chew Bahir and upstream lakes into Lake Turkana. Volcanic and tectonic phenomena occur at Lake Turkana, but do not have the magnitude required to explain large changes in lake level. Water level fluctuations have also been inferred for Lake Chad on the basis of pollen data, especially towards the end of the AHP. In the Taoudenni lake fluctuations of about a quarter-millennium have been recorded and frequent droughts occurred in the Eastern Sahara.


Other variations appear to have occurred 9,500–9,000 and 7,400–6,800 as well as 10,200, 8,200, 6,600 and 6,000 years before present; they were accompanied by decreased population density in parts of the Sahara, and other dry interludes in Egypt have been noted 9,400–9,300, 8,800–8,600, 7,100–6,900 and 6,100–5,900 years ago. The duration and severity of dry events is difficult to reconstruct and the impact of events like the Younger Dryas is heterogeneous even between neighbouring areas. During dry episodes, humans might have headed to waterbodies which still had resources, and cultural changes in the central Sahara have been linked to some dry episodes. Aside from fluctuations, a decrease in Nile discharge and southward retreat of the humid period may have been underway after 8,000 years ago with a major drought around 7,800 years ago.


End

The African humid period ended about 6,000–5,000 years ago; an ending date of 5,500 years before present is often used. After vegetation declined, the Sahara became barren and was claimed by sand. Wind erosion increased in northern Africa, and dust export from the now-desert and from dried up lakes such as the Bodélé Basin grew; Bodélé today is the largest single source of dust on Earth. The lakes dried up, mesic vegetation disappeared, and sedentary human populations were replaced by more mobile cultures. The transition from the "green Sahara" to the present-day dry Sahara is considered to be the greatest environmental transition of the Holocene in northern Africa; today almost no precipitation falls in the region. The end of the AHP but also its beginning could be considered a "climate crisis" given the strong and extended impact. Drying extended as far as the Azores, Canary Islands and southeastern Iran, and there is evidence of climate change on São Nicolau, Cape Verde.


The Piora Oscillation cold period in the Alps coincides with the end of the AHP; the period 5,600–5,000 years ago was characterized by widespread cooling and more variable precipitation changes around the world and was possibly forced by changes in solar activity and orbital parameters. It has also been named "Mid-Holocene Transition". Some changes in climate possibly extended into southeastern Australia, Central America and into South America. The neoglacial began. In the Chew Bahir basin, several short droughts may have "heralded" the end of the AHP; such short-term climatic fluctuations are common before a major climatic change.


A major pan-tropical environmental change took place about 4,000 years ago. This change was accompanied by the collapse of ancient civilizations, severe drought in Africa, Asia and the Middle East and the retreat of glaciers on Mount Kilimanjaro and Mount Kenya.


Chronology

Whether the drying happened everywhere at the same time and whether it took place in centuries or millennia is unclear in part due to disagreeing records and has led to controversy, and such a disagreement on timing also exists with respect to the expected vegetation changes. Marine cores usually indicate an abrupt change but not without exceptions while pollen data do not, perhaps due to regional and local differences in vegetation. Africa is a diverse landscape and groundwater and local vegetation can modify local conditions; groundwater-fed water bodies for example persisted longer than those nourished by rain. The debate on how quickly the Sahara formed goes back to 1849, when the Prussian naturalist Alexander von Humboldt suggested that only a quick drying could form the desert.


In the 2010s the idea has taken hold that the end of the African humid period occurred from north to south in a stepwise fashion. In northeastern Asia, the western Sahara and east Africa the humid period ended within 500 years with a one-step drying 6,000 – 5,000 years ago north of the present-day monsoon belt. Farther south, precipitation decrease was more protracted and closer to the equator the AHP ended between 4,000 and 2,500 years ago. In East Africa, pronounced drying occurred between 4,500 and 3,500 years ago, centered on 4,000 years ago; Egypt during the Old Kingdom was still wetter than today. A later end in northeast Africa about 4,000 years ago may reflect the different configuration of landmasses and thus monsoon behaviour, while other research has found a westward propagating drying trend. An earlier end by 6,100 years ago has also been suggested.


Some evidence points to a two-phase change in climate with two distinct dry transitions caused by the existence of two different steps of insolation decrease at which climate changes.
Distinct environmental changes may have occurred in Central Africa, Western Africa and East Africa. In Asia, an abrupt drying has been noted in numerous Chinese lakes. Finally, sometimes the 4.2 kiloyear event – the transition from the Northgrippian to the Meghalayan stage of the Holocene –  is considered to be the true end of the AHP, especially in central Africa.


Increased variability in precipitation may have preceded the end of the AHP; this is commonly observed before a sudden change in climate. In Gilf Kebir, between 6,300 and 5,200 years ago apparently a winter rainfall regime became established as the AHP ended. Later fluctuations in climate that produced brief humid spells also took place, such as moister periods 2,100 years before present in the western Sahel, between 2,200 - 1,500 years ago in Ethiopia and between 500 BCE – 300 CE in Roman Northern Africa and along the Dead Sea. By 2,700 years ago the central Sahara had become a desert and remained one until the present-day.


Sahara and Sahel

After a first brief lake level drop between 5,700 and 4,700 calibrated years ago that might reflect climate variability towards the end of the African humid period, water levels in Lake Megachad decreased quickly after 5,200 years before present. It shrank to about 5% of its former size, with the deeper northern Bodele basin drying up entirely about 2,000–1,000 years ago as it was disconnected from the southern basin where its major tributary, the Chari River, enters Lake Chad. The dried out basin was now exposed to the Harmattan winds, which blow dust out of the dry lake bed, making it the single largest source of dust in the world. Dunes formed in the dried-up Sahara and Sahel or began moving again after stabilizing during the AHP.


The tropical vegetation was replaced by desert vegetation, in some places suddenly and in others more gradually. Along the Atlantic coast, the vegetation retreat was slowed by a stage of sea level rise that increased soil moisture levels, delaying the retreat by about two millennia. A gradual decline has been noted in the Tibesti. In Libya at Wadi Tanezzuft the end of the humid period was also delayed by leftover water in dune systems and in the Tassili mountains until 2,700 years ago, when river activity finally ceased. A brief moist pulse between 5,000 – 4,000 years ago in the Tibesti led to the development of the so-called "Lower Terrace". The Egyptian Sahara might still have been vegetated until 4,200 years ago, based on depictions of savanna environments in Fifth Dynasty tombs in Egypt.


At Lake Yoa, which is groundwater-fed, vegetation decreased and the desert took over between 4,700–4,300 and 2,700 years ago, while the lake became hypersaline 4,000 years ago. Lake Teli dried out completely about 4,200 years ago. However, the climate of the Ounianga lakes may have been affected by the Tibesti Mountains and the end of the AHP thus delayed, and fossil groundwater left by the AHP nourishes the lake to this day. In the central Sahara, water resources in the mountains persisted longer.


East Africa and Arabia

In northern East Africa, water levels dropped rapidly about 5,500 years ago while in Hoti cave in Arabia a southward retreat of the Indian Monsoon took place about 5,900 years ago. Drying is also documented from Oman, and rivers and lakes of Arabia became intermittent or entirely dry. The Blue Nile basin became less moist with a noticeable decrease of Nile discharge about 4,000 years ago. Decreased discharge of the Nile led to the cessation of sapropel deposition and turbidite activity off its delta, the concentration/ abandonment of river channels in its delta and upstream and increased seawater influence in the delta.


Reconstructions from Lake Abiyata in Ethiopia suggest that the end of the African humid period took the form of severe droughts rather than a gradual decrease of precipitation. Drying in Arabia commenced about 7,000 calibrated years ago and there are large disparities in the timing between various parts of Arabia but a tendency towards an arid climate between 7,000 and 5,000 years ago has been observed which continued until 2,700 years ago. In the Bale Mountains and the Sanetti Plateau of Ethiopia vegetation changes signalling a drier climate took place around 4,600 years ago.


Forest cover in the area of the African Great Lakes decreased between 4,700 and 3,700 years ago, although drying at Lake Rukwa began 6,700 years ago and transition to saline conditions took place 5,500 years ago. At Lake Edward major changes in lake chemistry consistent with drying are noted 5,200 years ago. There a minor recovery in vegetation took place between 2,500 and 2,000 years ago, followed by a much more rapid appearance of grasses accompanied also by substantial wildfire activity. This might have been the most severe drought of the Lake Edward region in the Holocene, with many lakes such as Lake George dropping significantly or drying up altogether. Other lakes such as Nakuru, Turkana, Lake Chew Bahir, Lake Abbe and Lake Zway also dropped between 5,400 and 4,200 years ago. Decreased vegetation cover in the catchment of the Blue Nile has been correlated with increased sediment transport in the river beginning 3,600 – 4,000 years ago. 5,000 years ago, Lake Nabugabo separated from Lake Victoria owing to decreasing water levels. The expansion of savanna in East Africa has been linked to the end of the AHP.


The end of the AHP at Lake Turkana occurred about 5,000–5,300 years before present, accompanied by a lake level decline and the cessation of overflow from other lakes in its area into Lake Turkana. Between 5,000 and 4,200, Lake Turkana became more saline and its water levels decreased below the level of outflow to the Nile. Towards the end of the AHP water temperatures in the lake and in other regional lakes appear to have increased, followed by a drop after its end possibly resulting from the insolation seasonality pattern that was in force at the time of the end of the AHP. The decrease in water level was accompanied by a decrease in sedimentation rates and there is evidence that faulting and volcanism may have increased at Turkana due to the water level drop. The decrease of water levels in Lake Turkana also impacted the Nile and the Predynastic societies dependent on it.


Mediterranean

The southern Aegean, Libya and the Middle Atlas became gradually more dry, and drying in Morocco took place about 6,000 radiocarbon years ago, Drier conditions in Iberia and the Western Mediterranean accompanied the end of the African humid period between 6,000 and 4,000 years ago, perhaps as a consequence of increasingly frequent positive North Atlantic Oscillation episodes and the shift of the ITCZ More complicated changes have been found for the northern margin of the Mediterranean, and winter rainfall increased in the Levant at the end of the AHP. A 4.2 kiloyear event is recorded in dust records from the Mediterranean and might have been caused by changes in the circulation of the Atlantic Ocean.


Tropical West Africa

In Lake Bosumtwi the African humid period ended about 3,000 years ago after a brief moistening between 5,410 ± 80 years ago that ended 3,170 ± 70 years ago. This, earlier but similar changes off western Senegal and later but similar changes in the Congo Fan appear to reflect a southward shift of the precipitation zone over time. Some drying occurred simultaneously between the Sahel and the Gulf of Guinea. Some lakes in the Guineo-Congolian region dried out, while others were relatively unaffected.


A general tendency towards a drier climate is observed in West Africa at the end of the AHP. There, dense vegetation became progressively thinner between 5,000 and 3,000 years ago, and major perturbations of the vegetation took place between 4,200 and 2,400 years ago. A brief return of moister conditions took place 4,000 years ago while a substantial dry phase occurred between 3,500 and 1,700 years ago. Aridity became established between 5,200 and 3,600 years ago in the Sahara. In Senegal mangroves collapsed 2,500 years ago and modern-type vegetation arose about 2,000 years ago, aided by a decrease of sea level after the middle Holocene.


Central Africa

Farther south at the equator between 6,100 and 3,000 calibrated years before present savannah expanded at the expense of forests, with the transition possibly lasting until 2,500 calibrated years before present; a different time course estimate for the area between 4° southern and 7° northern latitude states that forest cover decreased between 4,500 and 1,300 years ago. In the Adamawa Plateau (Cameroon), the Ubangui Plateau (Central African Republic) and the Cameroon Volcanic Line montane forests disappeared at the end of the African humid period. In the Adamawa Plateau savanna has continuously expanded since 4,000 calibrated years ago. Such a change took also place in Benin and Nigeria between 4,500 and 3,400 calibrated years ago. Climate around the Gulf of Guinea became drier at the end of the AHP, although forests remained stable on Sao Tome. In the Congo Basin, there were changes in the composition and density of the forests rather than their extent, and along the equator precipitation may have increased around 4.2 ka. Many vegetation changes in the tropical regions were probably caused by a longer dry season and perhaps a smaller latitudinal range of the ITCZ.


Southern Hemisphere Africa

In the Southern Hemisphere at Lake Malawi drying began later – 1,000 years before present – as did the African humid period which there began only about 8,000 years ago. Contrarily, increased water levels in Etosha Pan (Namibia) appear to relate to a southward movement of the ITCZ at the end of the AHP although stalagmite growth data in Dante Cave also in Namibia has been interpreted as indicating a wetter climate during the AHP. Several records indicate that 5,500 years ago, precipitation changed in an east-west dipole-like way with drying in the west and moistening in the east. This pattern was probably driven by shifts in atmospheric moisture transport and of rain belt width.


Mechanisms

The end of the humid period appears to reflect the changes in insolation during the Holocene, as a progressive decrease of summer insolation caused the insolation gradients between Earth's hemispheres to decrease. However, the drying appears to have been much more abrupt than the insolation changes; it is not clear whether non-linear feedbacks led to abrupt changes in climate and it is also unclear whether the process, driven by orbital changes, was abrupt. Also, the Southern Hemisphere warmed and this resulted in a southward shift of the ITCZ; orbitally-driven insolation has increased over the Holocene in the Southern Hemisphere.


As precipitation decreased, so did vegetation, in turn increasing the albedo and further decreasing precipitation. Furthermore, vegetation may have responded to increased variations in precipitation towards the end of the AHP although this view has been challenged. This could have directed sudden changes in precipitation, although this view has been cast in doubt by the observation that in many places the end of the African humid period was gradual rather than sudden. Plants at higher and lower latitudes might respond differently to climate change; for example more diverse plant communities might have slowed down the end of the AHP.


Other proposed mechanisms:


The orbitally-induced changes of precipitation may have been modified by the solar cycle; specifically, solar activity maxima during the ending phase of the AHP may have offset the orbital effect and thus stabilized precipitation levels, while solar activity minima compounded the orbital effects and thus induced rapid decreases in water levels of Lake Turkana. At Lake Victoria on the other hand, solar variations appear to sometimes lead to drought and sometimes lead to wetness, probably due to changes in the ITCZ.


Potentially human-mediated changes

Major changes in vegetation in East Africa about 2,000 years ago may have been caused by human activity, including large-scale deforestation for iron production during the Iron Age. Similar changes have been observed on the Adamawa Plateau (Cameroon) but later dating of archaeological sites has found no correlation between human expansion in Cameroon and environmental degradation. Similar rainforest degradation across Western African took place between 3,000 and 2,000 years ago and the degradation is also known as "third millennium rainforest crisis". Climate-mediated processes may have increased the impact of land use changes in East Africa. In the Sudanian and Sahelian savannah on the other hand human activity seems to have had little impact, and in Central Africa forest changes were clearly triggered by climate change with little or no evidence of anthropogenic changes. The question has led to intense debate among paleoecologists and archaeologists.


While humans were active in Africa during the end of the African humid period, climate models analyzed by Claussen and colleagues 1999 indicate that its end does not need any human activity as an explanation although vegetation changes may have been induced by human activity and grazing. Later it was suggested that overgrazing may have triggered the end of the AHP around 5,500 years ago; human influence might explain why the Sahara became a desert without the accompanying onset of an ice age; usually the existence of a Sahara desert is associated with the expansion of high latitude glaciers. Later research has on the contrary suggested that human pastoralism may have actually delayed the end of the AHP by half a millennium as moving herds of animals driven by humans seeking good pasture conditions may lead to more balanced impacts of pastures on the vegetation and thus to greater vegetation quality. Which effects prevailed is still controversial. Increased grazing has been invoked to explain the increase in dust emissions after the end of the AHP. The effects of grazing on vegetation cover are context-dependent and hard to generalize over wider regions.


Global

A general drying tendency is observed in the northern tropics and between 5,000 – 4,500 calibrated years ago the monsoons weakened. Perhaps as a consequence of the end of the AHP, Asian monsoon precipitation declined between 5,000 and 4,000 years ago. A drought 5,500 years ago is recorded in Mongolia and eastern America, where drought conditions around 5,500–5,000 years ago occurred in places like Florida and between New Hampshire and Ontario. A drying tendency is also noted in the Caribbean and the Central Atlantic. The final retreat of vegetation from the Sahara may have helped cause the 4.2 kiloyear event.


Conversely, in South America there is evidence that the monsoon behaves in an opposite fashion consistent with precessional forcing; water levels in Lake Titicaca were low during the middle Holocene and began to rise again after the end of the AHP. Likewise, a trend towards increased wetness took place in the Rocky Mountains at this time although it was accompanied by a drier phase around Lake Tahoe, California and in the Western United States. Widespread climate change occurred around the North Atlantic at the time the AHP ended, and there are connections between North American and African climate. The end of the AHP may have reduced heat transport into the Arctic, causing cooling there.


Consequences

Humans

As observed in archaeological sites, settlement activity decreased in the Sahara after the AHP. Beginning from the north, population in Northern Africa decreased between 6,300 - 5,200 or 5,300 years ago, taking less than a millennium. In inner Arabia many settlements were abandoned about 5,300 years ago and there was a transition in Arabian monument construction. Some Neolithic people in the desert persisted for longer thanks to the exploitation of groundwater.


Different human populations responded to the drying in diverse manners, with responses in the Western Sahara being distinct from those in the Central Sahara. In the Sahara, subsistence and pastoralism replaced hunter-gatherer activity and a more nomadic lifestyle replaced semi-sedentary lifestyles as observed in the Acacus Mountains of Libya. Nomadic lifestyles also developed in the Eastern Sahara/Red Sea Hills in response to the end of the AHP. There was a shift in domestic animal use from cattle to sheep and goats as these are more suited in arid climates, a change reflected in rock art from which cattle disappeared at this time.


The development of irrigation systems in Arabia may have been an adaptation to the drying tendency. The decreased availability of resources forced human populations to adapt, in general fishing and hunting declined in favour of farming and herding. However, the effects of the end of the AHP on human food production have been subject to controversy.


The warm episode and coinciding drought may have triggered animal and human migration to less inhospitable areas and the appearance of pastoralists where previously fishery-dependent societies had existed, as happened at Lake Turkana. Humans moved to the Nile, where the society of Ancient Egypt with pharaohs and pyramids was eventually forged by these climate refugees perhaps reflecting renewed exuberance; thus the end of the AHP can be considered responsible for the birth of Ancient Egypt. Lower water levels in the Nile also aided the settlement of its valley as has been observed at Kerma. A similar process may have led to the development of the Garamantian civilization. Such human migrations towards more hospitable conditions along rivers and the development of irrigation also took place along the Euphrates, Tigris and Indus, leading to the development of the Sumerian and Harappan civilizations. During the so-called "Dark Millennium" between 6,000–5,000 years ago, people left the southern coast of the Persian Gulf for more hospitable areas in what is present-day Oman. Population shifts into mountain areas have also been reported for the Air Mountains, Hoggar and Tibesti. In other places, such as the Acacus Mountains, populations conversely remained in oases and hunter-gatherers also stayed in the Horn of Africa.


The Nile itself was not totally unaffected however; the 4.2 kiloyear event and the end of the AHP may be linked to the collapse of the Old Kingdom in Egypt when the Nile floods failed for three decades around 4,160 years before present and the final drying occurred. The ongoing decrease of precipitation after the end of the AHP could be the cause of the end of the Akkadian Kingdom in Mesopotamia. The end of the Garamantian civilization may also relate to climate change although other historical events were probably more important; at Tanezzuft oasis after 1,600 years ago it certainly relates to the drying trend.


In Central Africa, forests became discontinuous and savannahs formed in some places, facilitating the movement and growth of Bantu speaking populations; these in turn may have affected the ecosystem. The vegetation changes may have aided in the establishment of agriculture. The relatively slow decline of precipitation gave humans more time to adapt to the changing climate conditions. In East Africa, the beginning of the "Pastoral Neolithic" and the appearance of Nderit pottery have been attributed to the climatic changes at the end of the AHP.


Cultural changes may also have occurred as a consequence of climate change, such as changes in gender roles, the development of elites, the increased presence of human burials where formerly cattle burials predominated, as well as an increase of monumental architecture in the Sahara may have also been a response to increasingly adverse climates. A spread in cattle domestication at the time of climate change and as herders escaped the drying Sahara southwards may also relate to these events, although the details of the exact process by which cattle domestication spread are still controversial. Finally, changes in agricultural practices at the end of the AHP may be associated with the propagation of malaria and one of its causative pathogens Plasmodium falciparum; in turn these may correlate with the origin of human genome variants such as sickle cell disease that are linked to malaria resistance.


Non-human

In the Sahara, animal and plant populations were fragmented and restricted to certain favoured areas such as moist areas of mountain ranges; this happened for example to fish and crocodiles which only persist in isolated water bodies. Mediterranean plants such as cypresses too persist only in mountains, along with some reptiles that may have also been stranded in mountains by the drying. The whip spider Musicodamon atlanteus is probably also a relic of past wetter conditions. The development of human-specific populations of the malaria-transmitting mosquito Aedes aegypti coincides with the end of the AHP. The buffalo species Syncerus antiquus probably went extinct from the increased competition of pastoralists triggered by the climate drying. Goat populations in Ethiopia shrunk during the droughts that followed the end of the AHP lion and possibly barley habitat declined across Africa. The drying of the African Great Lakes region split gorilla populations into western and eastern populations, and a similar population split between the insect species Chalinus albitibialis and Chalinus timnaensis in Northern Africa and the Middle East may have also been caused by the expansion of deserts there. Some aquatic species disappeared from the Sahara. Giraffes, widespread in the Sahara during the AHP, may have been forced to migrate into the Sahel; this together with the separating effect of Lake Megachad may have influenced the development of giraffe subspecies. Climate change together with human impacts may have led to the extinction of a number of large mammals in Egypt, such as the hartebeest in the Sahara. In the Ruwenzori Mountains, vegetation changes may have been enhanced by climatic cooling. In northern Madagascar, wildlife declined after the end of the AHP even before the arrival of humans. On the other hand, the decline of tree cover may have grown the niche available to domestic animals, and modern Afromontane vegetation and some drought-tolerant plant species may have expanded their range.


The Dahomey Gap formed 4,500–3,200 years before present, correlative to the end of the AHP. The harbour porpoise declined in the Mediterranean due to a switch to oligotrophic conditions as discharge from African rivers decreased. Desert varnish formed on exposed rocks in the Sahara and at Lake Turkana in East Africa. Tectonic faulting and mass wasting eroded shorelines left by the AHP.


Global climate

The shrinkage of subtropical wetlands probably led to a drop in atmospheric methane concentrations between 5,500 and 5,000 years ago, before boreal wetlands expanded and offset the loss of subtropical wetlands, leading to a return of higher atmospheric methane concentrations. Conversely, increases in atmospheric methane concentrations, detected in Greenland ice cores about 14,700 years ago, and atmospheric carbon dioxide decreases in the early Holocene may relate to the vegetation expansion caused by the AHP. The concentration increase beginning 7,000 years ago may reflect increased aridity, although other processes were probably more important.


A sudden increase in the amount of land-originating dust in an oceanic drill core off Cape Blanc, Mauritania, has been interpreted as reflecting the end of the AHP 5,500 years ago occurring in only a few centuries. Increased African dust deposition took place at Ciomad, central Portugal and the Durmitor Massif, all in Europe. Potentially, alluvial sediments emplaced during the AHP and dried up lake basins became an important source for dust and silt-sized particles. Today, the Sahara is the single largest source of dust in the world, with far ranging effects on climate and ecosystems, such as the growth of the Amazon rainforest.


In one climate model, the desertification of the Sahara at the end of the AHP reduces the amount of heat transported in the atmosphere and ocean towards the poles, inducing cooling of 1–2 °C (1.8–3.6 °F) especially in winter in the Arctic and an expansion of sea ice. Reconstructed temperatures in the Arctic indeed show a cooling, although less pronounced than in the climate model. Further, this climate transition in the climate model is accompanied by increased negative Arctic Oscillation states, a weaker subpolar gyre and increased precipitation and cold air outbreaks in much of Europe; such changes have also been observed in paleoclimate data. These findings imply that the vegetation state of the Sahara influences the Northern Hemisphere climate. In turn, this high latitude cooling may have further reduced precipitation over Africa. Hou et al. 2024 proposed that the drying of the Sahara induced drying in northern China and moistening in southern China, through a cooling in the Indo-Pacific Warm Pool and an eastward shift of the Walker circulation. This was accompanied by cultural changes in China, with a decline in the number of archeological sites, and the Walker circulation change may be a causal mechanism for the 4.2 ka event.


Present-day situation

Presently, the African Monsoon still influences the climate between 5° south and 25° north latitude; the latitudes around 10° north receive the bulk of their precipitation from the monsoon during summer, with smaller amounts of rainfall occurring farther north. Thus farther north deserts can be found while the moister areas are vegetated. In the Central Sahara, annual precipitation reaches no more than 50–100 millimetres per year (2.0–3.9 in/year). Even farther north, the margin of the desert coincides with the area where the westerlies bring precipitation; they also influence southernmost Africa. Subsidence of air over parts of Northern Africa is responsible for the existence of deserts, which is further increased by the radiative cooling over the desert. Climate variability exists to this day, with the Sahel suffering from droughts in the 1970s and 1980s when precipitation decreased by 30% and the flow of the Niger River and Senegal River even more, followed by an increase of precipitation. The droughts are one of the most significant climate anomalies of the 20th century. Sea surface temperatures and feedbacks from land surface conditions modulate the strength of the monsoon and the droughts may have been triggered by sea surface temperature changes forced by anthropogenic aerosols. A large increase in dust fluxes after 1800 AD has been explained with changed agricultural practices.


In East Africa the monsoon leads to two rain seasons in the equatorial area, the so-called "long rains" in March–May and the "short rains" in October–November when the ITCZ moves northward and southward over the region, respectively; in addition to the Indian Ocean-sourced precipitation there is also Atlantic- and Congo-sourced precipitation west of the Congo Air Boundary. In Arabia, the monsoon does not penetrate far from the Arabian Sea and some areas are under the influence of winter precipitation brought by cyclones from the Mediterranean Sea. East Africa is also under the influence of monsoon circulations. South Africa has both monsoonal climates, winter precipitation climates and climates without clear precipitation seasonality.


Implications for future global warming

Some simulations of global warming and increased carbon dioxide concentrations have shown a substantial increase in precipitation in the Sahel/Sahara. This and the increased plant growth directly induced by carbon dioxide could lead to an expansion of vegetation into present-day desert, although it would be less extensive than during the mid-Holocene and perhaps accompanied by a northward shift of the desert, i.e. a drying of northernmost Africa. Such a precipitation increase may also reduce the amount of dust originating in Northern Africa, with effects on hurricane activity in the Atlantic and increased threats of hurricane strikes in the Caribbean, the Gulf of Mexico and the East Coast of the United States.


The Special Report on Global Warming of 1.5 °C and the IPCC Fifth Assessment Report indicate that global warming will likely result in increased precipitation across most of East Africa, parts of Central Africa and the principal wet season of West Africa, although there is significant uncertainty related to these projections especially for West Africa. In addition, the end of the 20th century drying trend may be due to global warming. On the other hand, West Africa and parts of East Africa may become drier during given seasons and months. Currently, the Sahel is becoming greener but precipitation has not fully recovered to levels reached in the mid-20th century.


Climate models have yielded equivocal results about the effects of anthropogenic global warming on the Sahara/Sahel precipitation. Human-caused climate change occurs through different mechanisms than the natural climate change that led to the AHP: Human-mediated climate change drives increased precipitation mainly through increased atmospheric moisture availability and disproportionate warming of the extratropics which strengthen the monsoon circulation, moderated by increased atmospheric stability, while the natural climate change is driven by a stronger monsoon circulation. The direct effect of heat on plants may be detrimental. Non-linear increases in vegetation cover are also possible, with several climate models showing abrupt increases when global temperatures rise by 2–4 °C (3.6–7.2 °F). One study in 2003 showed that vegetation intrusions in the Sahara can occur within decades after strong rises in atmospheric carbon dioxide but would not cover more than about 45% of the Sahara. That climate study also indicated that vegetation expansion can only occur if grazing or other perturbations to vegetation growth do not hamper it. On the other hand, increased irrigation and other measures to increase vegetation growth such as the Great Green Wall could enhance it. A 2022 study indicated that while increased greenhouse gas concentrations by themselves are not sufficient to start an AHP if greenhouse gas-vegetation feedbacks are ignored, they lower the threshold for orbital changes to induce Sahara greening.


Plans to geoengineer the Sahara to increase its vegetation cover and precipitation have been proposed since the 19th century. The mechanisms and consequences of the AHP are important context to evaluate such proposals and their ramifications; precipitation may increase but the consumption of carbon dioxide would be small and there could be detrimental impacts on climate and dust fluxes in the far-field. Building the Great Green Wall and large solar farms in the Sahara desert would also act to decrease its albedo and may trigger similar climate responses.


A greening of the Sahara on the one hand may allow agriculture and pastoralism to expand into hitherto unsuitable areas, but increased precipitation can also lead to increased water borne diseases and flooding. Expanded human activity resulting from a wetter climate may be vulnerable to climate reversals as demonstrated by the droughts that followed the mid-20th century wet period.


See also

Notes

References

Sources

Further reading



The Staten Island Railway (SIR) is the only rapid transit line in the New York City borough of Staten Island and is operated by the Staten Island Rapid Transit Operating Authority, a unit of the Metropolitan Transportation Authority. The railway was historically considered a standard railroad line, but today only the western portion of the North Shore Branch, which is disconnected from the rest of the SIR, is used by freight and is connected to the national railway system.


While the first rail proposal for rail service on Staten Island was issued in 1836, construction did not begin until 1855 after the project was attempted a second time under the name Staten Island Railroad. This attempt was successful due to the financial backing of William Vanderbilt. The line opened in 1860 and ran from Tottenville to Vanderbilt's Landing and connected with ferries to Perth Amboy, New Jersey and New York, respectively. After the Westfield ferry disaster at Whitehall Street Terminal in 1871, the railroad went into receivership and was reorganized into the Staten Island Railway Company in 1873. In the 1880s, Erastus Wiman planned a system of rail lines encircling the island using a portion of the existing rail line, and organized the Staten Island Rapid Transit Railroad in 1880, in cooperation with the Baltimore & Ohio Railroad (B&O), which wanted an entry into New York. B&O gained a majority stake in the line in 1885, and by 1890 new extensions to the line were in service. In 1890, the Arthur Kill Bridge opened, connecting the island to New Jersey. This route proved to be a major freight corridor. After a period of financial turmoil in the 1890s which saw both B&O and the Staten Island Rapid Transit Railroad company enter bankruptcy, the railroad was restructured as the Staten Island Rapid Transit Railway (SIRT), and was purchased by the B&O in 1899.


In 1924, SIRT began electrification of its lines, to comply with the Kaufman Act, which had become law the previous year. New train cars, designed to be compatible with subway service, were ordered, and electric service was inaugurated on the system's three branches in 1925. Through the 1930s and 1940s grade-crossing elimination projects were completed on the three branches. During World War II, freight traffic on the SIRT increased dramatically, briefly making it profitable. In 1948, the New York City Board of Transportation took over all of the bus lines on Staten Island, resulting in a decrease in bus fares from five cents per zone to seven cents for the whole island. Riders of the SIRT flocked to the buses, resulting in a steep drop in ridership. Service on the branches was subsequently reduced. In 1953, the SIRT discontinued service on the North Shore Branch and South Beach Branch. The South Beach Branch was abandoned shortly thereafter while the North Shore Branch continued to carry freight. While the SIRT threatened to discontinue service on the Tottenville Branch, the service was preserved as New York City stepped in to subsidize the operation. The last grade crossings on the line were eliminated in 1965. In 1971, New York City purchased the Tottenville line, and the line's operation was turned over to the Staten Island Rapid Transit Operating Authority, a division of the state-operated Metropolitan Transportation Authority (MTA). Freight service continued until 1991.


Improvements were made under MTA operations. The line received its first new train cars since the 1920s, and several stations were renovated. The MTA rebranded the Staten Island Rapid Transit as the MTA Staten Island Railway (SIR) in 1994. Fares on the line between Tompkinsville and Tottenville were eliminated in 1997 with the introduction of the MetroCard. In 2010, fare collection was reintroduced at Tompkinsville. A new station on the main line, Arthur Kill, opened in 2017, replacing the deteriorated Nassau and Atlantic stations. It was the first new station opened on the main line in seventy years. While the railway does not serve residents on the western or northern sides of the borough, light rail and bus rapid transit have been proposed for these corridors. Freight service in northwestern Staten Island was restored in the 2000s.


Corporate history

Was commonly known as the SIRT.


First line: 1836–1885

Initial efforts: 1836–1860

The railway's predecessor, the Staten Island Rail-Road Company, was incorporated on May 21, 1836. The charter called for the construction of a single or double-tracked line "commencing at some point in the town of Southfield, within one mile of the steamboat landing at the Quarantine, and terminating at some point in the town of Westfield; opposite Amboy." The proposed line would have run between the present-day locations of Clifton and Tottenville.: 7 : 225 : 4  The 13-mile (21 km) route had an estimated cost of $300,000.: 687  However, the company lost its charter in 1838 because the railroad was not built within two years after it was incorporated.: 1253–1254 


Attempts to start a rail line on the island were restarted in 1849 and 1850, when residents of Perth Amboy and Staten Island held meetings concerning a possible Tottenville-to-Stapleton line. Like the previous attempt in 1836, they faced financial difficulties, and sought out the help of William Vanderbilt—a son of Cornelius "Commodore" Vanderbilt and a resident of Staten Island. Vanderbilt had conceived of such a railroad as a way to reduce the monopoly of the Camden and Amboy Railroad, which was the only mean of reaching Philadelphia. Passengers would take ferries from New York City to Amboy, the railroad to Camden, and finally a ferry to Philadelphia. Vanderbilt thought his railroad would cut travel times—passengers would take a ferry from New York to Staten Island, then take his railroad, before taking a ferry to Amboy—and eliminate the monopoly of the Camden and Amboy between New York City and its terminus in Amboy. Since the plan conceived by the local residents followed Vanderbilt's proposed route, he helped charter the Staten Island Railroad Company (SIRR) on August 2, 1851, in order to build the rail line. The articles of association for the company were filed on October 18, 1851.: 27–28 


In 1852, the line was expected to cost $322,195. Two possible route options were considered; the first would start at New Ferry Dock in Stapleton, before passing through Rocky Hollow, following the valley between Castleton and Southfield Heights before descending to New Dorp. After going 4 miles (6.4 km) through the valley, the line would curve toward Amboy Road before curving southward, passing Billop House, and ending near Biddle's Grove and the Amboy ferries. The second route would start at Vanderbilt's Landing and run through Clifton to New Dorp.: 14–16 


The railroad's charter gave it two years to be built. Like the original plan for the line, however, difficulties were encountered. One problem was the acquisition of property for the line's right-of-way; many property owners refused to sell their land, blocking the proposed line. To prevent the loss of the line's charter, in 1853, the company successfully petitioned for a two-year extension to build the line. In 1853, a bill giving the company the right to operate ferries between New York and Staten Island was passed. Still facing difficulties, in January 1855 the company applied to the New York State Legislature for a three-year extension to complete the project. After all of the property required for the right-of-way was acquired, construction commenced in November 1855.: 7 : 1254–1255 : 444  The company, however, had run out of money to complete the line and asked Cornelius Vanderbilt—the sole Staten Island-to-Manhattan ferry operator—for a loan.: 4  Vanderbilt agreed to finance the railroad,: 225  but changed the northern terminal of the line from Stapleton to Vanderbilt's Landing, his ferry landing further east.: 461  Vanderbilt tried to stop competitors who had obtained a lease for the ferry at Vanderbilt's Landing before he could get a lease. He appointed James R. Robinson to build a structure to block his competitors, but on July 28, 1851, people tried to deconstruct the almost-finished structure and threatened to hurt Robinson if he tried to block them.: 4  In 1858, Cornelius's son William Vanderbilt was inducted onto the railroad's board of directors.: 7 


Opening: 1860

Stockholders and officials took an inaugural ride on the double-track line between Vanderbilt's Landing and Eltingville on February 1, 1860, and passenger operations began on April 23 that year.: 225 : 4  At Vanderbilt's Landing, passengers continued their passage to Manhattan by ferry. The first locomotive was named "Albert Journeay", after the railroad's president. A second locomotive was added to the line on May 5, 1860; it was named "E. Bancker" after the company's vice-president. The remainder of the line was expected to be completed in a month.


Over the next month, the remainder of the line was built between Annadale and Pleasant Plains as a single-track line, with a passing siding at Huguenot, passing through mosquito-infested land laced with peat bogs and quicksand—an area known locally as Skunk's Misery. It took a lot of time and wood to build the sub-roadbed with logs. It was viewed as unlikely to be a worthwhile investment to double-track the line because of the low passenger volume south of New Dorp. South of Pleasant Plains, the line was double-tracked. The line was extended to Annadale on May 14, 1860, and was completed to Tottenville on June 2, 1860, with a formal opening of the railroad.: 5  The completion of the line to Tottenville allowed passengers to transfer to a ferry that crossed the Arthur Kill and allowed passage to Perth Amboy, New Jersey.: 7 : 225 : 36  Initially, services made eleven stops between Vanderbilt's Landing and Tottenville. Many stations were named after nearby large farms, such as Garretson's and Gifford's. The stations built at Eltingville and Annadale—whose namesakes, the Eltings family and Anna Seguine, were influential in paying for the construction of the rail line—were the most elaborate. The arrival of the railroad gave dignity to some locations on Staten Island; "Poverty Hollow" was renamed Rosebank, Oakwood became Oakwood Heights, and other places were renamed with the coming of the railroad.


In August 1860, the railroad was extended from the depot at Vanderbilt's Landing to the wharf, which allowed passengers to walk directly to the boat from the train instead of walking 100 feet (30 m) along the sand. At the time, it took one and a half hours to get to Tottenville from Manhattan. Patronage of the rail line exceeded the greatest expectations of its projectors. On February 27, 1861, the New Jersey Locomotive Works gave notice of foreclosure on the two locomotives; Cornelius Vanderbilt interceded again and on September 4, 1861, the SIRR was placed into receivership with William Vanderbilt to prevent the loss of the locomotives and rolling stock to creditors.: 7 : 225 : 5  The Vanderbilts had taken stock in the railroad but in 1863, William Vanderbilt managed the receivership well enough for it to be discharged, with the debt paid off. As a result, the railroad became property of the Vanderbilts; facilities were enlarged under their leadership—an expansion made possible by increasing the capital stock to $800,000 from $350,000.


On October 5, 1861, in what might have been the first major accident on the railroad, Mary Austin, age 16, was killed by a train at Princess Bay as she crossed the tracks.


Ferry conflicts: 1860–1884

Ferry service between Perth Amboy and Tottenville began in 1863, operated by the SIRR. It was necessary to have a direct connection between the trains and the infrequent ferries to and from Manhattan, but this was difficult during the beginning of operation. The ferries serving Vanderbilt's Landing were owned by the attorney George Law. Vanderbilt tried to operate a ferry service between Manhattan and Staten Island that would compete with Law's. He also started construction on a central dock on the island, but he abandoned the scheme after a storm destroyed the timber work. Only the large stone foundation remained; it was still visible at low tide in 1900.: 462 


Vanderbilt was eventually forced to sell his ferry service to Law after a franchise battle.: 226  After the battle, Vanderbilt lost interest in transit operations on Staten Island and he handed the ferry and railroad operations to his brother Jacob H. Vanderbilt, who was the president of the company until 1883.: 7 : 462  In March 1864, William Vanderbilt bought Law's ferries, bringing both the railroad and the ferries under the same company.: 226  In 1865, the railroad took over operation of the New York & Richmond Ferry Company, and would later assume direct responsibility for operating the ferry service to Manhattan.: 7  The Perth Amboy and Staten Island ferries were taken over by the railroad under the leadership of Jacob Vanderbilt, who increased service.


The SIRR and its ferry line were making a modest profit until the boiler of the ferry "Westfield" exploded at Whitehall Street Terminal on July 30, 1871, killing 85 and injuring hundreds.: 7 : 226  As president of the railway, Jacob Vanderbilt was arrested but was not charged.: 101  As a result of the disaster, on March 28, 1872, the railroad and the ferry went into receivership.: 228 : 553  On September 17, 1872, the property of the company was sold to George Law in foreclosure,: 228  with the exception of the ferry "Westfield", which was purchased by Horace Theall.: 5 : 462  Some time after, Law and Theall sold the SIRR and ferry to the Staten Island Railway Company (SIRW). Law had threatened to form a company of his own if the stockholders did not come to his terms promptly, but a deal was reached. The charter for the SIRW was created on March 20, 1873, and on April 1, 1873, Law transferred the SIRR's property to the SIRW for $480,000.: 1255 : 462 : 569  The ferry operation was transferred to the newly formed Staten Island Railway Ferry Company. The ferry and rail services were split into separate rail companies to prevent problems with one from leading to the demise of the other, while ensuring that connecting service could still be provided for passengers.


During the American Civil War, a boat connected with the SIRW, the "Southfield", was sold to the government and converted into a gunboat; it was destroyed during an attack on Mississippi. In 1876, competition to the SIRW's ferry system emerged: Commodore Garner obtained possession of a ferry and ran the "D. R. Martin" on the East Shore. However, this ferry service was suddenly ended when Garner died. His boats were purchased by John Starin, who paid $5,000 for each of them, and obtained a franchise. He operated it until it was taken over by the Staten Island Rapid Transit Railroad Company (SIRTR) on August 1, 1884.: 462 : 70 


SIRT/B&O operation: 1880–1900

Organization: 1880–1884

By 1880, the SIRW was barely operational, and as a result New York State Attorney General Hamilton Ward sued to have the company dissolved in May that year. The suit said the company had become "insolvent in September 1872, to have then surrendered its rights to others, and have failed to exercise those rights". The legal proceedings commenced after an injunction was obtained that restrained the creditors of the railway company from proceeding against it until after the suit of the people was determined.: 229  Although it was floundering, the railway become the centerpiece of a plan to develop the island by a Canadian, Erastus Wiman. In 1867, Wiman arrived in New York to oversee the main office of Dun, Barlow, and Company in New York.: 36  Wiman became one of the most prominent residents of Staten Island after moving into a mansion there. He was dubbed the "Duke of Staten Island," and was interested in developing the island; Wiman recognized that to succeed he would need to build a coordinated transportation hub with connections to New York City and New Jersey.: 230 : 37  To this end the Staten Island Rapid Transit Railroad Company (SIRTR) was organized on March 25, 1880: 569  and incorporated on April 14, 1880.: 1257 


Wiman's plan called for a system encircling the island using two miles of the SIRW between Vanderbilt's Landing and Tompkinsville. His plan also called for the centralization of all ferries from one terminal, replacing the six to eight terminals active near what is now St. George. Wiman approached Robert Garrett, president of the Baltimore & Ohio Railroad (B&O), to back the plan, and Garrett agreed.: 7  The SIRTR began to seek legislation to acquire various rights-of-way needed to implement Wiman's plan. At that time, Wiman's company neither owned nor controlled a railroad; If it gained a charter to build connections, it would have had nothing to connect to. The SIRTR then began surveying for the proposed routes; in April 1881, it acquired 1.5 miles (2.4 km) of critical right-of-way directly from George Law.: 229  When Wiman explained his plan he secured a waterfront option from Law; however, Law refused to renew the option when it expired. To persuade Law to renew it, Wiman offered to name the place "St. George." Law was amused by the gesture and granted Wiman the option.: 4 : 8  In October 1882, Wiman made an application for a wharf to land passengers from the SIRTR's planned new ferry service to Manhattan.: 229 


Clarence T. Barrett, Henry P. Judah, and Theodore C. Vermilyen were appointed as commissioners to appraise the value of the land required by the SIRTR to extend the Staten Island Railway from Vanderbilt's Landing to Tompkinsville. Work on the line was delayed until the commissioners reported. The SIRTR filed a map of the proposed route at the office of the clerk of Richmond County. The line as planned would cross the lawn of Ms. Post on the North Shore of the island; on February 26, 1883, Mr. Franklin Bartlett and Mr. Clifford Bartlett, on behalf of Ms. Post, notified the court a change of route would be demanded.


On April 3, 1883, the SIRTR gained control of the SIRW and its boats. On the same date, at the annual meeting of the SIRW, Erasmus Wiman gained control of the railway by being elected to the board of Directors of the Railroad and becoming the railway's president.: 229  At the meeting, Wiman laid out his proposals for rail lines on Staten Island.: 37  He proposed extending the Staten Island Railway line to Hyatt Street in what is today Saint George. From there, a line would run through New Brighton and Snug Harbor along the island's North Shore. The line would then go inland, running parallel to the Kill Van Kull. Additional spur lines would have been built in the interior of the island based on where people settled. Wiman also proposed a bridge across Arthur Kill from Tottenville to Perth Amboy, replacing the ferry that operated there. This would have been part of a direct route between New York City and Philadelphia via Perth Amboy and South Amboy, with a new bridge over the Raritan River. In the days before the meeting, Wiman had gained 7,450 out of the 11,800 shareholders to elect him, surprising many of the directors of the railroad. By the end of the month, Wiman resigned from the SIRTR to avoid any conflict of interest. On June 27, 1883, a meeting of the directors of the SIRW and the SIRTR formally ratified the merger of the two companies under the leadership of Erastus Wiman, who was named president.: 229  On June 30, 1883, the SIRTR leased the SIRW for a term of 99 years, to become effective when the line opened between Clifton and Tompkinsville.: 1256 : 569  The line between Vanderbilt's Landing and Tottenville continued to be operated by the SIRW.: 1258 


While the control of the railroad included control of Vanderbilt's ferry, the North Shore Ferry was leased separately and was operated by Starin, whose lease was set to expire on May 1, 1884. On July 18, 1884, the SIRTR outbid Starin for the North Shore operation. As part of the purchase, ferry service would have been operated every forty minutes instead of every hour. The fare for the railroad and the ferry would have been ten cents, except between five and seven in the morning and evening, when it would have been seven cents. Starin continued to fight the lease in the courts for several years.: 230 


On May 5, 1885, the Perth Amboy Railroad was incorporated in New York to build a 2-mile-long (3.2 km) line connecting the SIRW in Tottenville to Perth Amboy in New Jersey by bridging over the Arthur Kill. The railroad was incorporated with a capital stock of $1 million. This railroad line was never built, and was possibly in interest of the Pennsylvania Railroad. The Perth Amboy Railroad was to be an alternate freight route to avoid congestion at Harsimus Cove and through Elizabeth.: 25 


Expansion: 1884–1900

A controlling interest in the SIRTR was obtained by the B&O in November 1885 through purchases of stock. On November 21, 1885, Robert Garrett, President of the B&O, leased the SIRTR to the B&O for 99 years, which gave the B&O access to New York, allowing it to compete with the Pennsylvania Railroad (PRR).: 8 : 230 : 37  Wiman needed the proceeds of the sale to pay for the construction of the North Shore Branch. The funds also helped pay for the construction of a bridge over the Kill Van Kull, the acquisition of 2 miles (3.2 km) of waterfront property, and for terminal facilities at St. George. In 1885, Jacob Vanderbilt retired as President of the SIRW. The new lines opened by the B&O were operated by the SIRTR, while the original line from Clifton to Tottenville was called the SIRW, which was maintained as a separate corporation.: 536  The passenger cars used by the SIRW were leased by the SIRTR.: 841 


Construction of the North Shore Branch began on March 17, 1884, after a number of legal proceedings; a party of surveyors started marking out the grades and broke ground for the roadbed.: 230  The rights of a horse car line to operate in Richmond Terrace were bought to build the line; the right of way followed the island's North Shore and reached a ferry to Elizabeth, New Jersey that had been operating since the mid-1700s. The B&O built about 2 miles (3.2 km) of rock fill out from shore and along the Kill Van Kull to deal with opposition from property owners in the neighborhood of Sailors' Snug Harbor, costing an additional $25,000.: 8 : 691  The company underwent a contest in litigation to acquire property for the line to pass over the cove at Palmer's run.: 691  Some properties in Port Richmond were acquired, displacing several home and business owners. A farm on the northwestern corner of Staten Island at Old Place—which was renamed Arlington by the B&O—was also purchased.


Grading work on the section between Clifton (previously Vanderbilt's Landing) and Tompkinsville began at this time, and during early 1884, construction continued with such energy that this section, which had been expected to open on September 1, opened on July 31 that year.: 7 : 690  The first train on the section contained the managers and officers, a few invited guests, and several passengers who had boarded prior to the train's arrival at Tompkinsville. The ride took three and a half minutes.: 690  The extension opened for passenger service on August 1, 1884. The opening of the line made the SIRTR's 99-year lease of the SIRW effective; under this agreement, the railroad to Tottenville and its properties became part of the rapid transit system.: 690 : 1256 


Wiman wanted to extend the line to St. George so all of the branches under the company's control could meet in one place and connect with the ferries to Manhattan. Most of the course of the line, however, had followed the shore along the bluffs, where ground had to be made upon to build the road. State laws could not grant the right to run a railroad through the property of the United States, hindering construction by the grounds of the lighthouse department near Tompkinsville. The company secured an Act of Congress permitting them to tunnel through a hill a near the shore. The grant for the tunnel was surrounded with restrictions that slowed progress. Construction of the tunnel began in 1885;: 7–8  it was 585 feet (178 m) long, and was protected by massive masonry walls on the sides and a brick-built arch 2 feet (0.61 m) in thickness overhead. The tunnel was wide enough to fit two trains side by side at a time. The cost of the project was $190,000.: 690–691  On November 16, 1884, Wiman, James M. Davis, Sir Roderick Cameron, Herman Clark, and Louis de Jonge incorporated the Saint George Improvement Company to handle the land and waterfront, which had been recently purchased from the estate of George Law. The new company was to handle the building of a new ferry terminal at Saint George.: 230 


Opening of the North Shore and South Beach Branches

On January 16, 1886, constructing engineers for the SIRTR said that if weather continued to be good, trains on the East Shore Branch from Clifton and North Shore Branch trains from Elm Park would be able to run to St. George Terminal by February 1.


The North Shore Branch was completed in 1885 and opened for service on February 23, 1886, with trains terminating at Elm Park. Travel times between Manhattan and Elm Park were reduced from 90 minutes with the old ferry system to 39 minutes.: 691  On March 7, 1886, the key piece of Wiman's plan, the St. George Terminal, opened; North Shore trains operated between Elm Park and St. George, and East Shore trains operated between St. George and Tottenville.: 37  In early 1886, in anticipation of the opening of the terminal and the consolidation of operations, the former Staten Island Railway stations from Clifton to Tottenville were upgraded from low-level platforms to high-level platforms to match the platforms on the new lines. In mid-1886, the North Shore Branch opened its new terminal at Erastina Place.: 6  In 1889–1890, a station was built at the South Avenue grade crossing at Arlington as the tracks were extended to the Arthur Kill Bridge. At Arlington, trains were reversed for their trip back to St. George. Even a few years after its opening, most trains terminated at Erastina.


A 1.7-mile-long (2.7 km) branch, then known as the Arrochar Branch, was opened to Arrochar on January 1, 1888, as a double-tracked line.: 257–258  The branch split off at Clifton Junction; it had two stops—Fort Wadsworth and Arrochar. In its first year, the branch carried heavy traffic, especially during the summer months.: 257–258  As evidenced by a map from 1884, the South Beach Branch was originally intended to run to Prominard Street in Oakwood Beach. The extension, however, was not built because the SIRTR could not gain the Vanderbilt family's approval to cross their New Dorp Beach farm. Instead, the line was only built as far as South Beach. During fiscal year 1893, the SIRTR purchased land to extend the line 1.75 miles (2.82 km) to South Beach and the 2.3-mile (3.7 km) South Beach Branch was completed in 1894.: 1259 


Improvements on the line to Tottenville

In 1886, Grasmere station opened on the Tottenville Line for a cost of $555.35.


To improve service, under B&O control, a large portion of the line was double-tracked. A second track was built between New Dorp and a point near Clifton in 1887 and 1888.: 257  Two new stations, Garretson and New Dorp, were opened the following year.: 1259  In 1893, to pay for more improvements—including double-tracking and a new station at Tottenville—the SIRW issued a mortgage. In 1895, land in St. George and Stapleton was acquired for yard space and station use.: 482  Between June 1895 and December 1895, the line was double tracked to Annadale. Between 1896 and 1899, the portion of the 12.64-mile (20.34 km) line that was double tracked was increased from 5.8 miles (9.3 km) to 10.04 miles (16.16 km).: 536 : 485 


In 1896, the terminal at Tottenville was moved 600 feet (180 m) to provide closer connections to the Perth Amboy Ferry and to provide new ferry slips. The terminal had been located on the east side of Main Street but as part of the work it was moved to Bentley Street. The change had a negative effect on local businesses, changing the character of Main Street and marking a decline of its commercial viability. To build the new terminal, property had to be acquired.: 1255 : 847  In 1910, the SIRW stopped using the land for the old ferry docks at Main Street.: 834 


Extension to New Jersey

The B&O made various proposals for a railroad between Staten Island and New Jersey. The accepted plan consisted of a 5.25 miles (8.45 km)-long section from the Arthur Kill to meet the Jersey Central at Cranford, through Roselle Park and Linden in Union County. In October 1888, the B&O created the subsidiary Baltimore and New York Railway (B&NY) to build the line, which was to be operated by the SIRTR. Construction started in 1889 and the line was finished later that year.: 8  After three years of effort by Wiman, Congress passed a law on June 16, 1886, authorizing the construction of a 500-foot (150 m) swing bridge over the Arthur Kill. The start of construction was delayed for nine months because it awaited approval of the Secretary of War,: 8  and another six months due to an injunction from the State of New Jersey. Construction had to continue through the brutal winter of 1888 because Congress had set a completion deadline of June 16, 1888; two years after signing the bill.: 37–38  The bridge was completed three days early on June 13, 1888.


When it opened, the Arthur Kill Bridge was the largest drawbridge ever constructed; it cost $450,000 and was constructed without fatalities. The bridge consisted of five pieces of masonry, the center one being midstream with the draw resting on it. The bridge's drawspan was 500 feet (150 m), the fixed spans were 150 feet (46 m), and there were clear waterways of 208 feet (63 m) on either side of the draw, making the bridge 800 feet (240 m) wide. The bridge was 30 feet (9.1 m) above the low water mark. Construction of the draw needed 656 tons of iron, and 85 tons were needed for each of the approaches. Trains were planned to start running on the bridge by September 1, but because the approaches were not finished, this was delayed until January 1, 1890,: 8  when the first train from St. George to Cranford Junction crossed the bridge. Because the land for the approaches was low and swampy, 2 miles (3.2 km) of elevated structure was built; 6,000 feet (1,800 m) on Staten Island and 4,000 feet (1,200 m) in New Jersey. The North Shore Branch was opened to freight traffic on March 1, 1890.: 1259–1260  On July 1, 1890, all of the B&O's freight traffic started using the line. Freight service began running through from Baltimore to St. George, running via the Reading Railroad and the Central Railroad of New Jersey between Park Junction in Philadelphia and Cranford Junction in New Jersey.


The B&O paid the SIRTR 10 cents-per-ton trackage to use the line from Arthur Kill to St. George.: 537  Once the Arthur Kill Bridge was completed, pressure was brought upon the United States War Department by the Lehigh Valley Railroad and the PRR to have the newly built bridge torn down and replaced with a bridge with a different design, claiming it was an obstruction for the navigation of coal barges past Holland Hook on the Arthur Kill. Their efforts were not successful.: 37 


In September 1890, Wiman secured the rights for a tunnel between Brooklyn and Staten Island; these tunnel rights were acquired by the New Jersey and Staten Island Junction Railroad Company. In May 1900, the PRR and other railroads secured an informal agreement to use the North Shore Branch from the Arthur Kill Bridge and the tunnel rights for a tunnel to 39th Street in Brooklyn. This was intended to allow freight trains to travel directly between Boston and Washington.


Reorganization

The B&O was bankrupt by February 1896; in its attempt to reach the New York market, its western lines fell into disrepair. J.P. Morgan replaced the railroad's top management and refinanced it. The new terminal at St. George was completed in 1896 after work was contracted for the project in fiscal year 1893.: 569  The building was designed by the architects Carrere and Hastings, and was built with ironwork framing. At the time, it was the largest terminal in the United States to have ferry, rail, vehicular, pedestrian and trolley services. Trolley companies on Staten Island insisted on access to the new terminal, but were rebuffed by the B&O. The issue went to court, and the B&O ended up splitting the cost for the trolley terminal and the long viaduct with the trolley operators. Prior to October 1897 passengers placed their tickets into ticket choppers at stations to pay their fare. Afterwards, conductors collected tickets.


In 1892, trolley service was inaugurated on Staten Island; it attracted passengers from the SIRTR, ending the railroad's monopoly. As a result, the railroad went into bankruptcy. On April 20, 1899, the railroad company and all of the real and personal property held in the company was sold at auction for $2,000,000 to representatives of the B&O.: 464  The railroad already owned the line from Elizabethport, New Jersey to South Beach, including the Arthur Kill Bridge. At the time, it was rumored the B&O trains would be rerouted from Communipaw station to Saint George. There was no change in the SIRTRC's management after the purchase. On July 1, 1899, the SIRTR defaulted on its payment of interest on its second mortgage bonds, and its lease of the Staten Island Railway ended on July 14 when it was put into receivership.: 780  On July 31, 1899, the Staten Island Rapid Transit Railway Company—also shortened to Staten Island Rapid Transit, or SIRT–was incorporated for the purpose of operating the SIRTR, with the transaction taking place on August 1, 1899.: 511  The section of the SIRT's line between St. George and Clifton Junction was jointly operated with the SIRW.: 1246, 1257, 1250, 1262 


Modernization: 1900–1949

Pennsylvania Railroad control: 1900–1913

Pennsylvania acquisition

Improvements were made to the SIRT after the Pennsylvania Railroad (PRR) under the leadership of president Alexander Cassatt took control of the B&O. Cassatt was named president of the PRR in 1899, and he allied with the New York Central Railroad for a "community of interest" plan. Cassatt wanted to end the rebate practice being undertaken by Standard Oil and Carnegie Steel—both larger shippers—that kept the freight rates extremely low. To achieve this, the two railroads bought stock in smaller, weaker trunk line railroads. The New York Central bought stock in the Reading Company, while the PRR bought stock in the Chesapeake and Ohio Railway, the Norfolk Southern Railway, and in the B&O—including the SIRT and the ferries on Staten Island. The plan worked; the average freight rate for the two companies rose. Cassatt first purchased B&O stock in 1899, most of it being under PRR control by 1901. After the PRR took more direct control of the B&O, including the SIRT; in May 1901, improvements were made to the rail line. PRR control of the line decreased as a new PRR president had different priorities, and in 1906, the PRR sold half of its B&O stock to the Union Pacific Railroad. The remainder of the PRR's stock in the B&O was sold to the Union Pacific in 1913.: 194–195, 199–200 


Improvements

Under PRR control, the B&O was profitable again and became a stronger railroad. The PRR allowed the B&O's newly developed properties to remain intact. On October 13, 1902, the SIRT started a trial passenger service from Plainfield station in New Jersey to St. George, running via the Jersey Central past Cranford Junction. The SIRT operated four trains every day, except for Sunday, with direct connections with the B&O's Royal Blue service between New York City and Washington, D.C. at Plainfield. These trains consisted of a locomotive and two passenger coaches. While this service was in operation the B&O sold tickets for its main line trains at the railroad's ferry terminals in Brooklyn, at South Ferry, and at St. George. Four passengers alighted at Plainfield on the first trip. On November 3, 1902, it was announced that a complete schedule would be arranged to Plainfield, to take effect on November 22, 1902. The service was discontinued in 1903 because it was unprofitable. The PRR bought four large double-decker steamers to halve the travel time on the Staten Island Ferry. Even though the PRR improved ferry service, the B&O was ejected from the Whitehall Street terminal on October 25, 1905, when the city took ownership of the ferry and terminals. The city built a new St. George Terminal for $2,318,720.: 29 


In September 1906, the Jersey Central purchased 20 feet (6.1 m) of property on the north side of its track between Plainfield and Cranford to enable to construct a fifth track to accommodate B&O freight trains to Staten Island, of which several traveled westbound from Cranford Junction between 6 p.m. and 8 p.m.. The track was intended to allow freight traffic, which often had to wait at Cranford Junction for multiple hours to make way for local passenger trains that were using the freight track to stay out of the way of express trains, to keep moving.


The PRR increased the number of daily trips to 28, and in 1902, it began contemplating the electrification of the rail line. The PRR's investment in the southern portion of the Perth Amboy sub-division was credited for the increased development of the South Shore of Staten Island. As such, in about 1902, a new station was constructed at Whitlock to serve a new community being built by the Whitlock Realty Company on the South Shore. The development company incentivized prospective buyers to bid on newly built houses by promising a year's free commuting between Manhattan and Whitlock for the first 25 houses. In December 1912, the SIRT petitioned the Public Service Commission (PSC) to allow the railway to abandon the station and replace it with a station named Bay Terrace 1,594 feet (486 m) to the south. The change was made, anticipating a shift in the center of population in the community. A hearing to hear the application was held by the PSC on December 18.


On February 21, 1907, the Staten Island Railway petitioned the Public Service Commission (PSC) to get permission to move the Dongan Hills stop from its location south of Seaview Avenue to a location 100 feet (30 m) to the north in between Seaview Avenue and Garretson Avenue. On March 12, 1907, the PSC granted its permission.


In September 1909, the New York State Public Service Commission (PSC) allowed the B&O to acquire 227 shares of the Staten Island Railway's capital stock, giving the B&O all of the railway's stock except for the few shares that had to be held by officers. Practically all of the Staten Island Railway's stock had been purchased by the B&O by 1906.


After 1900, several new houses were built in the community of Annadale and several parts of the Little Farms development. In 1910, as part of the development, the building company built a new railroad station. As a result, on March 22, 1910, the SIRT petitioned the PSC to allow it to discontinue its service at Annadale station and replace it with a new station of the same name 450 feet (140 m) to the west.: 189  On November 18, 1910, trains started stopping at a new station at Annadale that was built by the Wood Harmon Company the previous summer, which was located on the eastbound side of the track. This station replaced the station on the westbound track. As part of the construction of the new station, the operation of the switches where the line narrowed from two tracks to one began to be done from the station instead of being done manually. In addition, in 1910, a new freight house went into operation at Clifton, and new 75 pounds (34 kg) rails were installed as far south as Richmond Valley on the Perth Amboy Division. These new rails, which were 8 pounds (3.6 kg) heavier than the rails already in place on the southern section of the Perth Amboy Division, were expected to reduce the jarring of cars.


The PSC held a public hearing on December 16, 1910, to consider a proposal to eliminate grade crossings at Crook's crossing on Amboy Road in Great Kills, Clove Road in Grasmere, and Amboy Road in Huguenot. Commissioner of Public Work L.L. Tribus and Borough President Cromwell had started the push to eliminate grade crossings in Staten Island, which was approved by officials of the Staten Island Railway. The Board of Estimate had approved an appropriation to create a tentative plan. In the previous session of the State Legislature, a bill was passed allocating $250,000 for the state's portion of a fund to eliminate grade crossings in New York City, of which $200,000 was for Queens and $50,000 was for Staten Island. As such, $100 million would be available for the elimination of grade crossings in the two boroughs. At the time, there were 89 grade crossings on Staten Island, of which 14 were protected by sign boards, 44 by bells, 14 by flagmen, and 17 by gates. 100 people were injured and 56 people were killed in accidents at grade crossings from 1907 to 1910.


On March 6, 1911, work began on the elimination of a grade crossing at Amboy Road in Huguenot, which the PSC had ordered to be eliminated, as it was considered the most dangerous crossing on the island. Work was to be done quickly with the intent of completing the project by the end of summer. As part of the project, Amboy Road would be depressed by 10 to 12 feet (3.0 to 3.7 m), and the tracks would be raised to provide 14 feet (4.3 m) of clearance. The crossing was to be 60 feet (18 m) wide with sidewalks, and the structure over the road was to consist of steel on concrete abutments. The project was estimated to cost $78,240, with $19,560 in funding coming from the state. The PSC had also ordered the elimination of a grade crossing at Great Kills with Crooke's crossing; work on the project was expected to begin shortly.


On March 24, 1911, an automatic block signal system manufactured by the Hall Signal Company was put into use between Pleasant Plains and Tottenville, eliminating the need for a telegraph block signal office at Atlantic station. The new signaling system reduced the work required for telegraph operators on the line to report when trains moved in and out of signal blocks. This system had already been in use on the North Shore Branch from St. George to Arlington, and the South Beach Branch from St. George to South Beach for some time.


In July 1911, the Public Service Commission (PSC) ordered the SIRT to install gates at several grade crossings on the North Shore Division to increase safety and reduce the frequent occurrence of accidents. This followed a PSC order to have flagmen staff the grade crossing at Amboy Road in Huguenot and Crooke's crossing in Great Kills at all times. Bids on projects to eliminate these crossings had been advertised earlier, but no work had yet started on them. There were 42 public grate crossings on the SIRT, and 43 on the SIR, in addition to 34 on private property. The PSC sent inspectors to investigate every crossing in the city with considerable traffic with the goal of eliminating all grade crossings in September 1911. The PSC's goal was to either have the SIRT eliminate all grade crossings or have them install gates at dangerous crossings. Plans for the elimination of the crossings in Huguenot and Great Kills were prepared, and work was expected to begin shortly.


In November 1911, the Public Service Commission gave notice for a hearing on November 23 to consider whether the Staten Island Railway Company and the Staten Island Rapid Transit Railway Company should electrify their lines, and whether any additional terminal facilities, tracks and switches were needed. This issue came up as a result of a complaint made by Reverend Father Charles A. Cassidy about noise from the switching of cars and steam locomotives in the St. George freight yards. In December 1911, the PSC ordered that the Staten Island Railway Company and the Staten Island Rapid Transit Railway Company consider electrifying their lines. A public hearing for this matter was set for January 15, 1912.


On December 12, 1911, following a series of hearings held between October 30 and December 4, the PSC ruled that grade crossings on Staten Island had to be protected. The hearings had been held following a petition made by residents of Prince Bay and Pleasant Plains after two boys were seriously injured when a train struck a stagecoach they were riding at the Sharrott Avenue crossing in Pleasant Plains. The PSC ordered that, as of March 15, 1912, gates had to be placed at Centre Street in Clifton, Seaview Avenue and Garretson Avenue in Dongan Hills, at Rose Avenue in New Dorp, at Giffords Lane in Great Kills, at Huguenot Avenue in Huguenot, at Seguine Avenue (Prince Bay Avenue) in Prince Bay, and at Amboy Road in Pleasant Plains on the Perth Amboy Division. In addition, the SIRT and SIR were required to have signal bells placed at Fisher Avenue in Tottenville, at Old Mill Road in Richmond Valley, Annadale Road in Annadale, and Ocean Avenue in New Dorp on the Perth Amboy Division, and have a flagman placed at each of these crossings from 7 a.m. to 7 p.m.. A flagman also had to be in place at the crossing at Main Street in Tottenville at all times. The PSC also ordered that, as of January 15, 1912, gates had to be installed at Newark Avenue and Morningstar Road in Elm Park, and at Central Avenue in Mariners' Harbor on the North Shore Division, and at Tompkins Avenue in Fort Wadsworth, St. Mary's Avenue, Clifton Avenue, and Pennsylvania Avenue in Rosebank, Maple Avenue in Clifton, and Wave Street and Prospect Street in Stapleton on the South Beach Division. In addition, signal bells had to be installed at Chestnut Avenue in Clifton on the South Beach Division, and Granite Avenue in Elm Park on the North Shore Division.


In May 1912, work began on the installation of the new signaling system between Clifton Junction and Pleasant Plains. In order to allow for the installation of the new signaling, the experimental Lacroix automatic signaling system, which had been installed between Dongan Hills and Grasmere, would be removed.


In August 1912, the New York City Board of Estimate provided notice that it would hold a hearing on September 19 on the Staten Island Railway Company's application to add an additional track between Amboy Road in Pleasant Plains and Huguenot Avenue in Huguenot. In October 1912, work installing the new signaling system on the Perth Amboy Division was complete, with the exception of the single-track segment between Annadale and Pleasant Plains, which had a switch at Huguenot. The remaining section was scheduled to receive the new signaling once the completion of the grade-crossing elimination project in Huguenot and the completion of a second track between Annadale and Pleasant Plains. In October 1912, the Board of Estimate held a hearing on the Staten Island Railway Company's application to double-track its line from Pleasant Plains to Huguenot. A hearing had previously been held on the company's application for Amboy Road in Huguenot by itself. Work on that grade separation project was being completed rapidly, with the concrete abutments for the railroad bridge being almost completed and large steel girders being ready to be put into place. Work to grade the right-of-way for the second track was extended from the Amboy Road grade crossing in Huguenot past Prince Bay station. Work to lay the second track was expected to begin following the passing of the application by the Board of Estimate. With the completion of the double-tracking, trains would no longer have to wait at sidings at Annadale and Pleasant Plains for trains in the opposite direction to proceed.


In November 1912, the Board of Estimate issued a notice for the final hearing on SIRT's application to double-track the Annadale to Pleasant Plains section, which would take place on December 5. No one had spoken in opposition to the plan at the October hearing. The contract between New York City and the Staten Island Railway Company would have the company pay the city $500 three months after the Mayor signed the contract, with a sum of $100 for each crossing the second track would pass over or an annual amount of $800 from the date of the contract's signing until October 28, 1934. The crossings that would be crossed were Woodvale Avenue, Sharrott Avenue and Amboy Road in Pleasant Plains, Bayview Avenue and Manee Avenue in Prince Bay, and Huguenot Avenue and Amboy Road in Huguenot. Work on the elimination of the grade crossing at Amboy Road was in progress, and new girders were being put into place. Work to grade the second track was completed as far south as Prince's Bay station. On December 17, 1912, the State Supreme Court in Brooklyn received an application for the appointment of a commission to assess and appraise the properties needed for the widening of Amboy Road, which would be part of the project to abolish the grade crossing there. It was expected that the road would open around January 1, 1913.


On December 26, 1912, the City of New York granted the SIRT the right to construct a second track between Huguenot and Pleasant Plains, with completion expected in three years. The grant was for 25 years.: 1256 


On March 25, 1913, work began on the double tracking of the Perth Amboy Division between Huguenot and Prince's Bay. Work was expected to be done to complete the second track from Prince's Bay to Pleasant Plains shortly afterwards. The automatic block signal system would be extended over the new second track. At the same time, work began on eliminating Crookes crossing in Great Kills. Crookes crossing, which was located between Oakwood and Great Kills station, crossed the train tracks on a diagonal with a descending grade from both sides. This was considered the deadliest crossing on Staten Island, especially as car traffic became heavier on Amboy Road. The work to eliminate the crossing would consist of the raising of the grade of Amboy Road above the rail line. Work was expected to be completed before the middle of the summer in 1913.


Work on the grade crossing project at Amboy Road was completed by May 1913 at the cost of $100,000. Amboy Road now passed under the rail line at this point. This was the first grade crossing eliminated on the SIRT. Some time earlier, at a PSC hearing, it was recommended that the SIRT consider eliminating eleven additional grade crossings, with those on the Amboy Division being at Jefferson Blvd in Annadale, at Liberty Avenue in Dongan Hills, and at Clove Road in Grasmere.


On June 18, 1913, trains began running over the new double track between Annadale and Prince's Bay, leaving the section between Prince's Bay and Pleasant Plains as the only single-track section on the Perth Amboy Division. The new double track section, which was about 2 miles (3.2 km) long, reduced train delays, and allowed for the transferring of three shifts of telegraph operators from Annadale to the new tower at Prince's Bay, which controlled the switches at the boundary of double track territory. Another tower was planned to be installed at Pleasant Plains as part of a series of improvements at that station. The passenger station would be moved to the existing freight station, and a new freight station would be constructed on the opposite side of the track. As part of the project, grades were adjusted, and the roadbed was well ballasted and had heavy rail tracks installed, improving its condition to the higher standard that was maintained on the rest of the line. In July 1913, a group of Annadale residents appealed to the PSC for the restoration of the telegraph station at Annadale.


On August 7, 1913, work began on the construction of a new two-story switch tower was being installed at the former site of the station at Amboy Road. The tower would automatically control the interlocking being installed between Pleasant Plains and Prince's Bay, and was to be similar to the one that had been installed at Prince's Bay. In September 1913, work was nearly completed on improvements at the Pleasant Plains station, and the tower was on track to be completed in October. The Pleasant Plains station house was completely renovated, and received electric lights on the platform and in the station house. The new station opened on October 8, 1913, and the tower was completed shortly afterwards. On December 18, 1913, trains started using the new elevated structure over Crooks crossing, eliminating another grade crossing.


During fiscal year 1915, a second track was completed between Annadale and Prince's Bay, and grade crossings elimination projects were undertaken at Amboy Road, Huguenot Park, and Pleasant Plains.: 834  On January 19, 1917, the Board of Estimate gave notice that they would hold a public hearing on January 26 to hear the application of the Staten Island Railway Company to abandon its right to double track its line between Pleasant Plains and Prince's Bay. When the grade crossings at Huguenot and Great Kills were eliminated, the PSC had notified the SIRC that it should also eliminate the five grade crossings between Pleasant Plains and Prince's Bay, and gave the company three years to do so, which did not happen. They had granted the company an extension of one year to complete the work, but that expired on December 26, 1916. The SIRC said that it had applied for permission to abandon its right to complete the second track as its operation did not require the double tracking of this section, and as it wanted to avoid the expense of eliminating the grade crossings and of double tracking until it was needed.


In August 1917, the PSC adopted an order directing the SIRT and other railways to keep the gates at 33 grade crossings closed between midnight and 5 a.m. for vehicle safety. The New York State Transit Commission, on August 15, 1919, ordered the SIRT to eliminate the grade crossing at Virginia Avenue by lowering that street under the existing rail line, with a clearance of 14 feet (4.3 m). However, due to World War I and issues with nearby grade crossing elimination projects, this work was delayed.: 60–61 


On April 13, 1922, the SIRT petitioned to the PSC to move Bay Terrace station 1,000 feet (300 m) to the east of the station to Kelvan Avenue (Bay Terrace) and to rename the station Rice Manor. The PSC denied the application as the move would allow fewer people to use the station. There were 35 homes immediately surrounding the existing station while there were only three at the proposed location. The move was intended to spur development in the surrounding area.


Increase in traffic

In 1890 and 1906, respectively, the car float terminal and freight yard at Saint George and Arlington Yard were opened. The two main freight yards on Staten Island, Arlington and Saint George, were at capacity, and in 1912, to ease the congestion, the B&O began running freight via the Jersey Central into Jersey City. The B&O profited from the heavy coal trade that operated via the lines on Staten Island. In 1920, 4,000,000 tons of freight had been handled on the railway. In addition, passenger traffic on the line increased. Between 1903 and 1920, daily trips on the North Shore Branch increased from 50 to 65; from 50 to 60 on the South Beach Branch; and from 22 to 34 on the Tottenville Branch. In 1920, 8,000,000 passengers used the North Shore and South Beach branches while 5,000,000 passengers used the main line. Up to 1921, 3,369,400 trains had been operated on the SIRT with no fatalities.


Electrification: 1923–1925

On June 2, 1923, the Kaufman Act was signed by Governor Al Smith, mandating that all railroads in New York City–including the SIRT—be electrified by January 1, 1926. As a result, the B&O drew up electrification plans, which were submitted to the PSC. The plans were approved by the PSC on May 1, 1924, and construction began on August 1, 1924. The SIRT was to be electrified using 600 volt D.C. third-rail power distribution so it would be compatible with the Brooklyn–Manhattan Transit (BMT) once a planned tunnel was completed under the Narrows to Brooklyn, connecting the line to the BMT Fourth Avenue Line of the New York City Subway system. The B&O planned to use this tunnel to connect its freight from New Jersey to freight terminals in Brooklyn and Queens, including to a planned port at Jamaica Bay. The city started construction on the Narrows tunnel in 1924. Due to political pressure and the project's increasing cost, the freight tunnel portion of the plan was eliminated in 1925, and the entire project was halted in 1926. Only the shafts at either end were constructed.: 133   The SIRT ordered ninety electric motors and ten trailers (later converted to motors) from the Standard Steel Car Company to replace the old steam equipment.: 133  These cars, the ME-1s, were designed to be similar to the Standards in use by the BMT.


The first electric train was operated on the South Beach Branch between South Beach and Fort Wadsworth on May 20, 1925, and regular electric operation began on the branch on June 5, 1925.: 7805  As part of the electrification project, the South Beach Branch was extended one stop to Wentworth Avenue from the previous terminus at South Beach. Wentworth Avenue had a short, wooden, half-car platform, and a shelter was built there. That location had previously been used as a servicing and turning point for the line's steam-powered locomotives. Electric service began on the Perth Amboy sub-division on July 1, 1925, to much fanfare.: 7805  The North Shore Branch's electrification was completed on December 25, 1925, and resulted in a time savings of ten minutes from Arlington to St. George. Following the electrification of all three branches, service was increased by nineteen trains on the Perth Amboy sub-division, by the fifteen trains on the South Beach Branch, and by five trains on the North Shore Branch.


Because of the high cost of electrification, however, St. George and Arlington Yards, along with the Mount Loretto Spur, and the Travis Branch were not electrified.: 8  Thirteen steam engines were retired and four new, wholly automatic substations opened at South Beach, Old Town Road, Eltingville, and Tottenville. The SIRT's old semaphore signals were replaced by new color position light signals. This was the first permanent use of the type of signal on the B&O–it later became the railroad's standard.: 133  A modern signaling system was put into place in the Saint George Yards, allowing one dispatcher to do all the work. The Clifton Junction Shops were updated to maintain electric equipment rather than steam equipment, and a large portion of the yard was electrified. Grade crossing elimination began between Prince's Bay and Pleasant Plains. While electrification was being installed, the system's roadbed was rebuilt with 100-pound rail.


The promise of a faster, more reliable electrified service spurred developers and private individuals to purchase land alongside the SIRT lines, with the intention of providing housing to attract residents to Staten Island. On July 2, 1925, for the first time since its opening, the railroad stopped reserving its trains' first cars for smokers. A petition was sent to the railroad to reverse this decision.


In the end of 1924, work began on a project to elevate the track to eliminate six grade crossings on the Perth Amboy Division between Pleasant Plains and Prince's Bay and to add 0.82 miles (1.32 km) of double-track to complete the double-tracking of the entire line. The cost of the project was shared by the SIRT, New York State, and New York City. Work on this project was completed in 1926, and the State made the final payment on the project on August 15, 1927.: 60  Work on the $1,050,000 project was 85 percent complete at the start of 1926. As part of the project, new brick building stations were constructed. The double-tracking required the installation of rock-fill.


One June 23, 1926, the Transit Commission ordered the elimination of a grade crossing at Bay Street in Clifton through the elevation of the train tracks for $573,000, and the elimination of the Tompkins Avenue, Hope Avenue, and Belair Road crossings in Fort Wadsworth for $647,000. The order called for the closure of the Belair Road crossing and for the partial depression of the tracks and elevation of the streets to allow Tompkins Avenue and Hope Avenue to pass over the South Beach Branch. The SIRT sued to test the validity of these orders by the Transit Commission, but the orders were upheld by the New York State Court of Appeals. In 1927, the SIRT planned on appealing this decision to the Supreme Court of the United States. On November 24, 1926, the Transit Commission ordered the elimination of the Tompkins Avenue grade crossing in Clifton through the elevation of the street over the rail line. The estimated cost of this project was $281,000.: 63 


In 1927, work on the Virginia Avenue grade crossing elimination project was still not underway. The SIRT had requested that the Transit Commission's order be modified on October 22, 1926, but this request was denied. The Richmond Borough President, on June 1, 1927, requested a modification to the order to provide for a clearance of 12.8 feet (3.9 m) instead of 14 feet (4.3 m).: 60–61  On July 12, 1937, the first hearings were held on the Port Richmond—Mariners Harbor Elimination project, which would eliminate 21 grade crossings on the North Shore Division for $4 million.: 64 


In 1926, the New York State Transit Commission ordered the installation of automatic signals at 18 grade crossings on the SIR for $47,000. These signals were in operation since the middle of September 1927.: 56 


Expansion

In the 1920s, a branch was built to haul materials to construct the Outerbridge Crossing. The branch ran along the West Shore from the Richmond Valley station, and originally ended at Allentown Lane in Charleston, past the end of Drumgoole Boulevard. The branch was cut back south of the bridge after the bridge was completed. The Gulf Oil Corporation opened a dock and tank farm along the Arthur Kill in 1928; to serve it, the Travis Branch was built south from Arlington Yard into the marshes of the island's western shore to Gulfport.: 41  At this time, the B&O proposed to join the two branches along the West Shore. The West Shore Line, once completed, would have allowed trains to run between Arlington on the North Shore sub-division and Tottenville on the Perth Amboy sub-division. In addition, freight from the Perth Amboy sub-division would no longer be delayed by the congestion of Saint George Yard and the frequent passenger service of the SIRT. This proposal was canceled because of the Great Depression.


In the 1930s, there was a proposal to build a loop joining the Perth Amboy sub-division at Grasmere with the North Shore Branch at Port Richmond. There also was a proposal to join the North Shore Branch to Tottenville without using the existing West Shore tracks. Staten Island Borough President Joseph A. Palma, in 1936, proposed to extend Staten Island Rapid Transit to Manhattan (via New Jersey) across the Bayonne Bridge, which had been built to accommodate two train tracks. The Port of New York Authority endorsed the second plan in 1937, with a terminal at 51st Street in Manhattan near Rockefeller Center to serve the trains of Erie, West Shore, Lackawanna, Jersey Central, and trains from Staten Island. This original proposal would be brought back in 1950, by Edward Corsi, a Republican candidate for Mayor of New York City.


In 1931, Cedar Avenue station opened with the construction of wooden platforms at the Cedar Avenue grade crossing on the South Beach Branch.


On February 4, 1932, the headway on trains was decreased to 15 minutes from 20 minutes between 9:29 p.m. and 10:29 p.m.; and was decreased to 30 minutes from 40 minutes between 10:29 p.m. and 1:29 a.m. on the Perth Amboy Division.


On June 14, 1948, a bill to permit the SIRT to widen its railroad tunnel at the Saint George Ferry Terminal was signed into law. The tunnel, which was constructed under Federally owned land, was widened 19 feet (5.8 m) for a distance of 456 feet (139 m). The tunnel allowed for the laying of a third track, and permitted the operation of more trains from Saint George to Tottenville and South Beach. The extra track also facilitated better handling of trains at the ferry terminal at Saint George.


Grade crossing elimination: 1926–1950

On June 25, 1926, the Transit Commission ordered the elimination of four grade crossings on Staten Island—at Bay Street in Clifton, and at Hope Avenue, Belair Road, and Tompkins Avenue in Fort Wadsworth. The project would cost $1,000,000, with half of the cost going to the railroad and a quarter each to the city and state. At the time, the grade crossing at Bay Street was thought of as the most dangerous grade crossing on Staten Island. The SIRT sued the Transit Commission, arguing that it did not have the power to order the construction of such projects. The Court of Appeals ruled in favor of the Transit Commission on July 23, 1926. The case was carried to the Supreme Court, which decided to hear the case "for a lack of jurisdiction.": 238 


On November 27, 1929, the Transit Commission held hearings on proposals to eliminate eleven grade crossings in Dongan Hills and Grasmere at Old Town Road, Buel Avenue, Liberty Avenue, Seaview Avenue, Garretson Avenue, Cromwell Avenue, Tysen Lane, Burgher Avenue, Clove Road, Parkinson Avenue and Grasmere Avenue. On September 14, 1932, the Transit Commission approved plans for the elimination of these eleven grade crossings.


The Grasmere–Dongan Hills grade crossing elimination project was completed in 1934. The project eliminated eleven crossings and cost $1,576,000. The crossings were removed by putting the line in an open cut through Grasmere and elevating it through Dongan Hills.: 27–28  As part of the project, a new street—North Railroad Avenue— was constructed, paralleling the line's north side from Clove Road to Parkinson Avenue.: 46 


The East Shore sub-division was elevated in 1936–1937 to remove several grade crossings. A huge undertaking was required to remove grade crossings on the North Shore sub-division. The Port Richmond-Tower Hill viaduct was built to remove eight grade crossings; it was longer than a mile and became the largest grade crossing elimination project in the United States. The viaduct opened on February 25, 1937, marking the final part of a $6,000,000 grade crossing elimination project on Staten Island, which eliminated 34 grade crossings on the north and south shores. A two-car special train, which carried Federal, state, and borough officials, made a run over the viaduct and the seven-mile project. Stations closed for the viaduct project at Tower Hill and Port Richmond were reopened on this date.


Between 1938 and 1940, a grade crossing elimination project was undertaken over three miles between Great Kills and Huguenot, eliminating seven grade crossings and costing $2,136,000, which was partially paid for by the city, state, and Progress Work Administration funds.: 50  The line was depressed into an open cut between Great Kills and Huguenot, with the exception of a section through Eltingville where it was elevated. Four stations—Great Kills, Eltingville, Annadale and Huguenot—were completely replaced with new stations along the rebuilt right-of-way. The project started on July 13, 1938, and was completed in October 1940.: 45  The stations themselves were completed in 1939, and therefore have the date 1939 inscribed either on road overpasses or on railroad bridges.


In that same year, grade crossing eliminations were completed in Richmond Valley and Tottenville. The Richmond Valley project eliminated the crossing at Richmond Valley Road and cost $300,000 while the Tottenville project eliminated seven crossings—including one at Main Street—and cost $997,000. The only remaining grade crossings to be removed were at Grant City, New Dorp, Oakwood Heights, and Bay Terrace. These projects were delayed due to material shortages during World War II. In 1949, a project to eliminate 13 grade crossings on the Perth Amboy sub-division, at Grant City, New Dorp, Oakwood Heights and Bay Terrace, was set to begin, with a projected cost of $7,400,000. On August 30, 1950, the PSC announced a $6,500,000 plan to eliminate grade crossings of the SIRT. The plan was only approved with the assurance from the city that if passenger service was discontinued, the city would guarantee residents of the area would have some form of public transportation. The plan also included the construction of a bridge over the never-built Willowbrook Expressway.


World War II

Freight and World War II traffic helped pay off some of the debt the SIRT had accumulated, briefly making it profitable. B&O freight trains operated to Staten Island and Jersey City. Around this time, B&O crews began running through without changing at different junctions. Regular B&O crews and Staten Island crews were separated, meaning the crews had to change before they could enter Staten Island. All traffic to and from Cranford Junction in New Jersey was handled by the SIRT crews. During the war, all east coast military hospital trains were handled by the SIRT—the trains came onto Staten Island through Cranford Junction, with some trains stopping at Arlington to transfer wounded soldiers to Halloran Hospital. Freight tonnage doubled on the SIRT between 1942 and 1944 to a record 3.2 million tons. The Baltimore and New York Railway line become extremely busy, handling 742,000 troops, 100,000 prisoners-of-war, and war material operating over this stretch to reach their destinations.: 161  Two B&O subsidiaries, the B&NY and the SIRT, were merged on December 31, 1944.: 605  Since the Baltimore and New York Railway opened in 1890, the SIRT operated this line with locomotives belonging to itself and to its parent company, the B&O. Around the time of World War II, the B&O operated special trains for important officials. One special was operated for former Prime Minister of the United Kingdom, Sir Winston Churchill. The train took him to Stapleton, from where he boarded a ship to Europe. The SIRT made special arrangements for the trip, including a shined-up locomotive sporting polished rods, white tires, and an engine crew clad in white uniforms.


Fires

Tottenville station was destroyed by a fire on September 3, 1929. The fire was attributed to a short-circuited third rail. The two 550 foot (170 m)-long platforms were destroyed, as were five train cars being stored near the station. The damage was estimated to cost $200,000. Passengers using the Perth Amboy Ferry were forced to use the nearby Atlantic station instead.


On June 25, 1946, a fire wrecked the terminal at Saint George, killing three people and causing damage worth $22,000,000.: 55  The fire destroyed the ferry terminal and the four slips used for Manhattan service, the terminal for Staten Island Rapid Transit trains, and a small building and slip owned by the city and used by the Army and Navy to transport personnel from Staten Island to the United States Naval Depot at Bayonne, New Jersey. Twenty rail cars were also destroyed in the fire. Since the power circuits were melted, the electric MUs were trapped in the station. Diesel cars were sent to rescue them, but would not couple with the MUs due to their different coupling systems. Some cars were saved through the use of rigging tow chains, but precious minutes were lost. 5 cars were totally destroyed in the fire, while 16 suffered heavy damage. A few cars were sent to Clifton shops, with the others kept at St. George with their windows boarded up.


Two days after the fire, the city voted $3,000,000 to start work on a new $12,000,000 terminal that would be opened in 1948. Until a temporary terminal could be built at Saint George, Tompkinsville was used as the main terminal. Even though the station was very narrow and its facilities were inadequate, service continued without an issue and without any injuries. The station handled the equivalent of 128 passenger loads per day.


On June 8, 1951, a modern replacement terminal for Saint George opened, although portions of the terminal were phased into service at earlier dates.


Service scaledowns and the end of B&O operation: 1947–1971

Transfer of ferry service: 1947–1948

On October 28, 1947, the SIRT filed with the Interstate Commerce Commission (ICC) to get permission to discontinue ferry service between Tottenville and Perth Amboy Ferry Slip in Perth Amboy, New Jersey. The SIRT said the abandonment should be permitted because of "the substantial deficits being incurred in operation of the service, which covers a distance of 3,600 feet". On the same date, Mayor John Delaney of Perth Amboy created a plan to fight the SIRT's proposal. He criticized the railroad for failing to notify the city of its intentions. On September 22, 1948, the ICC allowed the SIRT to abandon the ferry, which it had been operating for 88 years. On October 16, the ferry operation was transferred to Sunrise Ferries of Elizabeth, New Jersey, which had agreed to lease the railway's ferry facilities at Tottenville and to lease Perth Amboy wharf and dock properties there. The schedules and the five-cent fare for the ferry were maintained. In 1963, Perth Amboy ferry service was discontinued.


Service cuts and the discontinuation of service: 1948–1953

On July 1, 1948, the New York City Board of Transportation took over all of the bus lines on Staten Island.: 8  As a result, the bus fare on Staten Island dropped from five cents per zone (twenty cents Tottenville to the ferry) to seven cents for the whole island, to match the fare of the other city-owned bus lines. The cheaper bus fare resulted in an exodus of passengers from the SIRT. In 1947, the SIRT had carried 12.3 million passengers but after the decrease in bus fares the number decreased to 8.7 million in 1948 and to 4.4 million in 1949. Three months after the change, passenger traffic dropped 32 percent on the Tottenville Division and 40 percent on the other two divisions. The buses saw a 25 percent increase in ridership.


Due to the loss of ridership, on August 28, 1948, the SIRT announced it would reduce service on all three branches on September 5. Service would be reduced from 15-minute intervals in non-rush hours to 30 minutes during that time, and from 5 to 10 minutes in rush hours to 10 to 15 minutes during rush hours. The day afterwards, Richmond Borough President Cornelius A. Hall and Staten Island civic organizations announced they would oppose the proposed cuts. The PSC elected not to prevent the cut in service on September 2, 1948, and the cut went into effect three days later. 237 of the previous 492 weekday trains were cut and the schedule of expresses was reduced during rush hours. In addition, all night trains after 1:29 a.m. were eliminated. The reduction of trips resulted in the firing of 30 percent of the company's personnel.


On September 7, 1948, Borough President Hall continued to oppose the SIRT's cuts at a PSC hearing in Manhattan. Commuters testified that trains were missing connections to ferry boats and that some trains were being held at the St. George Terminal during rush hour to wait for two boatloads of passengers. Previously, they said, the trains pulled out with only one boatload of passengers. On September 13, 1948, the SIRT agreed to add four trains and to extend the schedule of four others. On January 5, 1949, the PSC recommended the SIRT restore the service cut; if it refused, the PSC would order the SIRT to restore the service. Hall suggested lowering the fare to 10 cents or a 20-cent round trip to make the service more competitive with the buses on the island. On January 29, 1949, the PSC ordered the SIRT to restore five trains and to reschedule seven other trains for public convenience, and gave the SIRT until February 13 to carry out the order.


On May 20, 1949, the SIRT announced it intended to discontinue all of its passenger services and that it would seek permission from the PSC to do so, citing the loss of $1,061,716 in 1948. The PSC response was to rule that the railroad must continue its operations or substitute them with buses, otherwise the city should take over the railroad service as part of the municipal transit system. The SIRT made another request to discontinue its passenger service on June 3, 1952, with a date set of July 7. On June 16, the PSC ordered the SIRT to continue all of its passenger services pending a decision on the line's request to abandon its service. On July 9, hearings concerning the proposed abandonment of the railroad began. On July 16, the PSC counsel stated the operating deficits that had been charged to the SIRT's passenger service would disappear if they were included with the freight profits of the B&O in the New York area. After the hearings, the SIRT changed its planned abandonment date to September 12, 1952. The commissioner council said a provision for an additional two months of service, extending it to November 12, 1952, needed to be made.


On December 19, 1952, the PSC gave the SIRT permission to discontinue service on the North Shore Branch and South Beach Branch after March 31, 1953, because of city-operated bus competition. The discontinuation brought the SIRT an estimated annual saving of $308,000. The South Beach Branch was abandoned shortly thereafter while the North Shore Branch continued to carry freight. Bus service on parallel lines was increased to make up for the loss of service on these branches. By 1955, the third rails on both of the lines were removed.: 8 


City steps in to subsidize Tottenville service: 1954–1971

While the SIRT had successfully discontinued service on the North Shore Branch and South Beach Branch, it was not successful in its endeavors to discontinue service to Tottenville. On September 7, 1954, the SIRT made an application to discontinue all passenger service on the Tottenville sub-division on October 7, 1954. The PSC warned that if it discontinued its passenger service, action would be taken to remove the SIRT's parent company, the B&O Railroad, from Staten Island, meaning the end of its prospering freight operation. A large city subsidy allowed passenger service on the Tottenville sub-division to continue. Since this sub-division did not need the trains cars left over from the closure of the North Shore and South Beach lines, the SIRT sold 35 of them, of which 5 were trailers, to the New York City Transit Authority (NYCTA) in 1953–1954 for $10,000 each.: 12 : 174 


A bill allowing New York City to lease service on the Tottenville line was approved by Governor Harriman on March 20, 1956, paving the way for an agreement between the city and the B&O. On December 13, 1956, the PSC approved an agreement between the B&O and New York City that ensured the Tottenville line would continue to operate; as part of the deal, New York City leased the line's passenger facilities for 20 years and received a small percentage of the line's net income. The SIRT continued to collect revenue and operate service. In addition, the city repaid all taxes owed by the railroad to the city. The agreement went into effect on January 1, 1957. The SIRT's financial troubles continued and in February 1960, it asked the city for $3,870,000 in subsidies, threatening to ask the PSC for permission to discontinue the service if the funds were not provided. In 1959, the SIRT lost $1,100,000, with an average daily ridership of 4,000. On August 25, 1960, the Board of Estimate approved an amendment of the city's contract with the SIRT to increase the annual subsidy. Over the next ten years, the aid was increased from $4,000,000 to $8,400,000.


On April 5, 1962, a fire at Clifton Yard destroyed seven ME-1 train cars and a warehouse, adding to 13 lost in two previous fires and two that were scrapped, leaving the SIRT with only 48 cars to operate regular service. This car shortage meant 44 of its 48 train cars were in service during rush hours, leaving a small margin for errors. To maintain the previous level of service, the SIRT had carefully scheduled maintenance for their train cars; a number of trains were rushed back to Saint George as passenger-free expresses after dropping their loads in the evening rush, helping make up for the lack of train cars. The NYCTA set aside nine BMT Standards for possible transfer to the SIRT. The SIRT also looked at a proposal to transfer some D-type cars. Neither of the proposals were acted upon.


On July 13, 1967, Mayor John Lindsay announced the city was considering purchasing the 48 air conditioned train cars used at Montreal's Expo 67 to transport passengers between the city and the exposition grounds. The cars would have cost $3,840,000, or about $80,000 each, and were expected to be available in October once the fair closed. These cars were 11.7 percent larger than the cars then in service on the SIRT.


Freight operations: 1957–1971

On October 21, 1957, four years after North Shore sub-division passenger trains ended, a train from Washington, D.C.—the last SIRT special—carried Queen Elizabeth II and Prince Philip across the Arthur Kill Bridge en route to the Staten Island Ferry after a meeting with President Eisenhower. The special train movement was conducted in secrecy and the tracks along the route were cleared for this occasion. The train traveled over B&O, Reading Company, and Lehigh Valley lines to get to Staten Island Junction and the SIRT. The royal train, along with a press train, ended its run at a freight yard at Stapleton.: 24 


In November 1947, the Arthur Kill swing bridge was knocked off its center pier foundation by a passing oil tanker, rendering the bridge useless.: 164  Freight had to be rerouted through float bridges, with most of it passing through the Central Railroad of New Jersey's yards. The bridge was then condemned by the Army Corps of Engineers. Work on a replacement span started in 1955.: 165   On August 25, 1959, the damaged bridge was replaced with a state-of-the-art, single track, 558-foot (170 m) vertical lift bridge. The 2,000 ton lift span was prefabricated then floated into place.: 8  The new bridge was raised 135 feet (41 m); because the new bridge aided navigation on the Arthur Kill, the United States Government assumed 90 percent of the $11,000,000 cost of the project.: 349  The old bridge had been condemned and ordered replaced by the United States Secretary of the Army in 1949, with an expected cost of $8,000,000 to be split between the SIRT and the U.S. Government. When the bridge reopened, long-unit coal trains from West Virginia began using an extension of the Travis Branch, on Staten Island's West Shore to serve a new Consolidated Edison power plant in Travis.: 8 


The closure of Bethlehem Steel in 1960 and of U.S. Gypsum in 1972 led to a dramatic decline in rail traffic via the Arthur Kill Bridge, although there still was enough traffic in the 1970s to keep car floats reasonably busy.


Final grade-crossing elimination: 1960–1964

On November 7, 1960, an accident took place at the grade crossing at North Railroad Avenue and Bancroft Avenue, four blocks away from the New Dorp station. An eight-year-old girl was killed and 31 children were injured as a train struck a crowded school bus as it was about to exit the crossing. A grand jury had ordered the closure of this crossing and 12 others along this stretch of the line, after a train killed a high-school girl at a crossing eight blocks away in December 1959. The railroad had been given an extension of time so it could install gates. On November 10, 1960, Staten Island Borough President Albert Maniscalco ordered the closure of this grade crossing and announced that he expected work on eliminating the grade crossing to begin the following year.


On August 29, 1964, the PSC approved a $10,923,000 project to eliminate the last remaining grade-crossings on the line between the Jefferson Avenue and Grant City stations. As part of the project, new platforms and station buildings were built at New Dorp and Grant City, and new platforms were built at Jefferson Avenue. To eliminate the grade crossing, the line was raised on an embankment for part of the way and was depressed into an open cut for the rest. To avoid interfering with train service, a temporary track was constructed to the east of the original line and a new, crossing-free line was constructed upon the original right of way.: 13 


Following the 1964 opening of the Verrazzano–Narrows Bridge, which directly connected Staten Island with the rest of the city by road, ridership on the SIRT, which had been steadily increasing at a rate of 300,000 a year in the early 1960s, increasing 34 percent between 1960 and 1964, precipitously declined. Annual ridership decreased from 6.4 million in 1964 to 5.7 million in 1965. The previous ridership growth had been despite increasing car ownership.


City operation of passenger service: 1971–present

Transfer to MTA operation: 1968–1971

In May 1968, the President of the Metropolitan Transportation Authority (MTA), William Ronan, proposed that New York City take control of the SIRT and spend $25 million on modernizing the line. This proposal was part of a 20-year, $2.9-billion plan announced by the agency on February 28. Some of the money allocated to the SIRT would be used to purchase new trains. On December 18, 1969, the Board of Estimate approved the purchase of the SIRT for $1 and the use of $3.5 million to purchase the line's 48 cars, 70 acres (28 ha) of real estate, the air rights over the tracks, and the line's Clifton Shops. The additional land was purchased to allow for a wider right-of-way, parking lots, and station improvements. As part of the agreement, the B&O would continue to operate freight service over the line once a day.


On January 1, 1970, New York City's lease of the Tottenville line was terminated; after that date the city reimbursed the railroad for its passenger deficits. On May 29, 1970, New York City and the SIRT entered an agreement for NYC to purchase the Tottenville line, as part of which the SIRT was reserved the right to operate freight service over the line. The city was required to ensure the track was in good condition for SIRT's freight operations.: 3009  The freight operation was reorganized as the Staten Island Railroad Corporation.: 174  On June 29, 1970, the Staten Island Rapid Transit Operating Authority was incorporated as a subsidiary of the state's MTA to acquire and operate the SIRT.: 469, 475–476 


On April 21, 1971, the ICC approved the city's purchase of the line. On July 1, 1971, operation of the Tottenville line was turned over to the Staten Island Rapid Transit Operating Authority, a division of the state's MTA. The line itself was purchased by the City of New York for $1 and the MTA paid the B&O $3.5 million for the line's equipment. Grade crossings along the Tottenville line had to be removed by the B&O to finalize the deal. Grade crossings were supposed to have been eliminated in the 1930s but a lack of finances—partially resulting from the Great Depression and World War II—prevented it.


Despite MTA improvements, problems persist: 1970s and 1980s

On June 15, 1972, five 1955-built, air-conditioned coaches on loan from the Long Island Rail Road (LIRR) went into service on the SIRT. One three-car train made one round trip during the morning and operated again during the afternoon peak.: 49 : 175  As a result of previous tests, the edges of the platforms at St. George were trimmed for the extra clearance required by the LIRR cars, which were 85 feet (26 m) long. The cars were only 15 feet (4.6 m) longer than the 45 cars in operation but had a seating capacity of 123 passengers—almost double the capacity of the other coaches.


On February 1, 1972, the fare on the SIRT was increased for the first time since 1958, to 35 cents from an average fare of 22 cents; the system used to have zoned fares. The fare increase applied to the whole system and was accompanied by the elimination of commutation tickets and student tickets. Previously, fares ranged from 20 to 35 cents. Sixteen percent of riders of the 17,000 daily riders had no change in fare. There was a 10 percent increase for 51 percent of passengers and a 15 percent increase for the remaining 33 percent. The fare increase was expected to bring in an extra $400,000 a year.


At the time, the line was operating at a deficit of $2.9 million a year, with $2.5 million of it offset by a subsidy from the city. The MTA had plans for a $25 million improvement program for the line including new train cars. Improvements were also planned for the tower and signal systems, for the roadbed and for the stations. Increased power, 8,000 feet (2,400 m) of new rails, and mercury-vapor lighting at 14 of the 22 stations were also part of the plan. Three quarters of the $25 million were to be provided by 1967 state transportation bond issue and the remaining $6.25 million was to be paid by the city.


On February 28, 1973, the first six new R44 cars were put into service on the SIRT. These were part of a 52-car order from the Saint Louis Car Company—the same type of cars as the newest cars then in use on subway lines in the other boroughs. The R44s replaced the ME-1 rolling stock inherited from the B&O that had remained in continuous service since 1925 when the system was electrified.: 49, 52 : 175 


During the 1970s, the MTA's ability to improve service on the line was hampered by several strikes by the SIRT's workers. From December 11, 1975, to April 1976, service on the SIRT was shut down because the line's 61 motormen and conductors went on strike. The strike ended once the MTA agreed to give them 14% wage increases.


In December 1976, SIRTOA recently completed projects to provide remote control of SIRT's electrical substations from the central one at St. George, and to install a two-way radio system. The former project improved the reliability of the system's power system, while the latter provided direct communication between the St. George control center and all trains, allowing information about the status of SIRT operations to be sent immediately to train staff and passengers on trains.


In December 1977, SIRTOA was preparing to begin construction on a pair of projects that would allow longer four-car trains to be run during rush hours regularly. New York City covered 25 percent of the cost of the projects, while New York State covered the remaining 75 percent. The first project was the construction of a new electrical substation capable of handling 4,000 kilowatts in Grant City, and the second was the expansion and modernization of the repair shop at Clifton Yard.


Work on the new Grant City substation building, which would cost $506,592, was expected to start in January 1978 and be completed by March 1979, while the installation of electrical equipment, which would cost $1.8 million, would begin in September 1978 and be completed in 1980. The project was the first in a series of improvement part of a long-term program to upgrade the SIRT's electrical supply and make it more reliable. In later phases, the new equipment installed in the new Grant City substation would also be installed in older substations on the line. In response to complaints from nearby residents, the design of the substation was modified to blend in with the character of the surrounding residential area, and would be constructed within the SIRT's open cut right-of-way. In addition, greenery and shrubs would be installed to beautify the structure's roof, which was located at street level. Work on the project was finally underway in April 1979, and was expected to be completed by the end of 1980.


The Clifton Yard project was planned to improve the maintenance of the SIRT's new fleet of R44 cars. This $3.9 million project, as of December 1977, was expected to begin in 1978 and be completed by the end of 1979. The project would extend and modernize the car maintenance shop, and construct new storage facilities, and a new two-story motor and wheel repair shop. The extension of the existing maintenance shop would allow the inspection pits to be rebuilt. These pits were designed to fit the ME-1 cars, which were 8 feet (2.4 m) shorter than the R44 cars. The improvements at the shop would increase the number of cars that could be overhauled and inspected daily by 50 percent. As of April 1979, work was expected to be completed by the middle of 1980.


On August 6, 1980, an MTA committee evaluated whether it should continue operating the rail line. On September 28, 1981, Assemblyman Robert Straniere announced that half of the $25 million allocated for capital improvements to the SIRT from bond use funds would be used for improved power distribution and the modernization of stations. Eleven stations would have their platforms extended, completing the extension of platforms along the line and two stations would have their wooden platforms replaced. In addition, equipment at four substations would be improved, new switches would be installed at Tottenville interlocking, and the wye track at St. George would be realigned to fit four-car trains, among other improvements. As part of the MTA's inaugural Capital Program from 1981, Pleasant Plains and Prince's Bay stations were rebuilt and the platforms at several other stations were lengthened.


On September 23, 1985, SIRTOA began work to repair structural problems at ten stations, including crumbling stairways. Temporary access paths, platforms and stairways were constructed to allow service to be maintained while work was underway.


On October 22, 1986, midday service was reduced in frequency from one train every 20 minutes to one every 30 minutes, due to a reduction in ferry service. Weekend service was reduced by 15 trains to 78 and weekday service was reduced by 15 to 109. In the following two years, ridership reduced from 6.47 million to 6.23 million.


Improvements in fare collection: 1985–1994

Beginning on December 18, 1985, passengers traveling between 5:50 a.m. and 10 a.m. were required to pay their fares after they exit the train. People exiting at St. George were required to pay their fares by dropping a pre-purchased ticket in a box near twelve turnstiles, which were rearranged to reduce backups. Previously these turnstiles were only used for evening rush hours. At other stations, riders were to pay their fares as they exited station platforms. The change was made to reduce fare evasion. Despite the fact that ridership on the railroad increased 38 percent since 1977, revenue did not grow as conductors collecting tickets often could not get to all passengers in time due to crowding on trains. At the time, tickets could be purchased at Tottenville and St. George at all times, or from Grant City, New Dorp, Dongan Hills, Oakwood Heights, Eltingville and Great Kills between 6 a.m. and 2 p.m.


Tokens began to be accepted at St. George on April 20, 1988. A month later, tokens were used by 80 percent of riders. Tokens could be bought at vending machines between 6 a.m. and 10 a.m. at Huguenot, Annadale, Eltingville, Great Kills, Oakwood Heights, New Dorp, Grant City, and Dongan Hills, and at St. George at all times. On October 18, 1992, the NYCTA began distributing free transfers between the SIRT and bus routes on Staten Island.


On March 31, 1994, MetroCards began being accepted for fare payment at the St. George station, making it the 50th subway or Staten Island Railway station to accept it.


On May 13, 1997, New York City Transit announced it would eliminate on-board fare collection on July 4, meaning that fares would only be paid by riders who boarded or exited St. George, which was the only station with turnstiles. At the time, fares were collected on board by the conductor except on  weekday trains arriving at St. George between 5:50 a.m. and 10:50 a.m.. Riders on those trains had to pay their fares upon exiting the station. Fares could be paid using MetroCards, tokens, special fare tickets, or transfers. Riders who alighted at other stations did not have to pay a fare during the morning rush hour.


The change, which was expected to cost $400,000, was part of the "One City, One Fare" fare cuts, would coincide with the elimination of the fare on the Staten Island Ferry, and the institution of free transfers between the railway and subways and buses in Manhattan. With the elimination of on-board fare collection, fares for trips between Tompkinsville and Tottenville would become free. A 1996 study had found that 2,370 of the 9,430 people who got off a St. George-bound train in the morning and 2,330 of the 10,430 people who boarded a Tottenville-bound evening train did so at a station other than St. George. Seven trainmen positions were expected to be affected by the elimination of the on-board collection of fares.


The changes took effect on July 4, 1997 as planned. The union representing the SIR's workers, United Transportation Union local 1440, was worried about the change, in part because of an expected rise in ridership. About 25 percent of riders would potentially get free rides with the change. The removal of fares was blamed for an immediate spike of crime along the line. Police officers and students said that disorderly conduct and fights by students took place more regularly. Crime increased over 200 percent between July and December 1997, compared to the same period in the prior year.


Rebranding and service improvements: 1990s

Ridership started to decrease in the early 1990s following the start of a series of major repair projects. Over nine years in the 1990s, all of the mainline track on the line was replaced. Though the third rail was supposed to be replaced, only a few hundred feet in Clifton and Tottenville yards were replaced by August 2003.


The MTA rebranded the Staten Island Rapid Transit as the MTA Staten Island Railway (SIR) on April 2, 1994; Staten Island Railway was the line's original name.: 9  This move followed the transfer of the Staten Island Rapid Transit from the New York City Transit Authority's Surface Transit Division to the Department of Rapid Transit on July 26, 1993.


On December 2, 1991, service on the SIRT changed from four-car trains to two-car trains between 9 p.m. and 5 a.m. to reduce track and car maintenance costs. In February 1992, it was estimated that the change led to reduced energy costs and a 13 percent reduction in car maintenance costs. This change was originally intended to go into effect by June 1988.: 75  In 1988, it was decided to transfer 12 R44 cars from the New York City Subway to the SIR in anticipation of expanding trains to five cars to accommodate an expected growth in ridership and to reduce overcrowding, which did not materialize.: 36  During the mid-2000s, some trains did run with five cars. The SIR timetable stated, "Select rush hour trains operate with additional cars. For a more comfortable ride, please use all available cars." Platforms at 18 stations were lengthened to accommodate five-car trains in 1988.: 75 


In 1993, a report by the Federal Transit Administration (FTA) criticized the MTA for not installing an automatic speed control (ASC) system on the railway to prevent collisions and overspeed violations. The agency had initially planned on installing a new signaling system on the line in the mid-1980s, but it was dropped from the capital program. The 1992-1996 program allocated $75 million for upgrades to the signal system with ASC, with funds allocated for work in 1996 and 1998. The FTA report also found that train cars did not have speedometers in them; they were added by February 1996.


In 1993, the Dongan Hills station became accessible under the Americans with Disabilities Act of 1990. In April 1996, a $6.7 million projects to repair the St. George, Tompkinsville, Stapleton, New Dorp, Richmond Valley, and Tottenville was scheduled for 1997. In 1998, those renovations were completed.


In November 1996, the results of the Staten Island Transit Needs Assessment Study, which was commissioned by New York City Transit, were made public. Recommendations related to the railway included operating shorter two-car trains instead of four-car trains to enable more frequent service, with not all trains timed to meet the ferry, better marketing for the railway, improved signs leading to train stations, consolidating the Nassau and Atlantic stations into an Arthur Kill station with a parking lot, adding a parking lot at Huguenot, creating a Grasmere transit center, where connections could be made with buses, and restarting service on the North Shore Branch. The study said that service could be restored for $108 million, and could have 17,200 riders a day in 2010. The study also recommended the addition of skip-stop service, the use of improved rolling stock which could accelerate more quickly and improvements in signaling.


On April 7, 1999, three additional afternoon express runs were added to the schedule, supplementing the four previously scheduled express trips. These trips skipped stops between St. George and Great Kills and then made all stops to Tottenville. The trips reduced travel times to Great Kills from 24 minutes to 15 minutes. The additional trips increased the span of express service from 5:11 to 6:02 p.m. to 4:50 to 6:31 p.m.. The existing four trips were used by about 2,280 passengers.  The change in service was made in response to a request in August 1998 by Representative Vito Fossella Jr..


On June 24, 2001, a small section of the easternmost portion of the North Shore Branch (a few hundred feet) was reopened to provide passenger service to the new Richmond County Bank Ballpark, home of the Staten Island Yankees minor-league baseball team. The station cost $3.5 million to build. One train was scheduled to travel to/from Tottenville and two or three shuttle trains from St. George served the station. Trains last served the station in September 2009 and the service was discontinued as part of budget cuts on June 18, 2010, the day of the first scheduled home game for the Yankees. The elimination of this service saved $30,000 annually.


Consistent growth: 2000s and 2010s

In 2000, work began on a four-year project to upgrade the railway's signal system. Work was initially supposed to begin in 1998 and be completed by 2001. In August 2003, work was already over three years late.


In August 2003, it was reported that the SIR would overhaul three railroad bridges (Bay Street south of Clifton, and two crossings of Amboy Road) the following year for $7.2 million, which would require service to operate on a single track in sections. The repair work included replacing the tracks, installing a new waterproof seal over the deck, repairing floor beams, metal panels, drip pans, and girder encasements, and repainting the structures. These bridges were identified for repairs in a 2000 engineering survey. Work would begin following the completion of the signal project and was estimated to take about two years. The 2000 study stated that eight other bridges would need repairs. Those repairs were to take place between 2005 and 2009. An additional 16 bridges would need their waterproofing replaced, and two others had minor structural defects.


In 2004, the SIR put out its preliminary budget for 2005; it proposed several service cuts to offset an increased debt. For 2005, the agency proposed implementing One Person Train Operation, adding fare collection at Tompkinsville, reducing the fleet to 52 cars, eliminating express service, and reducing trains to two cars during off-peak hours. For 2006, it proposed eliminating weekend service from 2 a.m. Saturday to 5 a.m. Monday. Starting on May 1, 2005, SIR trains started running with two cars between 9 p.m. to 5 a.m. every night, stopping near station entrances to deter crime.


In December 2004, the completion on the signaling project was already 18 months late. A three-month phased cutover of the new signaling system was to begin in January 2005, starting from Tottenville and heading northwards. Starting January 21, 2005, the entire line would be closed every weeknight from 10 p.m. to 5 a.m. and all weekends until April.


In June 2005, a new cab signaling system and a new control center went into effect at St. George; the system is still in use as of February 2018 and has received minor upgrades. These improvements cost $100 million and helped enhance safety and increase operational flexibility. The new signal system provided improved signal visibility, continuous speed enforcement, and central monitoring at St. George, and the ability to remotely change switch positions; all switches on the main line were interlocked as part of the project. Previously, dispatchers could only track the position of trains using the signal system as they entered and left Tottenville and St. George.


The cab signaling system sounds an alarm inside the cab to control train speeds and automatically applies the brake if needed. Prior to the new system's implementation, there were no safeguards in place to prevent trains traveling at full speed after ignoring a signal and continuing past a bumping block. The new signal system required the installation of 30 miles (48 km) of underground fiber optic cable. The signals system limits the maximum speed at certain points and at sharp curves, and limits trains to 15 miles per hour (24 km/h) when approaching a terminal.


On July 17, 2006, rush-hour express service was sped up, with all morning express trains running non-stop from New Dorp to St. George. Evening express service started earlier under the new timetable. In response to record ridership growth on the SIR, rush-hour express service was expanded in late 2007. Evening service was expanded on November 14 when five additional express trains were added, expanding the span of express service by 80 minutes. Morning express service was expanded on December 5. The span of express service was extended by 45 minutes during the morning with the addition of three express trains. Express service was also added in the off-peak direction to Tottenville during the morning, with five express trips leaving St. George. Five local trains were also added to the schedule.


In May 2007, emergency call boxes and surveillance cameras were installed at Old Town to improve safety for riders waiting for trains. The cameras were linked by fiber optic cables to St. George station, where the footage was recorded. The $1.75 million project would install the systems at all stations by 2009, and was funded by former State Senator John Marchi and Councilman James Oddo. Work to consolidate the Nassau and Atlantic stations was expected to be completed in 2009 or 2010, and the R44 rolling stock was to be rehabilitated in 2008 before being replaced in 2014.


A new stationhouse at Tompkinsville opened on January 20, 2010, with turnstiles installed for the first time. The stationhouse cost $6.9 million, and was equipped with low turnstiles and fare vending machines. The turnstiles were installed because passengers often avoided paying the fare by exiting at Tompkinsville and taking a short, 0.5-mile (800 m) walk to the St. George ferry terminal. It was estimated the SIR lost $3.4 million every year as a result. Trains heading to St. George during the morning rush hour, and trains leaving St. George in the evening rush hour had skipped Tompkinsville and Stapleton in an effort to reduce fare evasion. Service stopped at these stations every hour while all other rush hour trains bypassed them. After the installation of the turnstiles, the revenue generated at Tompkinsville made up 15% of the SIR's revenue. Schedules were changed, with locals and most non-rush hour trains stopping. In 2010, it was announced the MTA planned to restore fare collection on the entire line to raise more revenue and reduce crime. This was planned to be implemented once the MetroCard was replaced with a smartcard. In the interim, the MTA proposed installing turnstiles at Grasmere because it is a heavily used transfer point to the S53 bus to Brooklyn.


In a 2006 report, the Staten Island Advance explored the restoration of passenger services on 5.1-mile (8.2 km) of the North Shore Branch between St. George Ferry Terminal and Arlington station. Completion of the study is necessary to qualify the project for the estimated $360 million. A preliminary study found ridership could reach 15,000 daily. U.S. Senator Chuck Schumer from New York requested $4 million of federal funding for a detailed feasibility study. In 2012, the MTA released an analysis of transportation solutions for the North Shore, which included proposals for the reintroduction of heavy rail, light rail, or bus rapid transit using the North Shore line's right-of-way. Other options included transportation systems management, which would improve existing bus service, and the possibility of future ferry and water taxi services. Bus rapid transit was preferred for its cost and relative ease of implementation, which would require $352 million in capital investment. As of January 2018, the project has yet to receive funding.: 61 


On May 21, 2012, Grasmere station started to be rehabilitated. The construction included demolition and rebuilding of the station platform and station house. A temporary platform and entrance were built north of the main station. Construction was finished in April 2014.


A new, ADA-compliant station named Arthur Kill, near the southern terminus of the present line, opened on January 21, 2017, after numerous delays. The station is sited between Atlantic and Nassau stations, which it replaced. Atlantic and Nassau were in the poorest condition of all the stations on the line. Unlike the Atlantic and Nassau stations, Arthur Kill is able to platform a four-car train. The MTA has provided parking for 150 automobiles near the station. Ground was broken for the $15.3 million station on October 18, 2013. The contract for the project was awarded on July 31, 2013. The building of a station at Rosebank, which would bridge the gap between Grasmere and Clifton stations—the longest gap between stations on the line—has also been discussed. A Rosebank station once existed on the now-defunct South Beach Branch of the railway.


The 2015–2019 MTA Capital Plan called for the SIR's Richmond Valley station and 32 subway stations to undergo a complete overhaul as part of the Enhanced Station Initiative. Updates would include the addition of cellular service, Wi-Fi, USB charging stations, interactive service advisories and maps, improved signage, and improved station lighting. The capital plan also called for the reconstruction of the Clifton Yard, which had been damaged during Hurricane Sandy in 2012, as well as the addition of storm walls at St. George. The St. George flood walls were finished in mid-2017, while the Clifton Yard reconstruction was to be completed in 2021. In 2022, the MTA awarded a contract to install seven 150-foot (46 m) monopole antennas to replace the SIR's existing radio system.


Decline and rebirth of freight: 1971–present

Decline: 1971–2000

Through a merger with the Chesapeake and Ohio Railway, the B&O became part of the larger Chessie System. The freight operation on the island was renamed the Staten Island Railroad Corporation (SIRC) in 1971, after being separated from the island's passenger service. In 1976, when the CNJ was absorbed into the federal government-controlled Conrail, the Chessie System became isolated from their properties in New Jersey and Staten Island. Most of their freight service, with the exception of one daily train to Cranford Junction, was truncated to Philadelphia. By 1973, the car ferry yard at Jersey City operated by the CNJ had shut down. Afterward, the car ferry operation at Saint George Yard was reopened; the New York Dock Railway took over the operation in September 1979, but closed it the following year.: 173 


Only a few isolated industries on Staten Island continued to rely on rail service for freight, effectively abandoning the Saint George Yard.: 9  On April 24, 1985, following a major decline in freight traffic, the Chessie System sold the SIRC to the New York, Susquehanna and Western Railway (NYS&W), a subsidiary of the Delaware Otsego Corporation (DO), for $1.5 million via a promissory note payable for ten years. The NYS&W began to serve the SIRC's ten remaining customers with the hopes of hiring more, and only five employees became assigned to the SIRC's operations. In October 1989, the NYS&W embargoed 4 miles (6,400 m) of trackage east of Elm Park on the North Shore Branch, ending rail freight traffic to Saint George.: 176  In 1990, the SIRC's primary customer, Procter & Gamble, closed, resulting in a further decline in freight traffic.


The Arthur Kill Vertical Lift Bridge was removed from service on June 25, 1991, and the SIRC's final train under NYS&W ownership subsequently operated on April 21, 1992. Afterwards, the North Shore Branch and the Arthur Kill Bridge were obtained by the Chessie's successor, CSX. In 1994, the railway and bridge were sold again to the New York City Economic Development Corporation (NYCEDC), whose purchase was followed by a decade of false starts.: 9  In 1998, Conrail was split up, and some portions of trackage formerly operated by the B&O's competitors were acquired by CSX. The railroad line from Cranford Junction to Arlington was still intact, by that time.


Reactivation: 2000–present

During the early 2000s, the Port Authority of New York and New Jersey announced plans to reopen the Staten Island Railroad line in New Jersey. Since the Jersey Central became a New Jersey Transit line, a new junction would be built to the ex-Lehigh Valley Railroad line. Two rail tunnels from Brooklyn were planned—one to Staten Island and one to Greenville, New Jersey—and would allow freight to pass through New York on its way from New England to the South. In December 2004, a $72 million project to reactivate freight service on Staten Island and to repair the Arthur Kill Vertical Lift Bridge was announced by the NYCEDC and the Port Authority. Specific projects on the Arthur Kill Vertical Lift Bridge included repainting the steel superstructure and rehabilitating the lift mechanism. In June 2006, the freight line connection from New Jersey to the Staten Island Railroad was completed, and became operated in part by the Morristown and Erie Railway under contract with other companies and New Jersey.


The Arthur Kill Vertical Lift Bridge was renovated in 2006 and began regular service on April 2, 2007; 16 years after it was closed. As part of the project, a portion of the North Shore Branch was rehabilitated, the Arlington Yard was expanded, and 6,500 feet (1,981 m) of new track was laid along the Travis Branch to Fresh Kills. Soon after service restarted on the line, Mayor Michael Bloomberg officially commemorated the reactivation on April 17, 2007. Service was provided by CSX Transportation, Norfolk Southern Railway, and Conrail over the Travis Branch to haul waste from the Staten Island Transfer Station at Fresh Kills and ship container freight from the Howland Hook Marine Terminal and other industrial businesses. Along the remainder of the North Shore Branch, there are still tracks and rail overpasses in some places.: 15–17 


Notes

References







Fidel Alejandro Castro Ruz (13 August 1926 – 25 November 2016) was a Cuban politician and revolutionary who headed Cuba from 1959 to 2008, serving as prime minister from 1959 to 1976 and president from 1976 to 2008. Ideologically a Marxist–Leninist and Cuban nationalist, he also served as the first secretary of the Communist Party of Cuba from 1965 until 2011. Under his administration, Cuba became a one-party communist state; industry and business were nationalized, and socialist reforms were implemented throughout society.


Born in Birán, the son of a wealthy Spanish farmer, Castro adopted leftist and anti-imperialist ideas while studying law at the University of Havana. After participating in rebellions against right-wing governments in the Dominican Republic and Colombia, he planned the overthrow of Cuban president Fulgencio Batista, launching a failed attack on the Moncada Barracks in 1953. After a year's imprisonment, Castro travelled to Mexico where he formed a revolutionary group, the 26th of July Movement, with his brother, Raúl Castro, and Ernesto "Che" Guevara. Returning to Cuba, Castro took a key role in the Cuban Revolution by leading the Movement in a guerrilla war against Batista's forces from the Sierra Maestra. After Batista's overthrow in 1959, Castro assumed military and political power as Cuba's prime minister. The United States came to oppose Castro's government and unsuccessfully attempted to remove him by assassination, economic embargo, and counter-revolution, including the Bay of Pigs Invasion of 1961. Countering these threats, Castro aligned with the Soviet Union and allowed the Soviets to place nuclear weapons in Cuba, resulting in the Cuban Missile Crisis—a defining incident of the Cold War—in 1962.


Adopting a Marxist–Leninist model of development, Castro converted Cuba into a one-party, socialist state under Communist Party rule, the first in the Western Hemisphere. Policies introducing central economic planning and expanding healthcare and education were accompanied by state control of the press and the suppression of internal dissent. Abroad, Castro supported anti-imperialist revolutionary groups, backing the establishment of Marxist governments in Chile, Nicaragua, and Grenada, as well as sending troops to aid allies in the Yom Kippur, Ogaden, and Angolan Civil War. These actions, coupled with Castro's leadership of the Non-Aligned Movement from 1979 to 1983 and Cuban medical internationalism, increased Cuba's profile on the world stage. Following the dissolution of the Soviet Union in 1991, Castro led Cuba through the economic downturn of the "Special Period", embracing environmentalist and anti-globalization ideas. In the 2000s, Castro forged alliances in the Latin American "pink tide"—namely with Hugo Chávez's Venezuela—and formed the Bolivarian Alliance for the Americas. In 2006, Castro transferred his responsibilities to Vice President Raúl Castro, who was elected to the presidency by the National Assembly in 2008. Castro died at the age of 90 from natural causes in November 2016.


Castro was the longest-serving non-royal head of state in the 20th and 21st centuries and polarized world opinion about his rule. His supporters view him as a champion of socialism and anti-imperialism whose revolutionary government advanced economic and social justice while securing Cuba's independence from American hegemony. His critics view him as a dictator whose administration oversaw human rights abuses, the exodus of many Cubans, and the impoverishment of the country's economy.


Early life and career

Youth: 1926–1947

Fidel Alejandro Castro Ruz was born out of wedlock at his father's farm on 13 August 1926. His father, Ángel Castro y Argiz, was a migrant to Cuba from Galicia, northwest Spain. After the collapse of his first marriage he took his household servant, Lina Ruz González—of Canarian ancestry—as his mistress and later second wife; together they had seven children, among them Fidel. At age six, Castro was sent to live with his teacher in Santiago de Cuba, before being baptized into the Catholic Church at the age of eight. His baptism allowed Castro to attend the La Salle boarding school in Santiago, and was later sent to the Jesuit-run Dolores School in Santiago.


In 1942, Castro transferred to the Jesuit-run El Colegio de Belén in Havana. In 1945, Castro began studying law at the University of Havana where he became embroiled in student activism and the violent gangsterismo culture within the university. After becoming passionate about anti-imperialism and opposing US intervention in the Caribbean, he unsuccessfully campaigned for the presidency of the Federation of University Students. Castro became critical of the corruption and violence of President Ramón Grau's government, delivering a public speech on the subject in November 1946 that received coverage on the front page of several newspapers.


In 1947, Castro joined the Party of the Cuban People (Partido Ortodoxo), founded by Eduardo Chibás. Though Chibás came third in the 1948 general election, Castro remained committed to working on his behalf. Student violence escalated when Grau employed gang leaders as police officers, and Castro received a death threat urging him to leave the university, but he refused and began to carry a gun and surround himself with armed friends. Anti-Castro dissidents accused him of committing gang-related assassinations at the time, but these accusations remain unproven.


Rebellion and Marxism: 1947–1950

I joined the people; I grabbed a rifle in a police station that collapsed when it was rushed by a crowd. I witnessed the spectacle of a totally spontaneous revolution ... hat experience led me to identify myself even more with the cause of the people. My still incipient Marxist ideas had nothing to do with our conduct—it was a spontaneous reaction on our part, as young people with Martí-an, anti-imperialist, anti-colonialist and pro-democratic ideas.


In June 1947, Castro joined a planned expedition to overthrow the government of Rafael Trujillo in the Dominican Republic. The military force intended to sail from Cuba in July 1947, but Grau's government stopped the invasion under US pressure, and Castro evaded arrest. Returning to Havana, Castro took a leading role in student protests against the killing of a high school pupil by government bodyguards. The protests and subsequent crackdown on suspected communists led to violent clashes between activists and police in February 1948, in which Castro was badly beaten. His subsequent public speeches took a leftist slant, condemning social and economic inequality in Cuba.


In April 1948, Castro travelled to Bogotá, Colombia, leading a Cuban student group sponsored by President Juan Perón's Argentine government. There, the assassination of leftist leader Jorge Eliécer Gaitán Ayala led to rioting and clashes between the governing Conservatives—backed by the army—and leftist Liberals. Castro joined the Liberal cause by stealing guns from a police station; subsequent police investigations concluded that he had not been involved in killings. In April 1948, the Organization of American States was founded at a summit in Bogotá, leading to protests, which Castro joined.


Marxism taught me what society was. I was like a blindfolded man in a forest, who doesn't even know where north or south is. If you don't eventually come to truly understand the history of the class struggle, or at least have a clear idea that society is divided between the rich and the poor, and that some people subjugate and exploit other people, you're lost in a forest, not knowing anything.


Returning to Cuba, Castro became a prominent figure in protests against government attempts to raise bus fares. He married Mirta Díaz Balart, through whom he was exposed to the lifestyle of the Cuban elite. The subsequent election was won by Partido Auténtico's new candidate, Carlos Prío Socarrás. Castro had moved further to the left and interpreted Cuba's problems as an integral part of capitalist society, or the "dictatorship of the bourgeoisie", rather than the failings of corrupt politicians, and adopted the Marxist view that meaningful political change could only be brought about by proletariat revolution. Visiting Havana's poorest neighbourhoods, he became active in the student anti-racist campaign.


In September 1949, Mirta gave birth to a son, Fidelito, so the couple moved to a larger Havana flat. Castro continued to put himself at risk, staying active in the city's politics and joining the 30 September Movement, which contained within it both communists and members of the Partido Ortodoxo. The group's purpose was to oppose the influence of the violent gangs within the university; despite his promises, Prío had failed to control the situation, instead offering many of their senior members jobs in government ministries.


Castro volunteered to deliver a speech for the Movement on 13 November, exposing the government's secret deals with the gangs and identifying key members. Attracting the attention of the national press, the speech angered the gangs and Castro fled into hiding, first in the countryside and then in the US. Returning to Havana several weeks later, Castro laid low and focused on his university studies, graduating as a Doctor of Law in September 1950.


Career in law and politics: 1950–1952

Castro co-founded a legal partnership that primarily catered to poor Cubans, albeit it proved a financial failure. Caring little for money or material goods, Castro failed to pay his bills; his furniture was repossessed and electricity cut off, distressing his wife. He took part in a high school protest in Cienfuegos in November 1950, fighting with police to protest the Education Ministry's ban on student associations; he was arrested and charged for violent conduct, but the magistrate dismissed the charges. His hopes for Cuba still centered on Chibás and the Partido Ortodoxo, and he was present at Chibás' politically motivated suicide in 1951.


Seeing himself as Chibás' heir, Castro wanted to run for Congress in the June 1952 elections, though senior Ortodoxo members feared his radical reputation and refused to nominate him. He was instead nominated as a candidate for the House of Representatives by party members in Havana's poorest districts and began campaigning. The Ortodoxo had considerable support and was predicted to do well in the election.


During his campaign, Castro met with General Fulgencio Batista, the former president who had returned to politics with the Unitary Action Party. Batista offered him a place in his administration if he was successful; although both opposed Prío's administration, their meeting never got beyond polite generalities. On 10 March 1952, Batista seized power in a military coup, with Prío fleeing to Mexico. Declaring himself president, Batista cancelled the planned presidential elections, describing his new system as "disciplined democracy"; Castro was deprived of being elected in his run for office by Batista's move, and like many others, considered it a one-man dictatorship.


Batista moved to the right, solidifying ties with both the wealthy elite and the United States, severing diplomatic relations with the Soviet Union, suppressing trade unions and persecuting Cuban socialist groups. Intent on opposing Batista, Castro brought several legal cases against the government, but these came to nothing, and Castro began thinking of alternative ways to oust the regime.


Cuban Revolution

The Movement and the Moncada Barracks attack: 1952–1953

Castro formed a group called "The Movement", which operated along a clandestine cell system, publishing underground newspaper El Acusador (The Accuser), while arming and training anti-Batista recruits. From July 1952 they went on a recruitment drive, gaining around 1,200 members in a year, the majority from Havana's poorer districts. Although a revolutionary socialist, Castro avoided an alliance with the communist Popular Socialist Party (PSP), fearing it would frighten away political moderates, but kept in contact with PSP members like his brother Raúl. Castro stockpiled weapons for a planned attack on the Moncada Barracks, a military garrison outside Santiago de Cuba, Oriente. Castro's militants intended to dress in army uniforms and arrive at the base on 25 July, seizing control and raiding the armoury before reinforcements arrived. Supplied with new weaponry, Castro intended to spark a revolution among Oriente's impoverished cane cutters and promote further uprisings. Castro's plan emulated those of the 19th-century Cuban independence fighters who had raided Spanish barracks; Castro saw himself as the heir to independence leader José Martí.


Castro gathered 165 revolutionaries for the mission, ordering his troops not to cause bloodshed unless they met armed resistance. The attack took place on 26 July 1953, but ran into trouble; 3 of the 16 cars that had set out from Santiago failed to get there. Reaching the barracks, the alarm was raised, with most of the rebels pinned down by machine gun fire. Four were killed before Castro ordered a retreat. The rebels suffered 6 fatalities and 15 other casualties, whilst the army suffered 19 dead and 27 wounded. Meanwhile, some rebels took over a civilian hospital; subsequently stormed by government soldiers, the rebels were rounded up, tortured and 22 were executed without trial. Accompanied by 19 comrades, Castro set out for Gran Piedra in the rugged Sierra Maestra mountains, several kilometres to the north, where they could establish a guerrilla base. Responding to the attack, Batista's government proclaimed martial law, ordering a violent crackdown on dissent, and imposing strict media censorship. The government broadcast misinformation about the event, claiming that the rebels were communists who had killed hospital patients, although news and photographs of the army's use of torture and summary executions in Oriente soon spread, causing widespread public and some governmental disapproval.


Over the following days, the rebels were rounded up; some were executed and others—including Castro—transported to a prison north of Santiago. Believing Castro incapable of planning the attack alone, the government accused Ortodoxo and PSP politicians of involvement, putting 122 defendants on trial on 21 September at the Palace of Justice, Santiago. Acting as his own defence counsel, Castro cited Martí as the intellectual author of the attack and convinced the three judges to overrule the army's decision to keep all defendants handcuffed in court, proceeding to argue that the charge with which they were accused—of "organizing an uprising of armed persons against the Constitutional Powers of the State"—was incorrect, for they had risen up against Batista, who had seized power in an unconstitutional manner. The trial embarrassed the army by revealing that they had tortured suspects, after which they tried unsuccessfully to prevent Castro from testifying any further, claiming he was too ill. The trial ended on 5 October, with the acquittal of most defendants; 55 were sentenced to prison terms of between 7 months and 13 years. Castro was sentenced on 16 October, during which he delivered a speech that would be printed under the title of History Will Absolve Me. Castro was sentenced to 15 years' imprisonment in the hospital wing of the Model Prison (Presidio Modelo), a relatively comfortable and modern institution on the Isla de Pinos.


Imprisonment and 26 July Movement: 1953–1955

Imprisoned with 25 comrades, Castro renamed his group the "26th of July Movement" (MR-26-7) in memory of the Moncada attack's date, and formed a school for prisoners. He read widely, enjoying the works of Marx, Lenin, and Martí but also reading books by Freud, Kant, Shakespeare, Munthe, Maugham, and Dostoyevsky, analysing them within a Marxist framework. Corresponding with supporters, he maintained control over the Movement and organized the publication of History Will Absolve Me. Initially permitted a relative amount of freedom within the prison, he was locked up in solitary confinement after inmates sang anti-Batista songs on a visit by the president in February 1954. Meanwhile, Castro's wife Mirta gained employment in the Ministry of the Interior, something he discovered through a radio announcement. Appalled, he raged that he would rather die "a thousand times" than "suffer impotently from such an insult". Both Fidel and Mirta initiated divorce proceedings, with Mirta taking custody of their son Fidelito; this angered Castro, who did not want his son growing up in a bourgeois environment.


In 1954, Batista's government held presidential elections, but no politician stood against him; the election was widely considered fraudulent. It had allowed some political opposition to be voiced, and Castro's supporters had agitated for an amnesty for the Moncada incident's perpetrators. Some politicians suggested an amnesty would be good publicity, and the Congress and Batista agreed. Backed by the US and major corporations, Batista believed Castro to be no threat, and on 15 May 1955, the prisoners were released. Returning to Havana, Castro gave radio interviews and press conferences; the government closely monitored him, curtailing his activities. Now divorced, Castro had sexual affairs with two female supporters, Naty Revuelta and Maria Laborde, each conceiving him a child. Setting about strengthening the MR-26-7, he established an 11-person National Directorate but retained autocratic control, with some dissenters labelling him a caudillo (dictator); he argued that a successful revolution could not be run by committee and required a strong leader.


In 1955, bombings and violent demonstrations led to a crackdown on dissent, with Castro and Raúl fleeing the country to evade arrest. Castro sent a letter to the press, declaring that he was "leaving Cuba because all doors of peaceful struggle have been closed to me ... As a follower of Martí, I believe the hour has come to take our rights and not beg for them, to fight instead of pleading for them." The Castros and several comrades travelled to Mexico, where Raúl befriended an Argentine doctor and Marxist–Leninist named Ernesto "Che" Guevara, who was working as a journalist and photographer for "Agencia Latina de Noticias". Fidel liked him, later describing him as "a more advanced revolutionary than I was". Castro also associated with the Spaniard Alberto Bayo, who agreed to teach Castro's rebels the necessary skills in guerrilla warfare. Requiring funding, Castro toured the US in search of wealthy sympathizers, there being monitored by Batista's agents, who allegedly orchestrated a failed assassination attempt against him. Castro kept in contact with the MR-26-7 in Cuba, where they had gained a large support base in Oriente. Other militant anti-Batista groups had sprung up, primarily from the student movement; most notable was the Directorio Revolucionario Estudiantil (DRE), founded by José Antonio Echeverría. Antonio met with Castro in Mexico City, but Castro opposed the student's support for indiscriminate assassination.


After purchasing the decrepit yacht Granma, on 25 November 1956, Castro set sail from Tuxpan, Veracruz, with 81 armed revolutionaries. The 1,900-kilometre (1,200 mi) crossing to Cuba was harsh, with food running low and many suffering seasickness. At some points, they had to bail water caused by a leak, and at another, a man fell overboard, delaying their journey. The plan had been for the crossing to take five days, and on the Granma's scheduled day of arrival, 30 November, MR-26-7 members under Frank País led an armed uprising in Santiago and Manzanillo. However, the Granma's journey ultimately lasted seven days, and with Castro and his men unable to provide reinforcements, País and his militants dispersed after two days of intermittent attacks.


Guerrilla war: 1956–1959

The Granma ran aground in a mangrove swamp at Playa Las Coloradas, close to Los Cayuelos, on 2 December 1956. Fleeing inland, its crew headed for the forested mountain range of Oriente's Sierra Maestra, being repeatedly attacked by Batista's troops. Upon arrival, Castro discovered that only 19 rebels had made it to their destination, the rest having been killed or captured. Setting up an encampment, the survivors included the Castros, Che Guevara, and Camilo Cienfuegos. They began launching raids on small army posts to obtain weaponry, and in January 1957 they overran the outpost at La Plata, treating any soldiers that they wounded but executing Chicho Osorio, the local mayoral (land company overseer), who was despised by the local peasants and who boasted of killing one of Castro's rebels. Osorio's execution aided the rebels in gaining the trust of locals, although they largely remained unenthusiastic and suspicious of the revolutionaries. As trust grew, some locals joined the rebels, although most new recruits came from urban areas. With volunteers boosting the rebel forces to over 200, in July 1957 Castro divided his army into three columns, commanded by himself, his brother, and Guevara. The MR-26-7 members operating in urban areas continued agitation, sending supplies to Castro, and on 16 February 1957, he met with other senior members to discuss tactics; here he met Celia Sánchez, who would become a close friend.


Across Cuba, anti-Batista groups carried out bombings and sabotage; police responded with mass arrests, torture, and extrajudicial executions. In March 1957, the DRE launched a failed attack on the presidential palace, during which Antonio was shot dead. Batista's government often resorted to brutal methods to keep Cuba's cities under control. In the Sierra Maestra mountains, Castro was joined by Frank Sturgis who offered to train Castro's troops in guerrilla warfare. Castro accepted the offer, but he also had an immediate need for guns and ammunition, so Sturgis became a gunrunner. Sturgis purchased boatloads of weapons and ammunition from Central Intelligence Agency (CIA) weapons expert Samuel Cummings' International Armament Corporation in Alexandria, Virginia. Sturgis opened a training camp in the Sierra Maestra mountains, where he taught Che Guevara and other 26 July Movement rebel soldiers guerrilla warfare. Frank País was also killed, leaving Castro the MR-26-7's unchallenged leader. Although Guevara and Raúl were well known for their Marxist–Leninist views, Castro hid his, hoping to gain the support of less radical revolutionaries. In 1957 he met with leading members of the Partido Ortodoxo, Raúl Chibás and Felipe Pazos, authoring the Sierra Maestra Manifesto, in which they demanded that a provisional civilian government be set up to implement moderate agrarian reform, industrialization, and a literacy campaign before holding multiparty elections. As Cuba's press was censored, Castro contacted foreign media to spread his message; he became a celebrity after being interviewed by Herbert Matthews, a journalist from The New York Times. Reporters from CBS and Paris Match soon followed.


Castro's guerrillas increased their attacks on military outposts, forcing the government to withdraw from the Sierra Maestra region, and by spring 1958, the rebels controlled a hospital, schools, a printing press, slaughterhouse, land-mine factory and a cigar-making factory. By 1958, Batista was under increasing pressure, a result of his military failures coupled with increasing domestic and foreign criticism surrounding his administration's press censorship, torture, and extrajudicial executions. Influenced by anti-Batista sentiment among their citizens, the US government ceased supplying him with weaponry. The opposition called a general strike, accompanied by armed attacks from the MR-26-7. Beginning on 9 April, it received strong support in central and eastern Cuba, but little elsewhere.


Batista responded with an all-out-attack, Operation Verano, in which the army aerially bombarded forested areas and villages suspected of aiding the militants, while 10,000 soldiers commanded by General Eulogio Cantillo surrounded the Sierra Maestra, driving north to the rebel encampments. Despite their numerical and technological superiority, the army had no experience with guerrilla warfare, and Castro halted their offensive using land mines and ambushes. Many of Batista's soldiers defected to Castro's rebels, who also benefited from local popular support. In the summer, the MR-26-7 went on the offensive, pushing the army out of the mountains, with Castro using his columns in a pincer movement to surround the main army concentration in Santiago. By November, Castro's forces controlled most of Oriente and Las Villas, and divided Cuba in two by closing major roads and rail lines, severely disadvantaging Batista.


The US instructed Cantillo to oust Batista due to fears in Washington that Castro was a socialist, which were exacerbated by the association between nationalist and communist movements in Latin America and the links between the Cold War and decolonization. By this time the great majority of Cuban people had turned against the Batista regime. Ambassador to Cuba, E. T. Smith, who felt the whole CIA mission had become too close to the MR-26-7 movement, personally went to Batista and informed him that the US would no longer support him and felt he no longer could control the situation in Cuba. General Cantillo secretly agreed to a ceasefire with Castro, promising that Batista would be tried as a war criminal; however, Batista was warned, and fled into exile with over US$300 million on 31 December 1958. Cantillo entered Havana's Presidential Palace, proclaimed the Supreme Court judge Carlos Piedra to be president, and began appointing the new government. Furious, Castro ended the ceasefire, and ordered Cantillo's arrest by sympathetic figures in the army. Accompanying celebrations at news of Batista's downfall on 1 January 1959, Castro ordered the MR-26-7 to prevent widespread looting and vandalism. Cienfuegos and Guevara led their columns into Havana on 2 January, while Castro entered Santiago and gave a speech invoking the wars of independence. Heading toward Havana, he greeted cheering crowds at every town, giving press conferences and interviews. Castro reached Havana on 9 January 1959.


Provisional government

Consolidating leadership: 1959

At Castro's command, the politically moderate lawyer Manuel Urrutia Lleó was proclaimed provisional president, but Castro announced falsely that Urrutia had been selected by "popular election". Most of Urrutia's cabinet were MR-26-7 members. Entering Havana, Castro proclaimed himself Representative of the Rebel Armed Forces of the Presidency, setting up home and office in the penthouse of the Havana Hilton Hotel. Castro exercised a great deal of influence over Urrutia's regime, now ruling by decree. He ensured the government implemented policies to cut corruption and fight illiteracy, and that it attempted to remove Batistanos from positions of power by dismissing Congress and barring all those elected in the rigged elections of 1954 and 1958 from future office. He then pushed Urrutia to issue a temporary ban on political parties; he repeatedly said that they would eventually hold multiparty elections. Although repeatedly denying that he was a communist to the press, he began clandestinely meeting members of the PSP to discuss the creation of a socialist state.


We are not executing innocent people or political opponents. We are executing murderers and they deserve it.


In suppressing the revolution, Batista's government had killed thousands of Cubans; Castro and influential sectors of the press put the death toll at 20,000, but a list of victims published shortly after the revolution contained only 898 names—over half of them combatants. More recent estimates place the death toll between 1,000 and 4,000. In response to popular uproar, which demanded that those responsible be brought to justice, Castro helped to set up many trials, resulting in hundreds of executions. Although popular domestically, critics—in particular the US press, argued that many were not fair trials. Castro responded that "revolutionary justice is not based on legal precepts, but on moral conviction."
Acclaimed by many across Latin America, he travelled to Venezuela where he met with President-elect Rómulo Betancourt, unsuccessfully requesting a loan and a new deal for Venezuelan oil. Returning home, an argument between Castro and senior government figures broke out. He was infuriated that the government had left thousands unemployed by closing down casinos and brothels. As a result, Prime Minister José Miró Cardona resigned, going into exile in the US and joining the anti-Castro movement.


On 16 February 1959, Castro was sworn in as Prime Minister of Cuba. Castro also appointed himself president of the National Tourist Industry, introducing unsuccessful measures to encourage African-American tourists to visit, advertising Cuba as a tropical paradise free of racial discrimination. Judges and politicians had their pay reduced while low-level civil servants saw theirs raised, and in March 1959, Castro declared rents for those who paid less than $100 a month halved. The Cuban government also began to expropriate the casinos and properties from mafia leaders and taking millions in cash. Before his death, Russian-American gangster Meyer Lansky said Cuba "ruined" him.


On 9 April, Castro announced that the elections, which the 26th of July Movement had promised would occur after the revolution, would be postponed, so that the provisional government could focus on domestic reform. Castro announced this electoral delay with the slogan: "revolution first, elections later".


Later in April, he visited the US on a charm offensive where President Dwight D. Eisenhower would not meet with him, but instead sent Vice President Richard Nixon, whom Castro instantly disliked. After meeting Castro, Nixon described him to Eisenhower: "The one fact we can be sure of is that Castro has those indefinable qualities which made him a leader of men. Whatever we may think of him he is going to be a great factor in the development of Cuba and very possibly in Latin American affairs generally. He seems to be sincere. He is either incredibly naive about Communism or under Communist discipline-my guess is the former...His ideas as to how to run a government or an economy are less developed than those of almost any world figure I have met in fifty countries. But because he has the power to lead...we have no choice but at least try to orient him in the right direction".


Proceeding to Canada, Trinidad, Brazil, Uruguay and Argentina, Castro attended an economic conference in Buenos Aires, unsuccessfully proposing a $30 billion US-funded "Marshall Plan" for Latin America. In May 1959, Castro signed into law the First Agrarian Reform, setting a cap for landholdings to 993 acres (402 ha) per owner and prohibiting foreigners from obtaining Cuban land ownership. Around 200,000 peasants received title deeds as large land holdings were broken up; popular among the working class, it alienated the richer landowners, including Castro's own mother, whose farmlands were taken. Within a year, Castro and his government had effectively redistributed 15 per cent of the nation's wealth, declaring that "the revolution is the dictatorship of the exploited against the exploiters."


In the summer of 1959, Fidel began nationalizing plantation lands owned by American investors as well as confiscating the property of foreign landowners. He also seized property previously held by wealthy Cubans who had fled. He nationalized sugar production and oil refinement, over the objection of foreign investors who owned stakes in these commodities.


Although then refusing to categorize his regime as socialist and repeatedly denying being a communist, Castro appointed Marxists to senior government and military positions. President Urrutia increasingly expressed concern with the rising influence of Marxism. Angered, Castro in turn announced his resignation as prime minister on 18 July—blaming Urrutia for complicating government with his "fevered anti-Communism". Over 500,000 Castro-supporters surrounded the Presidential Palace demanding Urrutia's resignation, which he submitted. On 23 July, Castro resumed his premiership and appointed Marxist Osvaldo Dorticós as president.


On October 19, 1959, army commander Huber Matos wrote a resignation letter to Fidel Castro, complaining of Communist influence in government. Matos lamented in his resignation that communists were gaining positions of power that he felt were undeserved for having not participated in the Cuban Revolution. Matos planned for his officers to also resign en masse in support. Two days later, Castro sent fellow revolutionary Camilo Cienfuegos to arrest Matos. The same day Matos was arrested, Cuban exile Pedro Luis Díaz Lanz, a former air force chief of staff under Castro and friend of Huber Matos, flew from Florida and dropped leaflets into Havana that called for the removal of all Communists from the government. In response, Castro held a rally where he called for the reintroduction of revolutionary tribunals to try Matos and Diaz for treason. Shortly after Hubert Matos' detention various other disillusioned economists would send in their resignations. Felipe Pazos would resign as head of the National Bank and be replaced within a month by Che Guevara. Cabinet members Manuel Ray and Faustino Perez also resigned.


Castro's government continued to emphasise social projects to improve Cuba's standard of living, often to the detriment of economic development. Major emphasis was placed on education, and during the first 30 months of Castro's government, more classrooms were opened than in the previous 30 years. The Cuban primary education system offered a work-study program, with half of the time spent in the classroom, and the other half in a productive activity. Health care was nationalized and expanded, with rural health centers and urban polyclinics opening up across the island to offer free medical aid. Universal vaccination against childhood diseases was implemented, and infant mortality rates were reduced dramatically. A third part of this social program was the improvement of infrastructure. Within the first six months of Castro's government, 1,000 km (600 mi) of roads were built across the island, while $300 million was spent on water and sanitation projects. Over 800 houses were constructed every month in the early years of the administration in an effort to cut homelessness, while nurseries and day-care centers were opened for children and other centers opened for the disabled and elderly.


Diplomatic and political shifts: 1960

Castro used radio and television to develop a "dialogue with the people", posing questions and making provocative statements. His regime remained popular with workers, peasants, and students, who constituted the majority of the country's population, while opposition came primarily from the middle class; thousands of doctors, engineers and other professionals emigrated to Florida in the US, causing an economic brain drain. Productivity decreased and the country's financial reserves were drained within two years. After conservative press expressed hostility towards the government, the pro-Castro printers' trade union disrupted editorial staff, and in January 1960 the government ordered them to publish a "coletilla" (clarification) written by the printers' union at the end of articles critical of the government. Castro's government arrested hundreds of counter-revolutionaries, many of whom were subjected to solitary confinement, rough treatment, and threatening behaviour. Militant anti-Castro groups, funded by exiles, the CIA, and the Dominican government, undertook armed attacks and set up guerrilla bases in Cuba's mountains, leading to the six-year Escambray Rebellion.


At the time, 1960, the Cold War raged between two superpowers: the United States, a capitalist liberal democracy, and the Soviet Union (USSR), a Marxist–Leninist socialist state ruled by the Communist Party. Expressing contempt for the US, Castro shared the ideological views of the USSR, establishing relations with several Marxist–Leninist states. Meeting with Soviet First Deputy Premier Anastas Mikoyan, Castro agreed to provide the USSR with sugar, fruit, fibres, and hides in return for crude oil, fertilizers, industrial goods, and a $100 million loan. Cuba's government ordered the country's refineries—then controlled by the US corporations Shell and Esso—to process Soviet oil, but under US pressure they refused. Castro responded by expropriating and nationalizing the refineries. Retaliating, the US cancelled its import of Cuban sugar, provoking Castro to nationalize most US-owned assets on the island, including banks and sugar mills.


Relations between Cuba and the US were further strained following the explosion of a French vessel, the La Coubre, in Havana harbour in March 1960. The ship carried weapons purchased from Belgium, and the cause of the explosion was never determined, but Castro publicly insinuated that the US government was guilty of sabotage. He ended this speech with "¡Patria o Muerte!" ("Fatherland or Death"), a proclamation that he made much use of in ensuing years. Inspired by their earlier success with the 1954 Guatemalan coup d'état, in March 1960, US President Eisenhower authorized the CIA to overthrow Castro's government. He provided them with a budget of $13 million and permitted them to ally with the Mafia, who were aggrieved that Castro's government closed down their brothel and casino businesses in Cuba.


During a May Day speech in 1960, Fidel Castro announced that all future elections would be cancelled. Castro proclaimed that his administration was a direct democracy, in which Cubans could assemble at demonstrations to express their will, thus there was no need for elections, claiming that representative democratic systems served the interests of socio-economic elites. US Secretary of State Christian Herter announced that Cuba was adopting the Soviet model of rule, with a one-party state, government control of trade unions, suppression of civil liberties, and the absence of freedom of speech and press.


In September 1960, Castro flew to New York City for the General Assembly of the United Nations. Staying at the Hotel Theresa in Harlem, he met with journalists and anti-establishment figures like Malcolm X. Castro had decided to stay in Harlem as a way of expressing solidarity with the poor African-American population living there, thus leading to an assortment of world leaders such as Nasser of Egypt and Nehru of India having to drive out to Harlem to see him. He also met Soviet premier Nikita Khrushchev, with the two publicly condemning the poverty and racism faced by Americans in areas like Harlem. Relations between Castro and Khrushchev were warm; they led the applause to one another's speeches at the General Assembly. The opening session of the United Nations General Assembly in September 1960 was a highly rancorous one with Khrushchev famously banging his shoe against his desk to interrupt a speech by Filipino delegate Lorenzo Sumulong, which set the general tone for the debates and speeches. Castro delivered the longest speech ever held before the United Nations General Assembly, speaking for four and a half hours in a speech mostly given over to denouncing American policies towards Latin America. Subsequently, visited by Polish first secretary Władysław Gomułka, Bulgarian first secretary Todor Zhivkov, Egyptian president Gamal Abdel Nasser, and Indian premier Jawaharlal Nehru, Castro also received an evening's reception from the Fair Play for Cuba Committee.


Back in Cuba, Castro feared a US-backed coup; in 1959 his regime spent $120 million on Soviet, French, and Belgian weaponry and by early 1960 had doubled the size of Cuba's armed forces. Fearing counter-revolutionary elements in the army, the government created a People's Militia to arm citizens favourable to the revolution, training at least 50,000 civilians in combat techniques. In September 1960, they created the Committees for the Defense of the Revolution (CDR), a nationwide civilian organization which implemented neighbourhood spying to detect counter-revolutionary activities as well as organizing health and education campaigns, becoming a conduit for public complaints. By 1970, a third of the population would be involved in the CDR, and this would eventually rise to 80%.
On 13 October 1960, the US prohibited the majority of exports to Cuba, initiating an economic embargo. In retaliation, the National Institute for Agrarian Reform INRA took control of 383 private-run businesses on 14 October, and on 25 October a further 166 US companies operating in Cuba had their premises seized and nationalized. On 16 December, the US ended its import quota of Cuban sugar, the country's primary export.


Bay of Pigs Invasion and "Socialist Cuba": 1961–1962

There was ... no doubt about who the victors were. Cuba's stature in the world soared to new heights, and Fidel's role as the adored and revered leader among ordinary Cuban people received a renewed boost. His popularity was greater than ever. In his own mind he had done what generations of Cubans had only fantasized about: he had taken on the United States and won.


In January 1961, Castro ordered Havana's US Embassy to reduce its 300-member staff, suspecting that many of them were spies. The US responded by ending diplomatic relations, and it increased CIA funding for exiled dissidents; these militants began attacking ships that traded with Cuba, and bombed factories, shops, and sugar mills. Despite internal tensions, and diplomatic tensions, Castro garnered support in New York City. On 18 February 1961, 400 people—mainly Cubans, Puerto Ricans, and college students—picketed in the rain outside of the United Nations rallying for Castro's anti-colonial values and his effort to reduce the United States' power over Cuba. The protesters held up signs that read, "Mr. Kennedy, Cuba is Not For Sale.", "Viva Fidel Castro!" and "Down With Yankee Imperialism!". Around 200 policemen were on the scene, but the protesters continued to chant slogans and throw pennies in support of Fidel Castro's socialist movement. Some Americans disagreed with President John F. Kennedy's decision to ban trade with Cuba, and outwardly supported his nationalist revolutionary tactics.


Both President Eisenhower and his successor President Kennedy supported a CIA plan to aid a dissident militia: the Democratic Revolutionary Front, to invade Cuba and overthrow Castro; the plan resulted in the Bay of Pigs Invasion in April 1961. On 15 April, CIA-supplied B-26s bombed three Cuban military airfields; the US announced that the perpetrators were defecting Cuban air force pilots, but Castro exposed these claims as false flag misinformation. Fearing invasion, he ordered the arrest of between 20,000 and 100,000 suspected counter-revolutionaries, publicly proclaiming, "What the imperialists cannot forgive us, is that we have made a Socialist revolution under their noses", his first announcement that the government was socialist.


The CIA and the Democratic Revolutionary Front had based a 1,400-strong army, Brigade 2506, in Nicaragua. On the night of 16 to 17 April, Brigade 2506 landed along Cuba's Bay of Pigs and engaged in a firefight with a local revolutionary militia. Castro ordered Captain José Ramón Fernández to launch the counter-offensive, before taking personal control of it. After bombing the invaders' ships and bringing in reinforcements, Castro forced the Brigade to surrender on 20 April. He ordered the 1189 captured rebels to be interrogated by a panel of journalists on live television, personally taking over the questioning on 25 April. Fourteen were put on trial for crimes allegedly committed before the revolution, while the others were returned to the US in exchange for medicine and food valued at US$25 million. Castro's victory reverberated around the world, especially in Latin America, but it also increased internal opposition primarily among the middle-class Cubans who had been detained in the run-up to the invasion. Although most were freed within a few days, many fled to the US, establishing themselves in Florida.


After the banning of the film P.M., film critics hotly debated censorship in Cuba, which then caused the intervention of Castro, who met with the contesting writers and delivered his famed "Words to the Intellectuals" speech; which he delivered in June 1961. In the speech, Castro commented on Cuba's censorship policy, stating:


This means that within the Revolution, everything goes; against the Revolution, nothing. Nothing against the Revolution, because the Revolution has its rights also, and the first right of the Revolution is the right to exist, and no one can stand against the right of the Revolution to be and to exist, No one can rightfully claim a right against the Revolution. Since it takes in the interests of the people and Signifies the interests of the entire nation.

In an effort to consolidate "Socialist Cuba", Castro united the MR-26-7, PSP and Revolutionary Directorate into a governing party based on the Leninist principle of democratic centralism, what resulted was the Integrated Revolutionary Organizations (Organizaciones Revolucionarias Integradas – ORI), eventually renamed the United Party of the Cuban Socialist Revolution (PURSC) in 1962. The ORI began shaping Cuba using the Soviet model, persecuting political opponents and perceived social deviants such as prostitutes and homosexuals; Castro considered same-sex sexual activity a bourgeois trait.
Although the USSR was hesitant regarding Castro's embrace of socialism, relations with the Soviets deepened. Castro sent Fidelito for a Moscow schooling, Soviet technicians arrived on the island, and Castro was awarded the Lenin Peace Prize.


In order to plan the Cuban economy, the commission JUCEPLAN was tasked with creating a four year plan. Regino Boti, the head of JUCEPLAN, announced in August 1961, that the country would soon have a 10% rate of economic growth, and the highest living standard in Latin America in 10 years. The plan drafted by JUCEPLAN in 1961, was a four year plan devised to be implemented in 1962 through 1965. It stressed agricultural diversification and rapid industrialization via Soviet assistance. In September 1961, Castro publicly complained that the industrialization plan had stalled because of lazy uncooperative workers.


In December 1961, Castro admitted that he had been a Marxist–Leninist for years, and in his Second Declaration of Havana he called on Latin America to rise up in revolution. In response, the US successfully pushed the Organization of American States to expel Cuba; the Soviets privately reprimanded Castro for recklessness, although he received praise from China. Despite their ideological affinity with China, in the Sino-Soviet split, Cuba allied with the wealthier Soviets, who offered economic and military aid.


By 1962, Cuba's economy was in steep decline, a result of poor economic management and low productivity coupled with the US trade embargo. Food shortages led to rationing, resulting in protests in Cárdenas. Security reports indicated that many Cubans associated austerity with the "Old Communists" of the PSP, while Castro considered a number of them—namely Aníbal Escalante and Blas Roca—unduly loyal to Moscow. In March 1962 Castro removed the most prominent "Old Communists" from office, labelling them "sectarian". On a personal level, Castro was increasingly lonely, and his relations with Guevara became strained as the latter became increasingly anti-Soviet and pro-Chinese.


Cuban Missile Crisis and furthering socialism: 1962–1968

Militarily weaker than NATO, Khrushchev wanted to install Soviet R-12 MRBM nuclear missiles on Cuba to even the power balance. Although conflicted, Castro agreed, believing it would guarantee Cuba's safety and enhance the cause of socialism. Undertaken in secrecy, only the Castro brothers, Guevara, Dorticós and security chief Ramiro Valdés knew the full plan. Upon discovering it through aerial reconnaissance, in October the US implemented an island-wide quarantine to search vessels headed to Cuba, sparking the Cuban Missile Crisis. The US saw the missiles as offensive; Castro insisted they were for defence only. Castro urged that Khrushchev should launch a nuclear strike on the US if Cuba were invaded, but Khrushchev was desperate to avoid nuclear war. Castro was left out of the negotiations, in which Khrushchev agreed to remove the missiles in exchange for a US commitment not to invade Cuba and an understanding that the US would remove their MRBMs from Turkey and Italy. Feeling betrayed by Khrushchev, Castro was furious and soon fell ill. Proposing a five-point plan, Castro demanded that the US end its embargo, withdraw from Guantanamo Bay Naval Base, cease supporting dissidents, and stop violating Cuban air space and territorial waters. He presented these demands to U Thant, visiting Secretary-General of the United Nations, but the US ignored them. In turn Castro refused to allow the UN's inspection team into Cuba.


In May 1963, Castro visited the USSR at Khrushchev's personal invitation, touring 14 cities, addressing a Red Square rally, and being awarded both the Order of Lenin and an honorary doctorate from Moscow State University. Castro returned to Cuba with new ideas; inspired by Soviet newspaper Pravda, he amalgamated Hoy and Revolución into a new daily, Granma, and oversaw large investment into Cuban sport that resulted in an increased international sporting reputation. Seeking to further consolidate control, in 1963 the government cracked down on Protestant sects in Cuba, with Castro labelling them counter-revolutionary "instruments of imperialism"; many preachers were found guilty of illegal US links and imprisoned. Measures were implemented to force perceived idle and delinquent youths to work, primarily through the introduction of mandatory military service. In September, the government temporarily permitted emigration for anyone other than males aged between 15 and 26, thereby ridding the government of thousands of critics, most of whom were from upper and middle-class backgrounds. In 1963, Castro's mother died. This was the last time his private life was reported in Cuba's press. In January 1964, Castro returned to Moscow, officially to sign a new five-year sugar trade agreement, but also to discuss the ramifications of the assassination of John F. Kennedy. Castro was deeply concerned by the assassination, believing that a far-right conspiracy was behind it but that the Cubans would be blamed. In October 1965, the Integrated Revolutionary Organizations was officially renamed the "Cuban Communist Party" and published the membership of its Central Committee.


Beginning in 1965, gay men were forced into the Military Units to Aid Production (Unidades Militares de Ayuda a la Producción – UMAP). However, after many revolutionary intellectuals decried this move, the UMAP camps were closed in 1967, although gay men continued to be imprisoned.


The greatest threat presented by Castro's Cuba is as an example to other Latin American states which are beset by poverty, corruption, feudalism, and plutocratic exploitation ... his influence in Latin America might be overwhelming and irresistible if, with Soviet help, he could establish in Cuba a Communist utopia.


Despite Soviet misgivings, Castro continued to call for global revolution, funding militant leftists and those engaged in national liberation struggles. Cuba's foreign policy was strongly anti-imperialist, believing that every nation should control its own natural resources. He supported Che Guevara's "Andean project", an unsuccessful plan to set up a guerrilla movement in the highlands of Bolivia, Peru and Argentina. He allowed revolutionary groups from around the world, from the Viet Cong to the Black Panthers, to train in Cuba.
He considered Western-dominated Africa to be ripe for revolution and sent troops and medics to aid Ahmed Ben Bella's socialist regime in Algeria during the Sand War. He also allied with Alphonse Massamba-Débat's socialist government in Congo-Brazzaville. In 1965, Castro authorized Che Guevara to travel to Congo-Kinshasa to train revolutionaries against the Western-backed government. Castro was personally devastated when Guevara was killed by CIA-backed troops in Bolivia in October 1967 and publicly attributed it to Guevara's disregard for his own safety.


In 1966, Castro staged a Tri-Continental Conference of Africa, Asia and Latin America in Havana, further establishing himself as a significant player on the world stage. From this conference, Castro created the Latin American Solidarity Organization (OLAS), which adopted the slogan of "The duty of a revolution is to make revolution", signifying Havana's leadership of Latin America's revolutionary movement.


Castro's increasing role on the world stage strained his relationship with the USSR, now under the leadership of Leonid Brezhnev. Asserting Cuba's independence, Castro refused to sign the Treaty on the Non-Proliferation of Nuclear Weapons, declaring it a Soviet-US attempt to dominate the Third World. Diverting from Soviet Marxist doctrine, he suggested that Cuban society could evolve straight to pure communism rather than gradually progress through various stages of socialism. In turn, the Soviet-loyalist Aníbal Escalante began organizing a government network of opposition to Castro, though in January 1968, he and his supporters were arrested for allegedly passing state secrets to Moscow. Recognising Cuba's economic dependence on the Soviets, Castro relented to Brezhnev's pressure to be obedient, and in August 1968 he denounced the leaders of the Prague Spring and praised the Warsaw Pact invasion of Czechoslovakia.


Influenced by China's Great Leap Forward, in 1968 Castro proclaimed a Great Revolutionary Offensive, closing all remaining privately owned shops and businesses and denouncing their owners as capitalist counterrevolutionaries. The severe lack of consumer goods for purchase led productivity to decline, as large sectors of the population felt little incentive to work hard. This was exacerbated by the perception that a revolutionary elite had emerged, consisting of those connected to the administration; they had access to better housing, private transportation, servants, and the ability to purchase luxury goods abroad.


Grey years and Third World politics: 1969–1974

Castro publicly celebrated his administration's 10th anniversary in January 1969; in his celebratory speech he warned of sugar rations, reflecting the nation's economic problems. The 1969 crop was heavily damaged by a hurricane, and to meet its export quota, the government drafted in the army, implemented a seven-day working week, and postponed public holidays to lengthen the harvest. When that year's production quota was not met, Castro offered to resign during a public speech, but assembled crowds insisted he remain. Despite the economic issues, many of Castro's social reforms were popular, with the population largely supportive of the "Achievements of the Revolution" in education, medical care, housing, and road construction, as well as the policies of "direct democratic" public consultation. Seeking Soviet help, from 1970 to 1972 Soviet economists re-organized Cuba's economy, founding the Cuban-Soviet Commission of Economic, Scientific and Technical Collaboration, while Soviet premier Alexei Kosygin visited in October 1971. In July 1972, Cuba joined the Council for Mutual Economic Assistance (Comecon), an economic organization of socialist states, although this further limited Cuba's economy to agricultural production.


In May 1970, the crews of two Cuban fishing boats were kidnapped by Florida-based dissident group Alpha 66, who demanded that Cuba release imprisoned militants. Under US pressure, the hostages were released, and Castro welcomed them back as heroes. In April 1971, Castro was internationally condemned for ordering the arrest of dissident poet Heberto Padilla who had been arrested 20 March; Padilla was freed, but the government established the National Cultural Council to ensure that intellectuals and artists supported the administration.


In November 1971, Castro visited Chile, where Marxist President Salvador Allende had been elected as the head of a left-wing coalition. Castro supported Allende's socialist reforms but warned him of right-wing elements in Chile's military. In 1973, the military led a coup d'état and established a military junta led by Augusto Pinochet. Castro proceeded to Guinea to meet socialist President Sékou Touré, praising him as Africa's greatest leader, and there received the Order of Fidelity to the People. He then went on a seven-week tour visiting leftist allies: Algeria, Bulgaria, Hungary, Poland, East Germany, Czechoslovakia and the Soviet Union, where he was given further awards. On each trip, he was eager to visit factory and farm workers, publicly praising their governments; privately, he urged the regimes to aid revolutionary movements elsewhere, particularly those fighting the Vietnam War.


In September 1973, he returned to Algiers to attend the Fourth Summit of the Non-Aligned Movement (NAM). Various NAM members were critical of Castro's attendance, claiming that Cuba was aligned to the Warsaw Pact and therefore should not be at the conference. At the conference he publicly broke off relations with Israel, citing its government's close relationship with the US and its treatment of Palestinians during the Israel–Palestine conflict. This earned Castro respect throughout the Arab world, in particular from the Libyan leader Muammar Gaddafi, who became a friend and ally. As the Yom Kippur War broke out in October 1973 between Israel and an Arab coalition led by Egypt and Syria, Cuba sent 4,000 troops to aid Syria. Leaving Algiers, Castro visited Iraq and North Vietnam.


Cuba's economy grew in 1974 as a result of high international sugar prices and new credits with Argentina, Canada, and parts of Western Europe. A number of Latin American states called for Cuba's re-admittance into the Organization of American States (OAS), with the US finally conceding in 1975 on Henry Kissinger's advice. Cuba's government underwent a restructuring along Soviet lines, claiming that this would further democratization and decentralize power away from Castro. Officially announcing Cuba's identity as a socialist state, the first National Congress of the Cuban Communist Party was held, and a new constitution drafted that abolished the position of president and prime minister. Castro remained the dominant figure in governance, taking the presidency of the newly created Council of State and Council of Ministers, making him both head of state and head of government.


Castro considered Africa to be "the weakest link in the imperialist chain", and at the request of Agostinho Neto he ordered 230 military advisers into Angola in November 1975 to aid Neto's Marxist MPLA in the Angolan Civil War. When the US and South Africa stepped up their support of the opposition FLNA and UNITA, Castro ordered a further 18,000 troops to Angola, which played a major role in forcing a South African and UNITA retreat. The decision to intervene in Angola has been a controversial one, all the more so as Castro's critics have charged that it was not his decision at all, contending that the Soviets ordered him to do so. Castro always maintained that he took the decision to launch Operation Carlota himself in response to an appeal from Neto and that the Soviets were in fact opposed to Cuban intervention in Angola, which took place over their opposition.


Traveling to Angola, Castro celebrated with Neto, Sékou Touré and Guinea-Bissaun president Luís Cabral, where they agreed to support Mozambique's Marxist–Leninist government against RENAMO in the Mozambican Civil War. In February, Castro visited Algeria and then Libya, where he spent ten days with Gaddafi and oversaw the establishment of the Jamahariya system of governance, before attending talks with the Marxist government of South Yemen. From there he proceeded to Somalia, Tanzania, Mozambique and Angola where he was greeted by crowds as a hero for Cuba's role in opposing apartheid South Africa. Throughout much of Africa he was hailed as a friend to national liberation from foreign dominance. This was followed with visits to East Berlin and Moscow.


Constitutional government

Institutionalization and interventions: 1976-1979

Up until 1976, Cuba had been managed by a provisional government, headed by Fidel Castro, without a constitution. Cuba then adopted a new constitution in 1976, based on the 1936 Soviet Constitution. This adoption marked the end of 16 years of non-constitutional government. Up until this point, Castro had simply ruled by decree, but after the 1976 constitution, the Communist Party became the official decision-making body in Cuba. Some scholars like Peter Roman, Nino Pagliccia, and Loreen Collin have written books concluding that the system that developed after the 1976 constitution, particularly the National Assembly of People's Power, are part of a highly participatory democracy. Julio Cesar Guache offers a critical view of the "democracy" that developed, and argues it is informally controlled by the Committees for the Defense of the Revolution, who vet candidates. Samuel Farber argues that the National Assembly of People's Power is legally prohibited from political debate, and that real decision-making power lied for a long time with the Castro brothers as heads of the Communist Party of Cuba. Farber mentions that the Communist Party often passes legislation without any consideration from the National Assembly of People's Power. Fidel Castro would remain in the leadership position of First Secretary of the Communist Party of Cuba for 49 years, until stepping down in 2011.


There is often talk of human rights, but it is also necessary to talk of the rights of humanity. Why should some people walk barefoot, so that others can travel in luxurious cars? Why should some live for thirty-five years, so that others can live for seventy years? Why should some be miserably poor, so that others can be hugely rich? I speak on behalf of the children in the world who do not have a piece of bread. I speak on the behalf of the sick who have no medicine, of those whose rights to life and human dignity have been denied.


In 1977, the Ogaden War broke out over the disputed Ogaden region as Somalia invaded Ethiopia; although a former ally of Somali president Siad Barre, Castro had warned him against such action, and Cuba sided with Mengistu Haile Mariam's Marxist government of Ethiopia. In a desperate attempt to stop the war, Castro had a summit with Barre where he proposed a federation of Ethiopia, Somalia, and South Yemen as an alternative to war. Barre who saw seizing the Ogaden as the first step towards creating a greater Somalia that would unite all of the Somalis into one state rejected the federation offer and decided upon war. Castro sent troops under the command of General Arnaldo Ochoa to aid the overwhelmed Ethiopian army. Mengistu's regime was barely hanging on by 1977, having lost one-third of its army in Eritrea at the time of the Somali invasion. The intervention of 17,000 Cuban troops into the Ogaden was by all accounts decisive in altering a war that Ethiopia was on the brink of losing into a victory. After forcing back the Somalis, Mengistu then ordered the Ethiopians to suppress the Eritrean People's Liberation Front, a measure Castro refused to support.


On 22 December 1977, the Cuban exile group known as the "Antonio Maceo Brigade" took their first trip to Cuba, with the aim of cultural and political reconciliation. This visit came at the request of the Cuban government, after President Jimmy Carter briefly lifted the travel ban with Cuba. The brigade consisted of 55 Cuban exiles, who toured Cuba for two weeks. After the visit, Fidel Castro would call for dialogues with Cuban exiles abroad. These dialogues resulted in the release of political prisoners, family unifications, and relaxing of restrictions to visit Cuba.


Castro extended support to Latin American revolutionary movements, namely the Sandinista National Liberation Front in its overthrow of the Nicaraguan rightist government of Anastasio Somoza Debayle in July 1979. Castro's critics accused the government of wasting Cuban lives in these military endeavours; the anti-Castro Center for a Free Cuba has claimed that an estimated 14,000 Cubans were killed in foreign Cuban military actions. When American critics claimed that Castro had no right to interfere in these nations, he countered that Cuba had been invited into them, pointing out the US's own involvement in various foreign nations. Between 1979 and 1991 about 370,000 Cuban troops together with 50,000 Cuban civilians (mostly teachers and doctors) served in Angola, representing about 5% of Cuba's population. The Cuban intervention in Angola was envisioned as a short-term commitment, but the Angolan government used the profits from the oil industry to subsidize Cuba's economy, making Cuba as economically dependent upon Angola as Angola was militarily dependent upon Cuba.


In the late 1970s, Cuba's relations with North American states improved during the period with Mexican president Luis Echeverría, Canadian prime minister Pierre Trudeau, and US president Jimmy Carter in power. Carter continued criticizing Cuba's human rights abuses but adopted a respectful approach which gained Castro's attention. Considering Carter well-meaning and sincere, Castro freed certain political prisoners and allowed some Cuban exiles to visit relatives on the island, hoping that in turn Carter would abolish the economic embargo and stop CIA support for militant dissidents. Conversely, his relationship with China declined, as he accused Deng Xiaoping's Chinese government of betraying their revolutionary principles by initiating trade links with the US and attacking Vietnam. In 1979, the Conference of the Non-Aligned Movement (NAM) was held in Havana, where Castro was selected as NAM president, a position he held until 1982. In his capacity as both president of the NAM and of Cuba he appeared at the United Nations General Assembly in October 1979 and gave a speech on the disparity between the world's rich and poor. His speech was greeted with much applause from other world leaders, though his standing in NAM was damaged by Cuba's refusal to condemn the Soviet intervention in Afghanistan.


Reagan and Gorbachev: 1980–1991

By the 1980s, Cuba's economy was again in trouble, following a decline in the market price of sugar and 1979's decimated harvest. For the first time, unemployment became a serious problem in Castro's Cuba, with the government sending unemployed youth to other countries, primarily East Germany, to work there. Desperate for money, Cuba's government secretly sold off paintings from national collections and illicitly traded for US electronic goods through Panama. Increasing numbers of Cubans fled to Florida but were labelled "scum" and "lumpen" by Castro and his CDR supporters. In one incident, 10,000 Cubans stormed the Peruvian Embassy requesting asylum, and so the US agreed that it would accept 3,500 refugees. Castro conceded that those who wanted to leave could do so from Mariel port. In what was known as the Mariel boatlift, hundreds of boats arrived from the US, leading to a mass exodus of 120,000; Castro's government took advantage of the situation by loading criminals, the mentally ill, and homosexuals onto the boats destined for Florida. The event destabilized Carter's administration, and later, in 1980, Ronald Reagan was elected US president.


Reagan's administration adopted a hard-line approach against Castro, making its desire to overthrow his regime clear. In late 1981, Castro publicly accused the US of biological warfare against Cuba by orchestrating a dengue fever epidemic. Cuba's economy became even more dependent on Soviet aid, with Soviet subsidies (mainly in the form of supplies of low-cost oil and voluntarily buying Cuban sugar at inflated prices) averaging $4–5 billion a year by the late 1980s. This accounted for 30–38% of the country's entire GDP. Soviet economic assistance had not helped Cuba's long-term growth prospects by promoting diversification or sustainability. Although described as a "relatively highly developed Latin American export economy" in 1959 and the early 1960s, Cuba's basic economic structure changed very little between then and the 1980s. Tobacco products such as cigars and cigarettes were the only manufactured products among Cuba's leading exports and were produced using an expensive and labor-intensive pre-industrial process. The Cuban economy remained highly inefficient and over-specialized in a few highly subsidized commodities exported primarily to the Soviet bloc countries.


Although despising Argentina's right-wing military junta, Castro supported them in the 1982 Falklands War against Britain and offered military aid to the Argentinians. Castro supported the leftist New Jewel Movement that seized power in Grenada in 1979, befriending Grenadine president Maurice Bishop and sending doctors, teachers, and technicians to aid the country's development. When Bishop was executed in a Soviet-backed coup by hard-line Marxist Bernard Coard in October 1983, Castro condemned the killing but cautiously retained support for Grenada's government. However, the US used the coup as a basis for invading the island. Cuban soldiers died in the conflict, with Castro denouncing the invasion and comparing the US to Nazi Germany. In a July 1983 speech marking the 30th anniversary of the Cuban Revolution, Castro condemned Reagan's administration as a "reactionary, extremist clique" who were waging an "openly warmongering and fascist foreign policy". Castro feared a US invasion of Nicaragua and sent Ochoa to train the governing Sandinistas in guerrilla warfare but received little support from the USSR.


In 1985, Mikhail Gorbachev became general secretary of the Soviet Communist Party; a reformer, he implemented measures to increase freedom of the press (glasnost) and economic decentralization (perestroika) in an attempt to strengthen socialism. Like many orthodox Marxist critics, Castro feared that the reforms would weaken the socialist state and allow capitalist elements to regain control. Gorbachev conceded to US demands to reduce support for Cuba, with Soviet-Cuban relations deteriorating. On medical advice given him in October 1985, Castro gave up regularly smoking Cuban cigars, helping to set an example for the rest of the populace. Castro became passionate in his denunciation of the Third World debt problem, arguing that the Third World would never escape the debt that First World banks and governments imposed upon it. In 1985, Havana hosted five international conferences on the world debt problem.


By November 1987, Castro began spending more time on the Angolan Civil War, in which the Marxist MPLA government had fallen into retreat. Angolan president José Eduardo dos Santos successfully appealed for more Cuban troops, with Castro later admitting that he devoted more time to Angola than to the domestic situation, believing that a victory would lead to the collapse of apartheid. In response to the siege of Cuito Cuanavale in 1987–1988 by South African–UNITA forces, Castro sent an additional 12,000 Cuban Army troops to Angola in late 1987. From afar in Havana, Castro was closely involved in the decision-making about the defence of Cuito Cuanavle and came into conflict with Ochoa, whom he criticized for almost losing Cuito Cuanavle to a South African-UNITA assault on 13 January 1988 despite warning for almost two months prior that such an attack was coming. On 30 January 1988, Ochoa was summoned to a meeting with Castro in Havana where he was told that Cuito Cuanavale must not fall and to execute Castro's plans for a pull-back to more defensible positions over the objections of the Angolans. The Cuban troops played a decisive role in the relief of Cuito Cuanavale, breaking the siege in March 1988, which led to the withdrawal of most of the South African troops from Angola. Cuban propaganda turned the siege of Cuito Cuanavle into a decisive victory that changed the course of African history and Castro awarded 82 soldiers medals of the newly created Medal of Merit for the Defense of Cuito Cuanavle on 1 April 1988. Tensions were increased with the Cubans advancing close to the border of Namibia, which led to warnings from the South African government that they considered this an extremely unfriendly act, causing South Africa to mobilize and call up its reserves. In the spring of 1988, the intensity of South African-Cuban fighting drastically increased with both sides taking heavy losses.


The prospect of an all-out Cuban-South African war served to concentrate minds in both Moscow and Washington and led to an increased push for a diplomatic solution to the Angolan war. The cost of Cuba's wars in Africa were paid for with Soviet subsidies at a time when the Soviet economy was badly hurt by low oil prices while the apartheid government of South Africa had by the 1980s become a very awkward American ally as much of the American population, especially black Americans, objected to apartheid. From the viewpoint of both Moscow and Washington, having both Cuba and South Africa disengage in Angola was the best possible outcome. The low oil prices of the 1980s had also changed the Angolan attitude about subsidizing the Cuban economy as dos Santos found the promises made in the 1970s when oil prices were high to be a serious drain upon Angola's economy in the 1980s. South African whites were vastly outnumbered by South African blacks, and accordingly the South African Army could not take heavy losses with its white troops as that would fatally weaken the ability of the South African state to uphold apartheid. The Cubans had also taken heavy losses while the increasing difficult relations with dos Santos who become less generous in subsidizing the Cuban economy suggested that such losses were not worth the cost. Gorbachev called for a negotiated end to the conflict and in 1988 organized a quadripartite talk between the USSR, US, Cuba and South Africa; they agreed that all foreign troops would pull out of Angola while South Africa agreed to grant independence to Namibia. Castro was angered by Gorbachev's approach, believing that he was abandoning the plight of the world's poor in favour of détente.


When Gorbachev visited Cuba in April 1989, he informed Castro that perestroika meant an end to subsidies for Cuba. Ignoring calls for liberalization in accordance with the Soviet example, Castro continued to clamp down on internal dissidents and in particular kept tabs on the military, the primary threat to the government. A number of senior military officers, including Ochoa and Tony de la Guardia, were investigated for corruption and complicity in cocaine smuggling, tried, and executed in 1989, despite calls for leniency. In Eastern Europe, socialist governments fell to capitalist reformers between 1989 and 1991 and many Western observers expected the same in Cuba. Increasingly isolated, Cuba improved relations with Manuel Noriega's right-wing government in Panama—despite Castro's personal hatred of Noriega—but it was overthrown in a US invasion in December 1989. In February 1990, Castro's allies in Nicaragua, President Daniel Ortega and the Sandinistas, were defeated by the US-funded National Opposition Union in an election. With the collapse of the Soviet bloc, the US secured a majority vote for a resolution condemning Cuba's human rights violations at the United Nations Human Rights Commission in Geneva, Switzerland. Cuba asserted that this was a manifestation of US hegemony and refused to allow an investigative delegation to enter the country.


Special Period: 1992–2000

With favourable trade from the Soviet bloc ended, Castro publicly declared that Cuba was entering a "Special Period in Time of Peace". Petrol rations were dramatically reduced, Chinese bicycles were imported to replace cars, and factories performing non-essential tasks were shut down. Oxen began to replace tractors; firewood began being used for cooking and electricity cuts were introduced that lasted 16 hours a day. Castro admitted that Cuba faced the worst situation short of open war, and that the country might have to resort to subsistence farming. By 1992, Cuba's economy had declined by over 40% in under two years, with major food shortages, widespread malnutrition and a lack of basic goods. Castro hoped for a restoration of Marxism–Leninism in the USSR but refrained from backing the 1991 coup in that country. When Gorbachev regained control, Cuba-Soviet relations deteriorated further, and Soviet troops were withdrawn in September 1991. In December, the Soviet Union was officially dissolved as Boris Yeltsin abolished the Soviet Communist Party and introducing a capitalist multiparty democracy. Yeltsin despised Castro and developed links with the Miami-based Cuban American National Foundation. Castro tried improving relations with the capitalist nations. He welcomed Western politicians and investors to Cuba, befriended Manuel Fraga and took a particular interest in Margaret Thatcher's policies in the UK, believing that Cuban socialism could learn from her emphasis on low taxation and personal initiative. He ceased support for foreign militants, refrained from praising FARC on a 1994 visit to Colombia and called for a negotiated settlement between the Zapatistas and Mexican government in 1995. Publicly, he presented himself as a moderate on the world stage.


In 1991, Havana hosted the Pan American Games, which involved construction of a stadium and accommodation for the athletes; Castro admitted that it was an expensive error, but it was a success for Cuba's government. Crowds regularly shouted "Fidel! Fidel!" in front of foreign journalists, while Cuba became the first Latin American nation to beat the US to the top of the gold-medal table. Support for Castro remained strong, and although there were small anti-government demonstrations, the Cuban opposition rejected the exile community's calls for an armed uprising. In August 1994, Havana witnessed the largest anti-Castro demonstration in Cuban history, as 200 to 300 young men threw stones at police, demanding that they be allowed to emigrate to Miami. A larger pro-Castro crowd confronted them, who were joined by Castro; he informed media that the men were anti-socials misled by the US. The protests dispersed with no recorded injuries. Fearing that dissident groups would invade, the government organized the "War of All the People" defence strategy, planning a widespread guerrilla warfare campaign, and the unemployed were given jobs building a network of bunkers and tunnels across the country.


We do not have a smidgen of capitalism or neo-liberalism. We are facing a world completely ruled by neo-liberalism and capitalism. This does not mean that we are going to surrender. It means that we have to adapt to the reality of that world. That is what we are doing, with great equanimity, without giving up our ideals, our goals. I ask you to have trust in what the government and party are doing. They are defending, to the last atom, socialist ideas, principles and goals.


Castro believed in the need for reform if Cuban socialism was to survive in a world now dominated by capitalist free markets. In October 1991, the Fourth Congress of the Cuban Communist Party was held in Santiago, at which a number of important changes to the government were announced. Castro would step down as head of government, to be replaced by the much younger Carlos Lage, although Castro would remain the head of the Communist Party and commander-in-chief of the armed forces. Many older members of government were to be retired and replaced by their younger counterparts. A number of economic changes were proposed, and subsequently put to a national referendum. Free farmers' markets and small-scale private enterprises would be legalized in an attempt to stimulate economic growth, while US dollars were also made legal tender. Certain restrictions on emigration were eased, allowing more discontented Cuban citizens to move to the United States. Further democratization was to be brought in by having the National Assembly's members elected directly by the people, rather than through municipal and provincial assemblies. Castro welcomed debate between proponents and opponents of the economics reforms—although over time he began to increasingly sympathise with the opponent's positions, arguing that such reforms must be delayed.


Castro's government diversified its economy into biotechnology and tourism, the latter outstripping Cuba's sugar industry as its primary source of revenue in 1995. The arrival of thousands of Mexican and Spanish tourists led to increasing numbers of Cubans turning to prostitution; officially illegal, Castro refrained from cracking down on prostitution in Cuba, fearing a political backlash. Economic hardship led many Cubans toward religion, both in the form of Catholicism and Santería. Although long thinking religious belief to be backward, Castro softened his approach to religious institutions and religious people were permitted for the first time to join the Communist Party. Although he viewed the Catholic Church as a reactionary, pro-capitalist institution, Castro organized a visit to Cuba by Pope John Paul II for January 1998; it strengthened the position of both the Cuban Church and Castro's government.


In the early 1990s Castro embraced environmentalism, campaigning against global warming and the waste of natural resources and accusing the US of being the world's primary polluter. In 1994 a ministry dedicated to the environment was established, and new laws established in 1997 that promoted awareness of environmental issues throughout Cuba and stressed the sustainable use of natural resources. By 2006, Cuba was the world's only nation which met the United Nations Development Programme's definition of sustainable development, with an ecological footprint of less than 1.8 hectares per capita and a Human Development Index of over 0.8. Castro also became a proponent of the anti-globalization movement, criticizing US global hegemony and the control exerted by multinationals. Castro maintained his strong stance against apartheid, and at the 26 July celebrations in 1991, he was joined onstage by Nelson Mandela, recently released from prison. Mandela praised Cuba's involvement in battling South Africa during the Angolan Civil War and thanked Castro personally. Castro later attended Mandela's inauguration as President of South Africa in 1994. In 2001, Castro attended the Conference Against Racism in South Africa at which he lectured on the global spread of racial stereotypes through US film.


Battle of Ideas: 2000–2006

Mired in economic problems, Cuba was aided by the election of Hugo Chávez to the Venezuelan Presidency in 1999. Castro and Chávez developed a close friendship, with the former acting as a mentor and father-figure to the latter, and together they built an alliance that had repercussions throughout Latin America. In 2000, they signed an agreement through which Cuba would send 20,000 medics to Venezuela, in return receiving 53,000 barrels of oil per day at preferential rates; in 2004, this trade was stepped up, with Cuba sending 40,000 medics and Venezuela providing 90,000 barrels a day.
Meanwhile, in 1998, Canadian prime minister Jean Chrétien arrived in Cuba to meet Castro and highlight their close ties. He was the first Canadian government leader to visit the island since Pierre Trudeau was in Havana in 1976.


After a spontaneous march for the return of Elián González, in December 2000, a youth group named: "Group of the Battle of Ideas", was formed by the Young Communist League and the Federation of University Students. The group began organizing demonstrations across Cuba for the return of Elián González. After González's return, the group began regularly meeting with Fidel Castro to oversee various construction projects and government meetings in Cuba. Fidel Castro ensured that the group had special authorities, and could bypass the approval of various ministries. Along with domestic projects, the wider campaign known as the "Battle of Ideas" included attempts to provide medical aid to various pink tide governments.


In 2002, former US president Jimmy Carter visited Cuba, where he highlighted the lack of civil liberties in the country and urged the government to pay attention to the Varela Project of Oswaldo Payá.


Economic problems remained in Cuba, and in 2004, Castro shut down 118 factories, including steel plants, sugar mills and paper processors to compensate for a critical shortage of fuel. In September 2005, Castro established a group of medical professionals, known as the Henry Reeve Brigade, with the mission of international medical solidarity. The group were sent throughout the world to carry out humanitarian missions on behalf of the Cuban government.


Cuba and Venezuela became the founding members of the Bolivarian Alternative for the Americas (ALBA). ALBA's origins lay in a December 2004 agreement signed between the two countries and was formalized through a People's Trade Agreement also signed by Evo Morales' Bolivia in April 2006. Castro had also been calling for greater Caribbean integration since the late 1990s, saying that only strengthened cooperation between Caribbean countries would prevent their domination by rich nations in a global economy. Cuba has opened four additional embassies in the Caribbean Community including: Antigua and Barbuda, Dominica, Suriname, Saint Vincent and the Grenadines. This development makes Cuba the only country to have embassies in all independent countries of the Caribbean Community.


In contrast to the improved relations between Cuba and a number of leftist Latin American states, in 2004 it broke off diplomatic ties with Panama after centrist President Mireya Moscoso pardoned four Cuban exiles accused of attempting to assassinate Castro in 2000. Diplomatic ties were reinstalled in 2005 following the election of leftist President Martín Torrijos.
Castro's improving relations across Latin America were accompanied by continuing animosity towards the US. However, after massive damage caused by Hurricane Michelle in 2001, Castro successfully proposed a one-time cash purchase of food from the US while declining its government's offer of humanitarian aid. Castro expressed solidarity with the US following the 2001 September 11 attacks, condemning Al-Qaeda and offering Cuban airports for the emergency diversion of any US planes. He recognized that the attacks would make US foreign policy more aggressive, which he believed was counterproductive. Castro criticized the 2003 invasion of Iraq, saying that the US-led war had imposed an international "law of the jungle".


Final years

Stepping down: 2006–2008

Castro underwent surgery for intestinal bleeding, and on 31 July 2006, delegated his presidential duties to Raúl Castro. In February 2007, Raúl announced that Fidel's health was improving and that he was taking part in important issues of government. Later that month, Fidel called into Hugo Chávez's radio show Aló Presidente. On 21 April, Castro met Wu Guanzheng of the Chinese Communist Party's Politburo Standing Committee, with Chávez visiting in August, and Morales in September. That month, the Non-Aligned Movement held its 14th Summit in Havana, there agreeing to appoint Castro as the organization's president for a year's term.


Commenting on Castro's recovery, US president George W. Bush said: "One day the good Lord will take Fidel Castro away." Hearing about this, the atheist Castro replied: "Now I understand why I survived Bush's plans and the plans of other presidents who ordered my assassination: the good Lord protected me." The quote was picked up on by the world's media.


In a February 2008 letter, Castro announced that he would not accept the positions of President of the Council of State and Commander in Chief at that month's National Assembly meetings, remarking, "It would betray my conscience to take up a responsibility that requires mobility and total devotion, that I am not in a physical condition to offer". On 24 February 2008, the National Assembly of People's Power unanimously voted Raúl as president. Describing his brother as "not substitutable", Raúl proposed that Fidel continue to be consulted on matters of great importance, a motion unanimously approved by the 597 National Assembly members.


Retirement: 2008–2016

Following his retirement, Castro's health deteriorated; international press speculated that he had diverticulitis, but Cuba's government refused to corroborate this. He continued to interact with the Cuban people, published an opinion column titled "Reflections" in Granma, used a Twitter account, and gave occasional public lectures. In January 2009 Castro asked Cubans not to worry about his lack of recent news columns and failing health, and not to be disturbed by his future death. He continued meeting foreign leaders and dignitaries, and that month photographs were released of Castro's meeting with Argentine president Cristina Fernández.


In July 2010, he made his first public appearance since falling ill, greeting science center workers and giving a television interview to Mesa Redonda in which he discussed US tensions with Iran and North Korea. On 7 August 2010, Castro gave his first speech to the National Assembly in four years, urging the US not to take military actions against those nations and warning of a nuclear holocaust. When asked whether Castro may be re-entering government, culture minister Abel Prieto told the BBC, "I think that he has always been in Cuba's political life but he is not in the government... He has been very careful about that. His big battle is international affairs." In August 2010, Castro accepted responsibility for persecuting gay men in the 1960s and 70s, which included imprisonment in forced labor camps.


On 19 April 2011, Castro resigned from the Communist Party central committee, thus stepping down as First Secretary. Raúl was selected as his successor. Now without any official role in the country's government, he took on the role of an elder statesman. In March 2011, Castro condemned the NATO-led military intervention in Libya. In late March 2012, Pope Benedict XVI visited Cuba for three days, during which time he briefly met with Castro despite the Pope's vocal opposition to Cuba's government.


Later in 2012, it was revealed that along with Hugo Chávez, Castro had played a significant behind-the-scenes role in orchestrating peace talks between the Colombian government and the far left FARC guerrilla movement to end the conflict which had raged since 1964. During the North Korea crisis of 2013, he urged both the North Korean and US governments to show restraint. Calling the situation "incredible and absurd", Castro maintained that war would not benefit either side, and that it represented "one of the gravest risks of nuclear war" since the Cuban missile crisis.


In December 2014, Castro was awarded the Chinese Confucius Peace Prize for seeking peaceful solutions to his nation's conflict with the US and for his post-retirement efforts to prevent nuclear war. In January 2015, he publicly commented on the "Cuban Thaw", an increased normalization between Cuba-US relations, by stating that while it was a positive move for establishing peace in the region, he mistrusted the US government. He did not meet with US president Barack Obama on the latter's visit to Cuba in March 2016, although sent him a letter stating that Cuba "has no need of gifts from the empire".


In April 2016, he gave his most extensive public appearance in many years when addressing the Communist Party. Highlighting that he was soon to turn 90 years old, he noted that he would die in the near future but urged those assembled to retain their communist ideals. In September 2016, Castro was visited at his Havana home by the Iranian president Hassan Rouhani, and later that month was visited by Japanese prime minister Shinzo Abe. In late October 2016, Castro met with the Portuguese president Marcelo Rebelo de Sousa, who became one of the last foreign leaders to meet him.


Death

Castro died in Havana on the night of 25 November 2016 at the age of 90. The cause of death was not disclosed.


His brother, President Raúl Castro, confirmed the news in a brief speech: "The commander in chief of the Cuban revolution died at 22:29  this evening."


Fidel Castro was cremated the next day. A funeral procession travelled 900 kilometres (560 mi) along the island's central highway from Havana to Santiago de Cuba, tracing in reverse the route of the "Freedom Caravan" of January 1959.


After nine days of public mourning, his ashes were entombed in the Santa Ifigenia Cemetery in Santiago de Cuba.


Fidel's death came nine months after his older brother Ramón died at the age of 91 in February.


Ideology

Castro proclaimed himself to be "a Socialist, a Marxist, and a Leninist", and publicly identified as a Marxist–Leninist from December 1961 onward. Castro sought to transform Cuba from a capitalist state to a socialist society and ultimately to a communist society. Influenced by Guevara, he suggested that Cuba could evade most stages of socialism and progress straight to communism. According to Castro, a country could be regarded as socialist if its means of production were controlled by the state. In this way, his understanding of socialism was less about who controlled power in a country and more about the method of distribution.


Castro's government was also nationalistic and drew upon a longstanding tradition of Cuban nationalism. Historian Richard Gott remarked that one of the keys to Castro's success was his ability to use the "twin themes of socialism and nationalism" and keep them "endlessly in play". Castro described Karl Marx and Cuban nationalist José Martí as his main political influences, although Gott believed that Martí ultimately remained more important than Marx in Castro's politics. Castro also praised the Colombian Catholic revolutionary Camilo Torres Restrepo, stating that Camilo "is a symbol of the revolutionary unity which should inspire the movement of the peoples in Latin America."


Theodore Draper described Castro's approach as "Castroism", viewing it as a blend of European socialism with the Latin American revolutionary tradition. Political scientist Paul C. Sondrol described the approach as "totalitarian utopianism", with leadership that drew upon the wider Latin American phenomenon of the caudillo. Castro drew inspiration from the wider Latin American anti-imperialist movements of the 1930s and 1940s, including Argentina's Perón and Guatemala's Jacobo Árbenz. Castro took a relatively socially conservative stance on many issues, opposing drug use, gambling, and prostitution, which he viewed as moral evils. He advocated hard work, family values, integrity, and self-discipline. Although his government repressed homosexual activity for decades, he later described this persecution as a "great injustice".


Personal life

Religious beliefs

Castro's religious beliefs have been a matter of some debate; he was baptized and raised as a Catholic. He criticized the use of the Bible to justify the oppression of women and Africans, but commented that Christianity exhibited "a group of very humane precepts" which gave the world "ethical values" and a "sense of social justice", relating, "If people call me Christian, not from the standpoint of religion but from the standpoint of social vision, I declare that I am a Christian." During a visit of American minister and activist Jesse Jackson, Castro accompanied him to a Methodist church service where he even spoke from the pulpit with a Bible before him, an event that marked a beginning of increased openness towards Christianity in Cuba. He promoted the idea that Jesus Christ was a communist, citing the feeding of the 5,000 and the story of Jesus and the rich young man as evidence.


Wealth

Forbes magazine ranked Castro as the seventh wealthiest ruler in the world at an estimated personal wealth of approximately 900 million US dollars in 2006, going from 550 million US dollars in their 2005 list. The estimate is based on the magazine's assumption that Castro




